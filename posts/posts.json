[
  {
    "path": "posts/2022-11-05-spatial-data-heck/",
    "title": "Spatial data heck bingo",
    "description": "The bingo game we all play but don't want to win.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2022-11-05",
    "categories": [
      "rants",
      "spatial"
    ],
    "contents": "\nIt’s 2022.\nWorking with spatial data is easier now than ever before.\nBetween the rise of cloud native geospatial, SpatioTemporal Asset Catalogs, and the maturation of open source spatial libraries, it is extremely rare to find oneself in spatial data hell.\nAnd yet, there are many small annoying things that slow the day to day work.\nThis is spatial data heck.\nAnd this is the spatial data heck bingo card.\nPut it on your wall and know that you’re not alone.\n\n\n\nFigure 1: Spatial data heck bingo card.\n\n\n\n\n\n\n",
    "preview": "posts/2022-11-05-spatial-data-heck/spatial-data-heck_files/figure-html5/plot-img-1.png",
    "last_modified": "2022-11-05T22:57:49-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-28-a-step-by-step-guide-to-marginalizing-over-discrete-parameters-for-ecologists-using-stan/",
    "title": "A step-by-step guide to marginalizing over discrete parameters for ecologists using Stan",
    "description": "Everything you might have been afraid to ask about implementing models with \ndiscrete parameters in Stan. Written for ecologists that know BUGS, JAGS, or NIMBLE, and want to use Stan. Provides an example by marginalizing\nover partly observed presence/absence states in a simple occupancy model.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2020-04-28",
    "categories": [],
    "contents": "\nDiscrete parameters can be a major stumbling block for ecologists using Stan, because you need to marginalize over the latent discrete parameters (e.g., “alive/dead”, “occupied/not occupied”, “infected/not infected”, etc.).\nThis post demonstrates how to do it, step by step for a simple example.\nI’ll try to make it clear what we are doing along the way, as we work towards a model that we can represent in Stan.\nConsider a Bayesian site occupancy model.\nWe want to estimate occurrence states (presence or absence) using observed detection/non-detection data (MacKenzie et al. 2002).\nFor sites \\(i=1, ...,N\\) we have \\(K\\) replicate sampling occasions.\nOn each sampling occasion, we visit site \\(i\\) and look for a critter (a bug, a bird, a plant, etc.).\nRepresent the number of sampling occasions where we detected the critter at site \\(i\\) as \\(y_i\\).\nSo if we see the critter on two surveys, \\(y_i = 2\\).\nWe assume that if a site is occupied (\\(z_i=1\\)), we detect the animal with probability \\(p\\).\nIf a site is not occupied (\\(z_i=0\\)), we can’t observe it.\nHow a BUGS/JAGS/NIMBLE user might write the model\nWe can write the observation model for site \\(i\\) as:\n\\[y_i \\sim \\text{Binomial}(K, p z_i).\\]\n\nThis assumes that detections are independent across surveys \\(k=1, ..., K\\).\nAnd our prior for the occupancy state of site \\(i\\) is:\n\\[z_i \\sim \\text{Bernoulli}(\\psi),\\]\nwhere \\(\\psi\\) is the probability of occupancy.\nA Bayesian model specification is completed by assigning priors to the remaining parameters:\n\\[p \\sim \\text{Uniform}(0, 1),\\]\n\\[\\psi \\sim \\text{Uniform}(0, 1).\\]\nSquare bracket probability notation\nLet’s rewrite the model in a different way.\nFirst, I want to introduce a different notation for our observation model:\n\\[[y_i \\mid p, z_i] = \\text{Binomial}(y_i \\mid K, p z_i).\\]\nIn this notation, square brackets represent probability mass or density functions (for discrete or continuous quantities, respectively).\nHere, \\([y_i \\mid p, z_i]\\) is the probability mass function of \\(y_i\\) conditioned on the parameters \\(p\\) and \\(z_i\\).\nIf we assume that the observations \\(y_{1:N} = y_1, ..., y_N\\) for each site are conditionally independent, we can write the observation model for all sites \\(1:N\\) as:\n\\[[y_{1:N} \\mid p, z_{1:N}] = \\prod_{i=1}^N [y_i \\mid p, z_i],\\]\nwhich follows from the definition of the joint distribution of independent random variables.\nWe can rewrite the rest of the model in the same notation.\nThe state model for site \\(i\\) becomes:\n\\[[z_i \\mid \\psi] = \\text{Bernoulli}(z_i \\mid \\psi).\\]\nAnd, again if we assume that the occupancy states for site \\(i=1, ..., N\\) are conditionally independent, then we can write down the state model for all sites as:\n\\[[z_{1:N} \\mid \\psi] = \\prod_{i=1}^N \\text{Bernoulli}(z_i \\mid \\psi).\\]\nFinally, we can write the priors in square bracket notation:\n\\[[\\psi] = \\text{Uniform}(\\psi \\mid 0, 1),\\]\n\\[[p] = \\text{Uniform}(p \\mid 0, 1).\\]\nWriting the joint distribution\nWe are working towards a model specification that we can use in Stan, which means we need the log of the joint distribution of data and parameters.\n\nIf you just mouthed the words “what the ****”, stay with me.\nThe “joint distribution of data and parameters” is the numerator in Bayes’ theorem, which gives us an expression for the posterior probability distribution of parameters \\(\\theta\\) given data \\(y\\):\n\\[[\\theta \\mid y] = \\dfrac{[y, \\theta]}{[y]}.\\]\nIn this example, our parameters \\(\\theta\\) are:\n\\(z_{1:N}\\): the occupancy states\n\\(p\\): the detection probability\n\\(\\psi\\): the occupancy probability\nThe data consist of the counts \\(y_{1:N}\\).\nSo our joint distribution is:\n\\[[y_{1:N}, \\theta] = [y_{1:N}, z_{1:N}, p, \\psi],\\]\nWe can factor this using the rules of conditional probability and the components we worked out in the previous section.\nFirst, recognize that:\n\\(y\\) depends on \\(p\\) and \\(z\\),\n\\(z\\) depends on \\(\\psi\\), and\n\\(p\\) and \\(\\psi\\) don’t depend on any other parameters:\n\\[[y_{1:N}, z_{1:N}, p, \\psi] = [y_{1:N} \\mid p, z_{1:N}] \\times [z_{1:N} \\mid \\psi] \\times [p, \\psi].\\]\nThen, recall that we can represent the joint probability distribution for the capture histories and states as a product of site-specific terms:\n\\[[y_{1:N}, z_{1:N}, p, \\psi] = \\prod_{i=1}^N [y_i \\mid p, z_i] \\times \\prod_{i=1}^N [z_i \\mid \\psi] \\times [p, \\psi].\\]\nWe can simplify this a little:\n\\[[y_{1:N}, z_{1:N}, p, \\psi] = \\prod_{i=1}^N [y_i \\mid p, z_i] [z_i \\mid \\psi] \\times [p, \\psi].\\]\nLast, we have independent priors for \\(p\\) and \\(\\psi\\), so we can write the joint distribution as:\n\\[[y_{1:N}, z_{1:N}, p, \\psi] = \\prod_{i=1}^N [y_i \\mid p, z_i] [z_i \\mid \\psi] \\times [p] [\\psi].\\]\n\nThis follows from the definition of independent random variables. If \\(A\\) and \\(B\\) are independent, \\([A, B] = [A] \\times [B]\\).\nIn case that is confusing, it can also be useful to visualize this same dependence structure graphically.\n\n\n\nFigure 1: A directed acyclic graph for an occupancy model. Arrows represent dependence (e.g., p -> y means y depends on p).\n\n\n\nThis is almost in a form that we can use in Stan.\nBut we need to get rid of \\(z\\) from the model.\nIt’s a discrete parameter, and Stan needs continuous parameters.\nMarginalizing over discrete parameters\nTo get rid of our discrete parameter \\(z\\), we need to marginalize it out of the model.\nIn general, if you have a joint distribution for \\(y\\) and \\(z\\) that depends on \\(\\theta\\), you obtain the marginal distribution of \\(y\\) by summing the joint distribution over all possible values of \\(z\\):\n\\[[y \\mid \\theta] = \\sum_{z} [y, z \\mid \\theta].\\]\n\nThis is sometimes referred to as “summing out the responsibility parameter”. See why?\nIn our case, for the \\(i^{th}\\) site, this means that we need to marginalize over \\(z_i\\) as follows:\n\\[[y_i \\mid p, \\psi] = \\sum_{z_i=0}^1 [y_i, z_i \\mid p, \\psi].\\]\nWe are summing over all possible values of \\(z_i\\). In this case there are two (\\(z_i\\) can be 0 or 1).\nWe can factor the joint distribution:\n\\[[y_i \\mid p, \\psi] = \\sum_{z_i=0}^1 [y_i \\mid p, z_i] [z_i \\mid \\psi].\\]\nThis is:\n\\[[y_i \\mid p, \\psi] = [y_i \\mid p, z_i=0] [z_i=0 \\mid \\psi] + [y_i \\mid p, z_i=1] [z_i=1 \\mid \\psi].\\]\nEarlier we said \\(\\psi\\) is the probability that \\(z_i = 1\\).\nSo, \\(1-\\psi\\) is the probability that \\(z_i=0\\):\n\\[[y_i \\mid p, \\psi] = (1 - \\psi) [y_i \\mid p, z_i=0] + \\psi [y_i \\mid p, z_i=1].\\]\n\nReplace \\([z_i=0 \\mid \\psi]\\) with \\(1-\\psi\\), and \\([z_i=1 \\mid \\psi]\\) with \\(\\psi\\).\nWe can also simplify the observation model for unoccupied sites.\nBecause we assume that there are no false positive detections, unoccupied sites (\\(z_i=0\\)) can only generate zero counts for \\(y_i\\) (or, you could say that \\(y_i\\) is identically zero if \\(z_i=0\\)).\nWe can then write this as:\n\\[[y_i \\mid p, \\psi] = (1 - \\psi) I(y_i=0) + \\psi [y_i \\mid p, z_i=1],\\]\nwhere \\(I(y_i=0)\\) is an indicator function that is equal to one if \\(y_i=0\\), and otherwise is equal to zero.\nIt might be more intuitive to write this as:\n\\[[y_i \\mid p, \\psi] = \\begin{cases}\n        \\psi [y_i \\mid p, z_i=1], & \\text{for } y_i > 0\\\\\n        \\psi [y_i \\mid p, z_i=1] + 1 - \\psi, & \\text{for } y_i = 0\n        \\end{cases}\\]\nWe can make this even more explicit by bringing back in the fact that our probability model for \\(y_i\\) is Binomial:\n\\[[y_i \\mid p, \\psi] = \\begin{cases}\n        \\psi \\text{Binomial}(y_i \\mid p), & \\text{for } y_i > 0\\\\\n        \\psi \\text{Binomial}(y_i \\mid p) + 1 - \\psi, & \\text{for } y_i = 0\n        \\end{cases}\\]\nGreat - we just marginalized \\(z_i\\) out of the model.\nLet’s circle back to the joint distribution and see what it looks like now.\nPreviously we had:\n\\[[y_{1:N}, z_{1:N}, p, \\psi] = \\prod_{i=1}^N [y_i \\mid p, z_i] [z_i \\mid \\psi] \\times [p] [\\psi].\\]\nNow, if we marginalize over \\(z\\) for every site, we’d be computing:\n\\[[y_{1:N}, p, \\psi] = \\prod_{i=1}^N \\Big( \\sum_{z_i=0}^1 [y_i \\mid p, z_i] [z_i \\mid \\psi] \\Big) \\times [p] [\\psi],\\]\n\\[ = \\prod_{i=1}^N [y_i \\mid p, \\psi] \\times [p] [\\psi].\\]\nThis joint distribution is something we can work with in Stan.\nThe last thing we need to do is write down the log of the joint distribution, and translate that into Stan’s syntax.\nThe log of the joint distribution\nWe are going to specify the joint distribution in Stan on the log scale.\nTake the log of our joint distribution:\n\\[\\log([y_{1:N}, p, \\psi]) = \\log \\Bigg(\\prod_{i=1}^N [y_i \\mid p, \\psi] \\times [p] [\\psi] \\Bigg),\\]\nand by “\\(\\log\\)” I mean the natural log.\nRecall that the log of a product is the sum of logs: \\(\\log(ab) = \\log(a) + \\log(b)\\)).\nWe can apply this rule and find that:\n\\[\\log([y_{1:N}, p, \\psi]) = \\sum_{i=1}^N \\log [y_i \\mid p, \\psi] + \\log[p] + \\log[\\psi].\\]\nLet’s think about how to represent \\(\\log [y_i \\mid p, \\psi]\\).\nRecall from before that:\n\\[[y_i \\mid p, \\psi] = \\begin{cases}\n        \\psi \\text{Binomial}(y_i \\mid p), & \\text{for } y_i > 0\\\\\n        \\psi \\text{Binomial}(y_i \\mid p) + 1 - \\psi, & \\text{for } y_i = 0\n        \\end{cases}\\]\nTaking logarithms, we get:\n\\[\\log [y_i \\mid p, \\psi] = \\begin{cases}\n        \\log \\big( \\psi \\text{Binomial}(y_i \\mid p) \\big), & \\text{for } y_i > 0\\\\\n        \\log \\big( \\psi \\text{Binomial}(y_i \\mid p) + 1 - \\psi \\big), & \\text{for } y_i = 0\n        \\end{cases}\\]\nRecalling rules about logs of products, we can rewrite this as:\n\\[\\log [y_i \\mid p, \\psi] = \\begin{cases}\n        \\log \\psi + \\log(\\text{Binomial}(y_i \\mid p)), & \\text{for } y_i > 0\\\\\n        \\log \\big( \\psi \\text{Binomial}(y_i \\mid p) + 1 - \\psi \\big), & \\text{for } y_i = 0\n        \\end{cases}\\]\nStan has a function called binomial_lpmf (“binomial log probability mass function”) that gives us exactly what we need to compute \\(\\log(\\text{Binomial}(y_i \\mid p))\\) above.\nTo make this connection clear, let’s rewrite this as:\n\\[\\log [y_i \\mid p, \\psi] = \\begin{cases}\n        \\log \\psi + \\text{binomial_lpmf}(y_i \\mid p), & \\text{for } y_i > 0\\\\\n        \\log \\big( \\psi \\text{Binomial}(y_i \\mid p) + 1 - \\psi \\big), & \\text{for } y_i = 0\n        \\end{cases}\\]\nThen, let’s re-write the case where \\(y_i=0\\) as:\n\\[\\log [y_i \\mid p, \\psi] = \\begin{cases}\n        \\log \\psi + \\text{binomial_lpmf}(y_i \\mid p), & \\text{for } y_i > 0\\\\\n        \\log \\big( e^{\\log(\\psi \\text{Binomial}(y_i \\mid p))} + e^{\\log(1 - \\psi)} \\big), & \\text{for } y_i = 0\n        \\end{cases}\\]\n\nIt may seem silly to do this, but trust me it will be useful.\nThis is true because \\(e^{\\log(x)}=x\\).\nThen, apply the rule about \\(\\log(ab) = \\log a + \\log b\\) again:\n\\[\\log [y_i \\mid p, \\psi] = \\begin{cases}\n        \\log \\psi + \\text{binomial_lpmf}(y_i \\mid p), & \\text{for } y_i > 0\\\\\n        \\log \\big( e^{\\log \\psi + \\text{binomial_lpmf}(y_i \\mid p)} + e^{\\log(1 - \\psi)} \\big), & \\text{for } y_i = 0\n        \\end{cases}\n\\]\nAt this point, we are going to bring in the LogSumExp trick, which gives us a computationally stable way to compute terms like \\(\\log(\\sum_i \\exp(x_i))\\).\nStan has a function called log_sum_exp that does this for us, and it takes the terms to sum on the log scale as inputs.\nLet’s rewrite the model for the data with this function:\n\\[\\log [y_i \\mid p, \\psi] = \\begin{cases}\n        \\log \\psi + \\text{binomial_lpmf}(y_i \\mid p), &  y_i > 0\\\\\n        \\text{log_sum_exp}(\\log \\psi + \\text{binomial_lpmf}(y_i \\mid p),\\; \\log(1 - \\psi)), &  y_i = 0\n        \\end{cases}\n\\]\nTranslating our model to Stan\nHere’s the Stan model (written for clarity, not computational efficiency):\ndata {\n  int<lower = 1> N;\n  int<lower = 1> K;\n  int<lower = 0, upper = K> y[N];\n}\n\nparameters {\n  real<lower = 0, upper = 1> p;\n  real<lower = 0, upper = 1> psi;\n}\n\ntransformed parameters {\n  vector[N] log_lik;\n  \n  for (i in 1:N) {\n    if (y[i] > 0) {\n      log_lik[i] = log(psi) + binomial_lpmf(y[i] | K, p);\n    } else {\n      log_lik[i] = log_sum_exp(\n        log(psi) + binomial_lpmf(y[i] | K, p), \n        log1m(psi)\n      );\n    }\n  }\n}\n\nmodel {\n  target += sum(log_lik);\n  target += uniform_lpdf(p | 0, 1);\n  target += uniform_lpdf(psi | 0, 1);\n}\nThe observation model\nWe symbolically represented the observation model as:\n\\[\\log [y_i \\mid p, \\psi] = \\begin{cases}\n        \\log \\psi + \\text{binomial_lpmf}(y_i \\mid p), &  y_i > 0\\\\\n        \\text{log_sum_exp}(\\log \\psi + \\text{binomial_lpmf}(y_i \\mid p),\\; \\log(1 - \\psi)), &  y_i = 0\n        \\end{cases}\\]\nIn Stan syntax, we are storing \\(\\log [y_i \\mid p, \\psi]\\) in the \\(i^{th}\\) element of the vector log_lik.\nWe used an if-statement to deal with the different cases:\n...\n    if (y[i] > 0) {\n      log_lik[i] = log(psi) + binomial_lpmf(y[i] | K, p);\n    } else {\n      log_lik[i] = log_sum_exp(\n        log(psi) + binomial_lpmf(y[i] | K, p), \n        log1m(psi)\n      );\n    }\n...\nThe log joint distribution and target +=\nNotice how in the model block, we used target += syntax to add things to the log joint distribution:\n...\nmodel {\n  target += sum(log_lik);\n  target += uniform_lpdf(p | 0, 1);\n  target += uniform_lpdf(psi | 0, 1);\n}\n...\nThese terms correspond to the log joint distribution that we represented symbolically as\n\\[\\log([y_{1:N}, p, \\psi]) = \\sum_{i=1}^N \\log [y_i \\mid p, \\psi] + \\log[p] + \\log[\\psi].\\]\nBringing it all together\nTo recap, to go from our model specification with discrete parameters to a model that we can use in Stan, we did the following:\nWrote the joint distribution of our model\nMarginalized discrete parameter(s) out of the joint distribution\nTook the log of the joint distribution\nTranslated the log of the joint distribution into Stan code\nThis general approach applies to a variety of models, but occupancy models provide a simple example.\nMore resources\nThe Stan documentation has some great content on marginalization of discrete parameters: https://mc-stan.org/docs/2_18/stan-users-guide/latent-discrete-parameterization.html\nBob Carpenter put together a great case study for multi-species occupancy models in Stan, which provides a step up in complexity from this single-species model: https://mc-stan.org/users/documentation/case-studies/dorazio-royle-occupancy.html\nThere are a ton of ecological models with discrete parameters that Hiroki Itô has translated to Stan from the book “Bayesian Population Analysis using WinBUGS — A Hierarchical Perspective” (2012) by Marc Kéry and Michael Schaub (Kéry and Schaub 2011): https://github.com/stan-dev/example-models/tree/master/BPA\nMost of the example spatial capture-recapture models from the 2013 book Spatial Capture-Recapture by Royle, Chandler, Gardner, and Sollmann (Royle et al. 2013) have also been translated to Stan: https://github.com/mbjoseph/scr-stan\nThis post demonstrated how to marginalize symbolically (or “on paper” I guess), but there is another great blog post by Jacob Socolar that focuses on JAGS to Stan code translation (and includes a marginal model implementation in JAGS): https://github.com/jsocolar/occupancyModels\n\n\n\nKéry, Marc, and Michael Schaub. 2011. Bayesian Population Analysis Using WinBUGS: A Hierarchical Perspective. Academic Press.\n\n\nMacKenzie, Darryl I, James D Nichols, Gideon B Lachman, Sam Droege, J Andrew Royle, and Catherine A Langtimm. 2002. “Estimating Site Occupancy Rates When Detection Probabilities Are Less Than One.” Ecology 83 (8): 2248–55.\n\n\nRoyle, J Andrew, Richard B Chandler, Rahel Sollmann, and Beth Gardner. 2013. Spatial Capture-Recapture. Academic Press.\n\n\n\n\n",
    "preview": "posts/2020-04-28-a-step-by-step-guide-to-marginalizing-over-discrete-parameters-for-ecologists-using-stan/dag.png",
    "last_modified": "2022-11-01T22:13:42-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-13-neural-hierarchical-models/",
    "title": "Behind the paper: Neural hierarchical models of ecological populations",
    "description": "A high-level overview, an example, and a call to action.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2020-01-23",
    "categories": [
      "papers"
    ],
    "contents": "\nThis post gives some background and a demo for the paper “Neural hierarchical models of ecological populations” out today in Ecology Letters (Joseph 2020).\nDeep learning and model-based ecological inference may seem like totally separate pursuits.\nYet, if you think about deep learning as a set of tools to approximate functions, it’s not much of a leap to begin seeing opportunities to unite deep learning with some standard ecological modeling approaches.\nHierarchical models\nHierarchical models have been around for a while, and are now one of the workhorse methods of modern quantitative ecology (e.g., occupancy models, capture-recapture models, N-mixture models, animal movement models, state-space models, etc.).\nHierarchical models combine:\nA data model \\([y \\mid z, \\theta]\\) where we observe \\(y\\), that depends on a process \\(z\\), and parameter(s) \\(\\theta\\),\nA process model \\([z \\mid \\theta]\\), and\nA parameter model \\([\\theta]\\).\nA posterior distribution of the unknowns, conditioned on the data is:\n\\[[z, \\theta \\mid y] = \\dfrac{[y \\mid z, \\theta] \\times [z \\mid \\theta] \\times [\\theta]}{[y]}.\\]\nWe might also have some explanatory variables \\(x\\) that might tell us something about \\(z\\), \\(\\theta\\), and/or \\(y\\).\nNeural networks\nNeural networks approximate functions.\nOff the shelf neural networks usually just map \\(x\\) to \\(y\\), and allow us to predict new values of \\(y\\) for new values of \\(x\\).\nSometimes, predicting \\(y\\) is not really what we care about - we really want to learn something about a process \\(z\\) or some parameters \\(\\theta\\).\nNeural hierarchical models\nWe can parameterize a hierarchical model with a neural network to learn about \\(z\\).\nSo, for example, if \\(\\theta\\) represents the parameters of a neural network, then we can construct a process model \\([z \\mid \\theta]\\) where our input \\(x\\) is mapped to a process \\(z\\) by way of some neural network:\n\\[[z \\mid \\theta] = f(x, \\theta),\\]\nwhere \\(f\\) is a neural network that maps \\(x\\) and \\(\\theta\\) to some probabilistic model for \\(z\\) (here because \\(x\\) is observed, I’m not going to condition \\(z\\) on it on the left hand side of the equation - \\(x\\) is assumed to be constant and known without error).\nGraphically, you might consider a state-space model where some inputs \\(x\\) are mapped to a state transition matrix (for an example with an animal movement model, see Appendix S2 in the paper):\n\n\n\nFigure 1: Using a convolutional neural network to map some input raster to a state transition matrix in a hidden Markov model\n\n\n\nAn example: a neural N-mixture model\nAn N-mixture model can be used to estimate latent integer-valued abundance when unmarked populations are repeatedly surveyed and it is assumed that detection of individuals is imperfect (Royle 2004).\nAssume that \\(J\\) spatial locations are each surveyed \\(K\\) times, in a short time interval for which it is reasonable to assume that the number of individuals is constant within locations \\(j=1, ..., J\\).\nEach spatial location has some continuous covariate value represented by \\(x_j\\), that relates to detection probabilities and expected abundance.\nObservation model\nObservations at site \\(j\\) in survey \\(k\\) yield counts of the number of unique individuals detected, denoted \\(y_{j, k}\\) for all \\(j\\) and all \\(k\\).\nAssuming that the detection of each individual is conditionally independent, and that each individual is detected with site-specific probability \\(p_j\\), the observations can be modeled with a Binomial distribution where the number of trials is the true (latent) population abundance \\(n_j\\):\n\\[y_{j, k} \\sim \\text{Binomial}(p_j, n_j).\\]\nProcess model\nThe true population abundance \\(n_j\\) is treated as a Poisson random variable with expected value \\(\\lambda_j\\):\n\\[n_j \\sim \\text{Poisson}(\\lambda_j).\\]\nParameter model\nHeterogeneity among sites was accounted for using a single layer neural network that ingests the one dimensional covariate \\(x_i\\) for site \\(i\\), passes it through a single hidden layer, and outputs a two dimensional vector containing a detection probability \\(p_i\\) and the expected abundance \\(\\lambda_i\\):\n\\[\n\\begin{bmatrix}\n   \\lambda_i \\\\\n   p_i\n\\end{bmatrix} = f(x_i),\n\\]\nwhere \\(f\\) is a neural network with two dimensional output activations \\(\\vec{h}(x_i)\\) computed via:\n\\[\\vec{h}(x_i) = \\vec{W}^{(2)} g(\\vec{W}^{(1)} x_i ),\\]\nand final outputs computed using the log and logit link functions for expected abundance and detection probability:\n\\[f(x_i) = \\begin{bmatrix}\n   \\text{exp}(h_1(x_i)) \\\\\n   \\text{logit}^{-1}(h_2(x_i))\n\\end{bmatrix}.\\]\nHere too \\(\\vec{W}^{(1)}\\) is a parameter matrix that generates activations from the inputs, \\(g\\) is an activation function, and \\(\\vec{W}^{(2)}\\) is a parameter matrix that maps the hidden layer to the outputs.\nAdditionally \\(h_1(x_i)\\) is the first element of the output activation vector, and \\(h_2(x_i)\\) the second element.\nLoss function\nThe negative log likelihood was used as the loss function, enumerating over a large range of potential values of the true abundance (from \\(\\min(y_j.)\\) to \\(5 \\times \\max(y_j.)\\), where \\(y_{j.}\\) is a vector of counts of length \\(K\\)) to approximate the underlying infinite mixture model implied by the Poisson model of abundance (Royle 2004).\n\n\n\nSimulating some data\nFirst, load some python dependencies.\n\nimport matplotlib.pyplot as plt\nimport multiprocessing\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.distributions import Binomial, Poisson\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\nSimulate data at nsite sites, with nrep repeat surveys.\nHere it’s assumed that there is one continuous site-level covariate \\(x\\) that has some nonlinear relationship with the expected number of individuals at a site.\n\nnp.random.seed(123456)\nnsite = 200\nnrep = 5\nx = np.linspace(-2.5, 2.5, nsite, dtype=np.float32).reshape(-1,1)\n\n# Draw f(x) from a Gaussian process\ndef kernel(x, theta):\n    m, n = np.meshgrid(x, x)\n    sqdist = abs(m-n)**2\n    return np.exp(- theta * sqdist)\n\nK = kernel(x, theta=.2)\nL = np.linalg.cholesky(K + 1e-5* np.eye(nsite))\nf_prior = np.dot(L, np.random.normal(size=(nsite, 1)))\n\n\nGenerate some abundance values from a Poisson distribution:\n\noffset = 3\nlam = np.exp(f_prior + offset)\nn = np.random.poisson(lam)\n\nplt.scatter(x, n, c='k', alpha=.3)\nplt.plot(x, lam)\nplt.xlabel('Covariate value')\nplt.ylabel('True (latent) abundance')\n\n\nFigure 2: True relationship between latent abundance and the covariate, with sampled points.\n\n\n\nFor simplicity, assume that the probability of detection is constant across all sites and independent of \\(x\\).\n\npr_detection = np.array([0.5])\ny = np.random.binomial(n=n, \n                       p=pr_detection, \n                       size=(nsite, nrep))\n\nplt.plot(x, lam)\nfor i in range(nrep):\n    plt.scatter(x, y[:, i], c='b', s=4, alpha=.3)    \nplt.xlabel('Covariate value')\nplt.ylabel('Observed counts')\n\n\n(#fig:gen_data)Observed counts as a function of the covariate value.\n\n\n\nDefining a neural network\nWe will define a torch.nn.Module class for our neural network.\nThis ingests \\(x\\) and outputs a value for \\(\\lambda\\) and \\(p\\):\n\nclass Net(nn.Module):\n    \"\"\" Neural N-mixture model \n    \n    This is a neural network that ingests x and outputs:\n    - lam(bda): expected abundance\n    - p: detection probability\n    \"\"\"\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.fc1 = nn.Linear(1, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, 2)\n\n    def forward(self, x):\n        hidden_layer = torch.sigmoid(self.fc1(x))\n        output = self.fc2(hidden_layer)\n        lam = torch.exp(output[:, [0]])\n        p = torch.sigmoid(output[:, [1]])\n        return lam, p\n\n\nDefining a loss function\nWe will use the negative log likelihood as our loss function:\n\ndef nmix_loss(y_obs, lambda_hat, p_hat, n_max):\n    \"\"\" N-mixture loss.\n    \n    Args:\n      y_obs (tensor): nsite by nrep count observation matrix\n      lambda_hat (tensor): poisson abundance expected value\n      p_hat (tensor): individual detection probability\n      n_max (int): maximum abundance to consider\n    \n    Returns:\n      negative log-likelihood (tensor)\n    \"\"\"\n    batch_size, n_rep = y_obs.shape\n    \n    possible_n_vals = torch.arange(n_max).unsqueeze(0)\n    n_logprob = Poisson(lambda_hat).log_prob(possible_n_vals)\n    assert n_logprob.shape == (batch_size, n_max)\n    \n    y_dist = Binomial(\n      possible_n_vals.view(1, 1, -1), \n      probs=p_hat.view(-1, 1, 1), \n      validate_args=False\n    )\n    y_obs = y_obs.unsqueeze(-1).repeat(1, 1, n_max)\n    y_logprob = y_dist.log_prob(y_obs).sum(dim=1) # sum over repeat surveys\n    assert y_logprob.shape == (batch_size, n_max)\n    \n    log_lik = torch.logsumexp(n_logprob + y_logprob, -1)\n    return -log_lik\n\n\nPreparing to train\nInstantiate a model.\n\nnet = Net(hidden_size=32)\nnet\n\n\nCreate a data loader.\n\ndataset = TensorDataset(torch.tensor(x).float(), torch.tensor(y))\ndataloader = DataLoader(dataset, \n                        batch_size=16,\n                        shuffle=True, \n                        num_workers=multiprocessing.cpu_count())\n\n\nInstantiate an optimizer and choose the number of training epochs:\n\nn_epoch = 1000\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01, weight_decay=1e-6)\nrunning_loss = []\n\n\nTraining the model\nFinally, train the model, visualizing the estimated relationship between \\(x\\) and \\(N\\) after every gradient update.\n\n_ = plt.scatter(x, n, c='k')\n_ = plt.xlabel('Covariate value')\n_ = plt.ylabel('Abundance')\ncolors = plt.cm.viridis(np.linspace(0,1,n_epoch))\nfor i in range(n_epoch):\n    for i_batch, xy in enumerate(dataloader):\n        x_i, y_i = xy\n        optimizer.zero_grad()\n        lambda_i, p_i = net(x_i)\n        nll = nmix_loss(y_i, lambda_i, p_i, n_max = 200)\n        loss = torch.mean(nll)        \n        loss.backward()\n        optimizer.step()    # Does the update\n        running_loss.append(loss.data.detach().numpy())\n    lam_hat, p_hat = net(torch.from_numpy(x))\n    lam_hat = lam_hat.detach().numpy()\n    _ = plt.plot(x, lam_hat, color=colors[i], alpha=.1)\nplt.show()\n\n\nFigure 3: Estimated relationships between x and abundance as training progresses. Dark blue lines represent predictions from early training iterations, and green/yellow represent middle/late training iterations.\n\n\n\n\nFigure 3: Estimated relationships between x and abundance as training progresses. Dark blue lines represent predictions from early training iterations, and green/yellow represent middle/late training iterations.\n\n\n\nVisualize loss:\n\nn_step = len(running_loss)\n_ = plt.scatter(x=np.arange(n_step), y=running_loss, s=2,\n                color=plt.cm.viridis(np.linspace(0,1,n_step)))\n_ = plt.xlabel('Number of training iterations')\n_ = plt.ylabel('N-mixture loss')\nplt.show()\n\n\nFigure 4: Training loss over time. Each point corresponds to the loss after a gradient update.\n\n\n\n\nFigure 4: Training loss over time. Each point corresponds to the loss after a gradient update.\n\n\n\nMore on implementing hierarchical models\nIf you are interested in digging into the details of how to build these models, check out the companion repository on GitHub, which has all of the code required to reproduce the paper, as well as links to Jupyter Notebooks (thanks Binder!) to play with some toy occupancy, capture-recapture, and N-mixture models: https://github.com/mbjoseph/neuralecology\nParting thoughts\nDeep learning is somewhat of a mystery to many ecologists.\nThose who are currently applying it have done some amazing things (like counting plants and animals in imagery).\nBut, I worry that as a community, ecologists are thinking about deep learning too narrowly.\nWe can look to hydrology and physics to get a sense for how we can advance science with deep learning.\nHere are a few key papers that are shaping my thinking around this topic, which might help motivate future work in science-based deep learning for ecology:\nKarpatne, Anuj, et al. “Physics-guided neural networks (pgnn): An application in lake temperature modeling.” arXiv preprint arXiv:1710.11431 (2017). link\nRaissi, Maziar. “Deep hidden physics models: Deep learning of nonlinear partial differential equations.” The Journal of Machine Learning Research 19.1 (2018): 932-955. link\nRangapuram, Syama Sundar, et al. “Deep state space models for time series forecasting.” Advances in neural information processing systems. 2018. link\nRackauckas, Christopher, et al. “Universal Differential Equations for Scientific Machine Learning.” arXiv preprint arXiv:2001.04385 (2020). link\n\n\n\nJoseph, Maxwell B. 2020. “Neural Hierarchical Models of Ecological Populations.” Ecology Letters in press. https://doi.org/10.1111/ele.13462.\n\n\nRoyle, J Andrew. 2004. “N-Mixture Models for Estimating Population Size from Spatially Replicated Counts.” Biometrics 60 (1): 108–15.\n\n\n\n\n",
    "preview": "posts/2020-01-13-neural-hierarchical-models/neural-hierarchical-models_files/figure-html5/train-19.png",
    "last_modified": "2022-11-05T22:18:00-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-30-yes-but-does-it-still-run/",
    "title": "Yes, but does it (still) run?",
    "description": "Migrating from Jekyll to distill, with some reflections on the past 6 years.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2018-12-30",
    "categories": [],
    "contents": "\nI haven’t blogged recently.\nIt was useful as a PhD student to wrap my head around new\nmethods and track my path from a code-naive field ecologist to a\nnot entirely incompetent R programmer in 2016 when I graduated.\nBut, much has changed in the 5-6 years since I began the blog.\nI’ve developed new skills, and the tools around R programming have matured.\nIt’s easier now to publish content with R markdown\nwith tools like blogdown and\ndistill than it was six years ago.\nMy old workflow involved writing posts in R markdown, then generating markdown\nand publishing with a static site generator (first\nOctopress, then\nJekyll).\nThis is fine, but I ended up losing track of most of the original R markdown\nfiles that generated the markdown being served on the site.\nLooking back on 2018, I’ve come to better appreciate\ncontinuous integration through Travis CI, CircleCI, and AppVeyor.\nIt’s nice to know when builds break and code stops working.\nAs I was taking stock of the past year, I realized that I did not know whether\nthe code that I had posted years ago still worked, and I didn’t have the .Rmd\nfiles anymore to find out.\n\n\n\nFigure 1: An abandoned car that may have been useful in its time, but no longer runs.\n\n\n\nThis bothered me.\nFirst, I didn’t want to be responsible for publishing code that doesn’t run.\nSecond, I also realized that years ago, I did not appreciate\nportability as much as I do today, e.g.,\nwhen I felt comfortable assuming that users would need to manually download\ndata from a website and store it in a specific directory on their filesystem\nto run some code.\nWith time off over the holidays, I figured I would migrate my old site from\nJekyll to distill and put the resulting site under continuous integration to\nbe sure that the answer to the question “does it (still) run?” is yes.\nThe result is this site (as of Dec 2018), and unsurprisingly, not all of the\ncode that I had posted previously still ran.\nBut, now with distill I have\nTravis CI building the posts\nregularly, so at least if something breaks in the future, I should know sooner.\n\n\n\n",
    "preview": "posts/2018-12-30-yes-but-does-it-still-run/yes-but-does-it-still-run_files/figure-html5/plot-img-1.png",
    "last_modified": "2022-11-01T21:57:17-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-gaussian-predictive-process-models-in-stan/",
    "title": "Gaussian predictive process models in Stan",
    "description": "Gaussian processes that scale to larger data.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2016-08-14",
    "categories": [
      "stan",
      "spatial"
    ],
    "contents": "\nGaussian process (GP) models are computationally demanding for large datasets.\nMuch work has been done to avoid expensive matrix operations that arise in parameter estimation with larger datasets via sparse and/or reduced rank covariance matrices (Datta et al. 2016 provide a nice review).\nWhat follows is an implementation of a spatial Gaussian predictive process Poisson GLM in Stan, following Finley et al. 2009: Improving the performance of predictive process modeling for large datasets, with a comparison to a full rank GP model in terms of execution time and MCMC efficiency.\nGenerative model\nAssume we have \\(n\\) spatially referenced counts \\(y(s)\\) made at spatial locations \\(s_1, s_2, ..., s_n\\), which depend on a latent mean zero Gaussian process with an isotropic stationary exponential covariance function:\n\\[y(s) \\sim \\text{Poisson}(\\text{exp}(X^T(s) \\beta + w(s)))\\]\n\\[w(s) \\sim GP(0, C(d))\\]\n\\[[C(d)]_{i, j} = \\eta^2 \\text{exp}(- d_{ij} \\phi)) + I(i = j) \\sigma^2\\]\nwhere \\(y(s)\\) is the response at location \\(s\\), \\(X\\) is an \\(n \\times p\\) design matrix, \\(\\beta\\) is a length \\(p\\) parameter vector, \\(\\sigma^2\\) is a “nugget” parameter, \\(\\eta^2\\) is the variance parameter of the Gaussian process, \\(d_{ij}\\) is a spatial distance between locations \\(s_i\\) and \\(s_j\\), and \\(\\phi\\) determines how quickly the correlation in \\(w\\) decays as distance increases.\nFor simplicity, the point locations are uniformly distributed in a 2d unit square spatial region.\nTo estimate this model in a Bayesian context, we might be faced with taking a Cholesky decomposition of the \\(n \\times n\\) matrix \\(C(d)\\) at every iteration in an MCMC algorithm, a costly operation for large \\(n\\).\nGaussian predictive process representation\nComputational benefits of Gaussian predictive process models arise from the estimation of the latent Gaussian process at \\(m << n\\) locations (knots).\nInstead of taking the Cholesky factorization of the \\(n \\times n\\) covariance matrix, we instead factorize the \\(m \\times m\\) covariance matrix corresponding to the covariance in the latent spatial process among knots.\nKnot placement is a non-trivial topic, but for the purpose of illustration let’s place knots on a grid over the spatial region of interest.\nNote that here the knots are fixed, but it is possible to model knot locations stochastically as in Guhaniyogi et al. 2012.\n\n\n\nKnots are shown as stars and the points are observations.\nWe will replace \\(w\\) above with an estimate of \\(w\\) that is derived from a reduced rank representation of the latent spatial process.\nBelow, the vector \\(\\tilde{\\boldsymbol{\\epsilon}}\\) corrects for bias (underestimation of \\(\\eta\\) and overestimation of \\(\\sigma\\)) as an extension of Banerjee et al. 2008, and \\(\\mathcal{C}^T(\\theta) \\mathcal{C}^{*-1}(\\theta)\\) relates \\(w\\) at desired point locations to the value of the latent GP at the knot locations, where \\(\\mathcal{C}^T(\\theta)\\) is an \\(n \\times m\\) matrix that gets multiplied by the inverse of the \\(m \\times m\\) covariance matrix for the latent spatial process at the knots.\nFor a complete and general description see Finley et al. 2009, but here is the jist of the univariate model:\n\\[\\boldsymbol{Y} \\sim \\text{Poisson}(\\boldsymbol{X} \\beta + \\mathcal{C}^T(\\theta) \\mathcal{C}^{*-1}(\\theta)\\boldsymbol{w^*} + \\tilde{\\boldsymbol{\\epsilon}})\\]\n\\[\\boldsymbol{w^*} \\sim GP(0, \\mathcal{C}^*(\\theta))\\]\n\\[\\tilde{\\epsilon}(s) \\stackrel{indep}{\\sim} N(0, C(s, s) - \\mathcal{c}^T(\\theta) \\mathcal{C}^{*-1}(\\theta))\\mathcal{c}(\\theta)\\]\nwith priors for \\(\\eta\\), \\(\\sigma\\), and \\(\\phi\\) completing a Bayesian specification.\nThis approach scales well for larger datasets relative to a full rank GP model.\nComparing the number of effective samples per unit time for the two approaches across a range of sample sizes, the GPP executes more quickly and is more efficient for larger datasets.\nCode for this simulation is available on GitHub: https://github.com/mbjoseph/gpp-speed-test.\n\n\n\nRelevant papers\nFinley, Andrew O., et al. “Improving the performance of predictive process modeling for large datasets.” Computational statistics & data analysis 53.8 (2009): 2873-2884.\nBanerjee, Sudipto, et al. “Gaussian predictive process models for large spatial data sets.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 70.4 (2008): 825-848.\nDatta, Abhirup, et al. “Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets.” Journal of the American Statistical Association. Accepted (2015).\nGuhaniyogi, Rajarshi, et al. “Adaptive Gaussian predictive process models for large spatial datasets.” Environmetrics 22.8 (2011): 997-1007.\n\n\n\n",
    "preview": "posts/2018-12-27-gaussian-predictive-process-models-in-stan/gaussian-predictive-process-models-in-stan_files/figure-html5/plot-img2-1.png",
    "last_modified": "2022-11-01T21:55:44-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-the-five-elements-ninjas-approach-to-teaching-design-matrices/",
    "title": "The five elements ninjas approach to teaching design matrices",
    "description": "In-class activities to teach design matrices from multiple perspectives.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2016-04-25",
    "categories": [
      "teaching"
    ],
    "contents": "\nDesign matrices unite seemingly disparate statistical methods, including linear regression, ANOVA, multiple regression, ANCOVA, and generalized linear modeling.\nAs part of a hierarchical Bayesian modeling course that we offered this semester, we wanted our students to learn about design matrices to facilitate model specification and parameter interpretation.\nNaively, I thought that I could spend a few minutes in class reviewing matrix multiplication and a design matrix for simple linear regression, and if students wanted more, they might end up on Wikipedia’s Design matrix page.\nIt quickly became clear that this approach was not effective, so I started to think about how students could construct their own understanding of design matrices.\nAbout the same time, I watched a pretty incredible kung fu movie called Five Element Ninjas, and it occurred to me that the “five elements” concept could be an effective device for getting my students to think about model specification and design matrices.\nLearning goals\nStudents should be able to specify design matrices for many different types of models (e.g., linear models and generalized linear models), and they should be able to interpret the parameters.\nApproach\nThe broad idea was to get the students to think about model specification from five perspectives:\nModel specification via a design matrix\nModel specification via R syntax (e.g., the formula argument to lm)\nModel specification via “long form” equations\nGraphical model specification\nVerbal model specification (along with an interpretation of each of the parameter estimates)\nThis leverages what students already know, and encourages them to connect new concepts to their existing knowledge.\nIn our case, the students were all students in CU Boulder’s Ecology and Evolutionary Biology graduate program.\nMost of them had a strong grasp of perspective 2 (model specification in R syntax), but relatively weak understanding of the remaining perspectives.\nGetting the students started\nBefore we asked them to do anything, I demonstrated this five elements approach on a simple model: the model of the mean.\n1. Design matrix specification\n\\[y \\sim N(\\mu, \\sigma_y)\\]\n\\[\\mu = X \\beta\\]\n\\[X = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\\\ 1 \\end{bmatrix}\\]\n2. R syntax\nThe formula for a model of the mean is y ~ 1\n3. Long form equations\n\\[y_1 = \\beta + \\epsilon_1\\]\n\\[y_2 = \\beta + \\epsilon_2\\]\n\\[ \\vdots \\]\n\\[y_n = \\beta + \\epsilon_n\\]\n4. Graphical interpretation\n\n5. Verbal description\nI asked for a student to take a stab at a verbal description of the model specification, and also to explain the interpretation of the parameter \\(\\beta\\).\nIf they’re having a hard time understanding the task, you can tell them to pretend that they are talking to a classmate on the phone and trying to describe the model.\nThe activity\nWe provided students with a very simple data set that does not include the “response” variable.\nThis was printed ahead of time, so that each student had a paper copy that they could also use as scratch paper.\nCovariate 1\nCovariate 2\n1.0\nA\n2.0\nB\n3.0\nA\n4.0\nB\nThe omission of the response variable is deliberate, reinforcing the idea that one can construct a design matrix without knowing the outcome variable (this is useful later in our class for prior and posterior predictive simulations).\nWe organized the students into groups of three or four and had each group come up to the blackboard, which we partitioned ahead of time to have a space for each group to work.\nThen, we proceeded to work through incrementally more complex models with our five-pronged approach:\nA model that includes an effect of covariate 1.\nA model that includes an effect of covariate 2.\nA model that includes additive effects of covariate 1 and 2 (no interactions).\nA model that includes additive effects and an interaction between covariate 1 and 2.\nEach of these exercises took about 15 minutes, and once all the groups were done we checked in with each group as a class to see what they came up with.\nSome groups opted for effects parameterizations, while others opted for means parameterizations, which lead to a useful discussion of the default treatment of intercepts in R model formulas and the manual suppression of intercepts (e.g., y ~ 0 + x).\nThe outcome\nThis in-class activity was surprisingly well-received, and it seemed to provide the context and practice necessary for the students to understand design matrices on a deeper level.\nThroughout the rest of the semester, model matrices were preferred over other specifications by many of the students - a far cry from the confusion at the beginning of the semester.\n\n\n\nFigure 1: Katsushika Hokusai [Public domain]. Use different perspectives to improve concept delivery.\n\n\n\n\n\n\n",
    "preview": "posts/2018-12-27-the-five-elements-ninjas-approach-to-teaching-design-matrices/the-five-elements-ninjas-approach-to-teaching-design-matrices_files/figure-html5/plot-img-1.png",
    "last_modified": "2022-11-01T21:57:04-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-first-year-books/",
    "title": "First year books",
    "description": "10 books I wish I had entering graduate school.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2015-09-08",
    "categories": [
      "rants"
    ],
    "contents": "\nI had to read a lot of books in graduate school.\nSome were life-changing, and others were forgettable.\nIf I could bring a reading list back in time for my ‘first year’ graduate self, it would include the following:\nBayesian Data Analysis\nThird Edition, by Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin\nProbably the most useful book I’ve ever owned. Has staying power - can be used\nas a reference.\nThe Art of R Programming\nby Norman Matloff\nThis book made me less bad at programming in R early on.\nCausality\nSecond Edition, by Judea Pearl\nEcology is complicated. We often lack replicated controlled experiments with\nrandom treatment assignment. This book helped me organize my thinking around\nhow to translate mechanistic knowledge to statistical models.\nStatistics for Spatio-Temporal Data\nby Noel Cressie and Christopher Wikle\nA thoughtful treatment of hierarchical modeling in a spatial, temporal, and\nspatiotemporal context. Has breadth with a healthy dose of outside references\nfor depth. My dog ate this one, but it was great to have.\nEcological Models and Data in R\nby Benjamin M. Bolker\nCovers fundamental ideas about likelihood and process-oriented modeling while\nbuilding R proficiency.\nBayesian Models: A Statistical Primer for Ecologists\nby N. Thompson Hobbs & Mevin B. Hooten\nAn introduction to the process of model building and estimation for\nnon-math/stats oriented readers. Thoughtful treatment of notation, helped me to\nbetter understand how to communicate models.\nData Analysis Using Regression and Multilevel/Hierarchical Models\nby Andrew Gelman and Jennifer Hill\nA gentle introduction to multilevel modeling, with plenty of graphics and\nintegration with R.\nStatistical Inference\nSecond Edition, by George Casella and Roger L. Berger\nEssential for understanding the mathematical and probabilistic foundations of\nstatistics. Read it after brushing up on calculus. Checked this out from the\nlibrary, and my dog ate it. Had to buy a copy to replace, which was not cheap.\nBut, later found a pdf online.\nLinear Algebra\nby George Shilov\nI wish I had taken a class in linear algebra as an undergraduate, but I instead\nhad to catch up in my first year of grad school. This book made it relatively\npainless. Literally found it on the shelf of the office I moved into on day one.\nSingle and Multivariable Calculus\nby David Guichard and friends\nBecause I took a few calculus classes in high school and college and didn’t know\nwhy.\nMathematical Tools for Understanding Infectious Disease Dynamics\nby Odo Diekmann, Hans Heesterbeek & Tom Britton\nMathematical epidemiology is a huge topic. This book introduces common models\nand approaches from first principles, with plenty of problems along the way to\nmake sure you’re following along. Read it with a notebook and pencil handy.\n\n\n\nFigure 1: Credit: Wellcome Library, London. Wellcome Images images@wellcome.ac.uk http://wellcomeimages.org Plague treatise - a full page woodcut representing a man at a table studying a book, he is surrounded by four persons, - one holding a urine glass, Woodcut Darin durch sechs kurtzer Buchlin vil Heimlichkeiten der Natur beschriben werden Albertus Magnus Published: 1551\n\n\n\n\n\n\n",
    "preview": "posts/2018-12-27-first-year-books/first-year-books_files/figure-html5/plot-tape-1.png",
    "last_modified": "2022-11-01T21:55:41-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-the-iquit-r-video-series/",
    "title": "The IQUIT R video series",
    "description": "A series of introductory R videos.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2015-08-28",
    "categories": [
      "teaching"
    ],
    "contents": "\nI’ve uploaded 20+ R tutorials to YouTube for a new undergraduate course in\nEcology and Evolutionary Biology at CU developed by\nAndrew Martin and\nBrett Melbourne, which in\njocular anticipation was named IQUIT: an introduction to quantitative inference\nand thinking.\nWe made the videos to address the most common R programming problems that arose\nfor students in the first iteration of the course.\nThese short tutorials may be of use elsewhere:\nIntroduction to R\neverything is an object\naddition, subtraction, multiplication\nassignment\nNumeric vectors: 1\nvectors vs. scalars\ncreate vectors with c()\nNumeric vectors: 2\nhow to explore the structure of a vector\nclass, length, str\nFunctions in R\ninput and output\nsingle argument functions: sqrt, log, exp\nmulti-argument functions: round\nCreating special vectors: sequences and repetition\ngenerate integer sequence: :\ncreate sequence seq (hit args)\nrepeat something rep (also note argument structure)\nRelational operators and logical data types\nlogical types (intro to relational operators)\n==, !=, >, <, >=, <=\nTRUE and FALSE\nCharacter data\ncharacter objects\ncharacter vectors\nrelational operators on character vectors\n2-d data structures: matrices and data frames\ndata frames can hold lots of different data types\nmatrix elements must be of the same type\nIntro to indexing: matrices and vectors\nindexing and subsetting with [\nreview str\na bit with relational operators\nData frame subsetting and indexing\nindexing with relational operators\n3 ways to subset data frame: df[c(\"column names\")], df$column, df[, 1]\nR style & other secrets to happiness\nbasics of R style: spacing, alignment,\nbreaking up run-on lines\nworkspace management\nls, rm\nchoosing good names for files and objects\ncommenting\nWorking with data in R: 1\nreading in data with read.csv\nautomatic conversion of missing values to NA\nWorking with data in R: 2\nmixed type errors (numbers read in as characters because one cell has a letter)\nsearch path errors\nis.na\nVisualization part 1: intro to plot()\nplot\narguments: xlab, ylab, col\nVisualization part 2: other types of plots\nhistograms, jitter plots, line graphs\nVisualization part 3: adding data to plots\nadding points\nadding lines, and segments (also abline)\nVisualization part 4: annotation and legends\nannotation via text\nadding legends\nVisualization part 5: graphical parameters\ncommonly used parameters\nfor points: col, cex, pch (see ?points for pch options)\nfor lines: col, lwd, lty\nLooping repetitive tasks\nthe power of the for loop\ncreating objects to hold results ahead of time, rather than growing objects\nSummarizing data\nmean, sd, var, median\nRandomization & sampling distributions\nsample and rep\nDebugging R code 1: letting R find your data\nworking directory errors when reading in data\nproblems with typos, using objects that don’t exist\nDebugging R code 2: unreported errors\nerrors do not always bring error messages\nsteps to finding & fixing errors\nReplication and sample size\nexplore the effect of n on the uncertainty in a sample mean\nConveying uncertainty with confidence intervals while not obscuring the data\nconstructing confidence intervals\nplot CIs using the segments function\nDifferences in means\ngiven two populations, simulate the null sampling distribution of the difference in means\nrandomly assign individuals to a group using sample or some other scheme, then iteratively simulate differences in means with CIs\n\n\n\nFigure 1: Toby Hudson [CC BY-SA 2.5 au (https://creativecommons.org/licenses/by-sa/2.5/au/deed.en)], from Wikimedia Commons\n\n\n\n\n\n\n",
    "preview": "posts/2018-12-27-the-iquit-r-video-series/the-iquit-r-video-series_files/figure-html5/plot-tape-1.png",
    "last_modified": "2022-11-01T21:57:08-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-plotting-spatial-neighbors-in-ggplot2/",
    "title": "Plotting spatial neighbors in ggplot2",
    "description": "How to visualize spatial neighbors using ggplot2, spdep, and sf.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2015-06-15",
    "categories": [
      "visualization",
      "spatial"
    ],
    "contents": "\nThe R package spdep\nhas great utilities to define spatial neighbors (e.g. dnearneigh,\nknearneigh, with a nice\nvignette to\nboot), but the plotting functionality is aimed at base graphics.\nSo, to save others some trouble, I thought I’d share a little snippet to\nconvert a spatial neighbors object (of class nb) to an sf data frame.\n\n\nlibrary(sf)\nlibrary(spdep)\nlibrary(ggplot2)\n\nfname <- system.file(\"shape/nc.shp\", package=\"sf\")\nnc <- st_read(fname, quiet = TRUE)\n\nnc_sp <- as(nc, 'Spatial')\nneighbors <- poly2nb(nc_sp)\nneighbors_sf <- as(nb2lines(neighbors, coords = coordinates(nc_sp)), 'sf')\nneighbors_sf <- st_set_crs(neighbors_sf, st_crs(nc))\n\nggplot(nc) + \n  geom_sf(fill = 'salmon', color = 'white') +\n  geom_sf(data = neighbors_sf) +\n  theme_minimal() +\n  ylab(\"Latitude\") +\n  xlab(\"Longitude\")\n\n\n\n\n\n\n",
    "preview": "posts/2018-12-27-plotting-spatial-neighbors-in-ggplot2/plotting-spatial-neighbors-in-ggplot2_files/figure-html5/plot-neighbors-1.png",
    "last_modified": "2022-11-01T21:55:57-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-why-i-think-twice-before-editing-plots-in-powerpoint-illustrator-inkscape-etc/",
    "title": "Why I think twice before editing plots in Powerpoint, Illustrator, Inkscape, etc.",
    "description": "TLDR: scripting plots is more reproducible and efficient long term",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2015-02-26",
    "categories": [
      "visualization",
      "rants"
    ],
    "contents": "\nThanks to a nice post by Meghan Duffy on the Dynamic Ecology blog\n(How do you make figures?),\nwe have some empirical evidence that many figures made in R by ecologists are\nsecondarily edited in other programs including MS Powerpoint, Adobe Illustrator,\nInkscape, and Photoshop.\nI do not do this for two reasons: reproducibility and bonus learning.\nReproducibility\nR is nice because results are relatively easy to reproduce.\nIt’s free, and your code serves as a written record of what was done.\nWhen figures are edited outside of R, they can be much more difficult to\nreproduce.\nIndependent of whether I am striving to maximize the reproducibility of my work\nfor others, it behooves me to save time for my future self, ensuring that we\n(I?) can quickly update my own figures throughout the process of paper writing,\nsubmission, rewriting, resubmission, and so on.\nI had to learn this the hard way.\nThe following figure was my issue: initially I created a rough version in R,\nedited it in Inkscape (~30 minutes invested), and ended up with a\n“final” version for\nsubmission.\n\n\n\nFigure 1: Figure from Does life history mediate changing disease risk when communities disassemble? (Joseph et al. 2013)\n\n\n\nTurns out that I had to remake the figure three times throughout the revision\nprocess (for the better). Eventually I realized I should\nto make the plot in R than to process it outside of R.\nIn retrospect, two things are clear:\nMy energy allocation strategy was not conducive to the revision process. I\nwasted time trying to make my “final” version look good in Inkscape, when I\ncould have invested time to figure out how to make the figure as I wanted it in\nR. The payoff from this time investment will be a function of how much\nmanipulation is done outside R, how hard it is to get the desired result in R,\nand how many times a figure will be re-made.\nI probably could have found a better way to display the data. Another post\nperhaps.\nLearning\nForcing myself to remake the figure exactly as I wanted it using only R had an\nunintended side effect: I learned more about base graphics in R.\nNow, when faced with similar situations, I can make similar plots much faster,\nbecause I know more graphical parameters and plotting functions.\nIn contrast, point-and-click programs are inherently slow because I’m manually\nmanipulating elements, usually with a mouse, and my mouse isn’t getting any\nfaster.\n\n\n\nJoseph, Maxwell B, Joseph R Mihaljevic, Sarah A Orlofske, and Sara H Paull. 2013. “Does Life History Mediate Changing Disease Risk When Communities Disassemble?” Ecology Letters 16 (11): 1405–12. https://doi.org/10.1111/ele.12180.\n\n\n\n\n",
    "preview": "posts/2018-12-27-why-i-think-twice-before-editing-plots-in-powerpoint-illustrator-inkscape-etc/why-i-think-twice-before-editing-plots-in-powerpoint-illustrator-inkscape-etc_files/figure-html5/plot-tape-1.png",
    "last_modified": "2022-11-01T21:57:12-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-notes-on-shrinkage-and-prediction-in-hierarchical-models/",
    "title": "Notes on shrinkage and prediction in hierarchical models",
    "description": "Partial pooling and the best NBA free throw shooters of all time.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2014-12-13",
    "categories": [
      "teaching"
    ],
    "contents": "\nEcologists increasingly use mixed effects models, where some intercepts or\nslopes are fixed, and others are random (or varying).\nOften, confusion exists around whether and when to use fixed vs. random\nintercepts/slopes, which is understandable given their\nmultiple definitions.\nIn an attempt to help clarify the utility of varying intercept models (and more\ngenerally, hierarchical modeling), specifically in terms of shrinkage and\nprediction, here is a\nGitHub repo with materials\nand a slideshow from our department’s graduate QDT (quantitative (th)ink tank)\ngroup.\nFor fun, I’ve included a toy\nexample\ndemonstrating the value of shrinkage when trying to rank NBA players by their\nfree throw shooting ability, a situation with wildly varying amounts of\ninformation (free throw attempts) on each player.\n\n\n\nFigure 1: Kobe_Bryant_7144.jpg: Sgt. Joseph A. Leederivative work: JoeJohnson2 [Public domain], via Wikimedia Commons\n\n\n\nThe example admittedly is not ecological, and sensitive readers may replace free\nthrow attempts with prey capture attempts for topical consistency.\nMany if not most ecological datasets suffer from similar issues, with varying\namounts of information from different sites, species, individuals, etc., so\neven without considering predation dynamics of NBA players, the example’s\nrelevance should be immediate.\n\n\n\n",
    "preview": "posts/2018-12-27-notes-on-shrinkage-and-prediction-in-hierarchical-models/notes-on-shrinkage-and-prediction-in-hierarchical-models_files/figure-html5/plot-tape-1.png",
    "last_modified": "2022-11-01T21:55:52-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-dynamic-occupancy-models-in-stan/",
    "title": "Dynamic occupancy models in Stan",
    "description": "Dynamic multi-year occupancy models, marginalizing over latent occurrence states.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2014-11-14",
    "categories": [
      "stan"
    ],
    "contents": "\nOccupancy modeling is possible in Stan\nas shown here,\ndespite the lack of support for integer parameters (without marginalization).\nIn many Bayesian applications of occupancy modeling, the true occupancy states\n(0 or 1) are directly modeled, but this can be avoided by marginalizing out the\ntrue occupancy state.\nThe Stan manual (pg. 96) gives an example of\nthis kind of marginalization for a discrete change-point model.\nFor a Stan implementation of a dynamic (multi-year) occupancy model\n(MacKenzie et al. 2003), see: https://github.com/stan-dev/example-models/tree/master/BPA/Ch.13\n\n\n\nMacKenzie, Darryl I, James D Nichols, James E Hines, Melinda G Knutson, and Alan B Franklin. 2003. “Estimating Site Occupancy, Colonization, and Local Extinction When a Species Is Detected Imperfectly.” Ecology 84 (8): 2200–2207. https://doi.org/10.1890/02-3090.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-11-01T21:55:37-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-categorical-spatial-data-extraction-around-buffered-points-in-r/",
    "title": "Categorical spatial data extraction around buffered points in R",
    "description": "Computing the proportion of land cover types using R and the raster package.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2014-11-08",
    "categories": [
      "spatial"
    ],
    "contents": "\nQuantifying categorical spatial data (e.g. land cover) around points can be done\nin a variety of ways, some of which require considerable amounts of patience,\nclicking around, and/or cash for a license.\nHere’s a bit of code that I cobbled together to quickly extract land cover data\nfrom the National Land Cover Database for\na buffered point between Denver and Boulder - two cities in the state of\nColorado.\nFirst, get data using the FedData package:\n\n\nlibrary(knitr)\nlibrary(raster)\nlibrary(FedData)\nlibrary(rgdal)\n\nsite_df <- data.frame(city = c('Denver', 'Boulder'), \n                      lat = c(39.7392, 40.0150), \n                      lon = c(-104.9903, -105.2705))\nmidpoint <- data.frame(lat = mean(site_df$lat), \n                       lon = mean(site_df$lon))\n\n# create spatial point data frame\ncoordinates(site_df) <- ~lon + lat\nproj4string(site_df) <- CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84\")\n\ncoordinates(midpoint) <- ~lon + lat\nproj4string(midpoint) <- CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84\")\n\nnlcd_raster <- get_nlcd(site_df, \n                        label = 'categorial-extraction', \n                        year = 2011, \n                        extraction.dir = '.')\n\n# reproject midpoint to raster's crs\nmidpoint <- spTransform(midpoint, projection(nlcd_raster))\n\nbuffer_distance_meters <- 5000\n\n# visualize buffered point and land cover data\nbuff_shp <- buffer(midpoint, buffer_distance_meters)\nplot(buff_shp) # I will plot over this, but it sets the plotting extent\nplot(nlcd_raster, add = TRUE)\nplot(buff_shp, add = TRUE)\n\n\n\nNow, we can use raster::extract to extract raster data around our point\nwithin some buffer, specified in meters.\n\n\nlandcover <- extract(nlcd_raster, midpoint, buffer = buffer_distance_meters)\n\n\nBut this object does not immediately provide the proportions of each cover type.\nInstead, it contains values from the cells within the buffer:\n\n\nstr(landcover)\n\nList of 1\n $ : num [1:87274] 22 21 21 21 22 22 21 21 21 21 ...\n\nWe can get the proportions of each class within the buffer as follows:\n\n\nlandcover_proportions <- lapply(landcover, function(x) {\n  counts_x <- table(x)\n  proportions_x <- prop.table(counts_x)\n  sort(proportions_x)\n  })\nsort(unlist(landcover_proportions))\n\n          43           42           41           31           82 \n8.020716e-05 1.604143e-04 1.294773e-03 2.406215e-03 3.781195e-03 \n          81           90           95           24           52 \n5.202007e-03 1.184774e-02 1.911222e-02 3.251828e-02 6.305429e-02 \n          11           21           23           22           71 \n8.288837e-02 1.150056e-01 1.328804e-01 1.734766e-01 3.562917e-01 \n\nResources\nLarge .img file processing in R (GIS) on Stack Overflow by Israel Del Toro\nNLCD website\n\n\n\n",
    "preview": "posts/2018-12-27-categorical-spatial-data-extraction-around-buffered-points-in-r/categorical-spatial-data-extraction-around-buffered-points-in-r_files/figure-html5/get-data-1.png",
    "last_modified": "2022-11-01T21:55:35-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-multilevel-modeling-of-community-composition-with-imperfect-detection/",
    "title": "Multilevel modeling of community composition with imperfect detection",
    "description": "A guest post by Joe Mihaljevic.",
    "author": [
      {
        "name": "Joseph Mihaljevic",
        "url": {}
      }
    ],
    "date": "2014-06-19",
    "categories": [
      "jags"
    ],
    "contents": "\nThis is a guest post generously provided by Joe Mihaljevic.\nA common goal of community ecology is to understand how and why species\ncomposition shifts across space. Common techniques to determine which\nenvironmental covariates might lead to such shifts typically rely on ordination\nof community data to reduce the amount of data. These techniques include\nredundancy analysis (RDA), canonical correspondence analysis (CCA), and\nnonmetric multi-dimensional scaling (NMDS), each paired with permutation tests.\nHowever, these ordination techniques do not discern species-level covariate\neffects, making it difficult to attribute community-level pattern shifts to\nspecies-level changes (Jackson et al. 2012). Jackson et al. (2012) propose a\nhierarchical modeling framework as an alternative, which we extend in this post\nto correct for imperfect detection.\nMultilevel models can estimate species-level random and fixed covariate effects\nto determine the relative contribution of environmental covariates to changing\ncomposition across space (Jackson et al. 2012). For presence/absence data, such\nmodels are often formulated as:\n\\[y_{q} \\sim \\text{Bernoulli}(\\psi_{q})\\]\n\\[\\psi_{q} = logit^{-1}(\\alpha_{spp[q]} + b_{spp[q]}  x_{site[q]})\\]\n\\[\\alpha_{spp[q]} \\sim N(\\mu_{\\alpha}, \\sigma_{intercept}^2)\\]\n\\[b_{spp[q]} \\sim N(\\mu_{b}, \\sigma_{slope}^2)\\]\nHere \\(y_q\\) is a vector of presences/absence of each species at each site\n(\\(q=1, ... , nm,\\) where \\(n\\) is the number of species and \\(m\\) the number of\nsites). This model can be extended to incorporate multiple covariates.\nWe are interested in whether species respond differently to environmental\ngradients (e.g. elevation, temperature, precipitation). If this is the case,\nthen we expect community composition changes along such gradients. Concretely,\nwe are interested in whether \\(\\sigma_{slope}^2\\) for any covariate differs from\nzero.\nJackson et al. (2012) provide code for a maximum likelihood implementation of\ntheir model with data from Southern Appalachian understory herbs using the\nR package lme4.\nHere we present a simple extension of Jackson and colleague’s work, correcting\nfor detection error with repeat surveys (i.e. multi-species occupancy modeling).\nSpecifically, the above model could be changed slightly to:\n\\[y_{q} \\sim \\text{Binomial}(z_q p_{spp[q]}, j_q)\\]\n\\[z_q \\sim \\text{Bernoulli}(\\psi_q)\\]\n\\[\\psi_{q} = logit^{-1}(\\alpha_{spp[q]} + b_{spp[q]}  x_{site[q]})\\]\n\\[\\alpha_{spp[q]} \\sim N(\\mu_{\\alpha}, \\sigma_{intercept}^2)\\]\n\\[b_{spp[q]} \\sim N(\\mu_{b}, \\sigma_{slope}^2)\\]\nNow \\(y_q\\) is the number of times each species is observed at each site over \\(j\\)\nsurveys. \\(p_{spp[q]}\\) represents the species-specific probability of detection\nwhen the species is present, and \\(z_q\\) represents the ‘true’ occurence of the\nspecies, a Bernoulli random variable with probability, \\(\\psi_q\\).\nTo demonstrate the method, we simulate data for a 20 species community across\n100 sites with 4 repeat surveys. We assume that three site-level environmental\ncovariates were measured, two of which have variable affects on occurrence\nprobabilities (i.e. random effects), and one of which has consistent effects for\nall species (i.e. a fixed effect). We also assumed that species-specific\ndetection probabilities varied, but were independent of environmental\ncovariates.\n\n\nlibrary(reshape2)\nlibrary(ggplot2)\n\n################################################\n# Simulate data\n################################################\n\nNsite <- 100\nNcov <- 3\nNspecies <- 20\nJ <- 4\nset.seed(1234)\n\n# species-specific intercepts:\nalpha <- rnorm(Nspecies, 0, 1)\n\n# covariate values\nXcov <- matrix(rnorm(Nsite*Ncov, 0, 2),\n               nrow=Nsite, ncol=Ncov)\n\n# I'll assume 2 of the 3 covariates have effects that vary among species\nBeta <- array(c(rnorm(Nspecies, 0, 2),\n                rnorm(Nspecies, -1, 1),\n                rep(1, Nspecies)\n                ),\n              dim=c(Nspecies, Ncov)\n              )\n\n# species-specific detection probs\np0 <- plogis(rnorm(Nspecies, 1, 0.5))\n\n#### Occupancy states ####\nYobs <- array(0, dim = c(Nspecies, Nsite)) # Simulated observations\n\nfor(n in 1:Nspecies){\n  for(k in 1:Nsite){\n    lpsi <- alpha[n] + Beta[n, ] %*% Xcov[k, ] # Covariate effects on occurrence\n    psi <- 1/(1+exp(-lpsi)) #anti-logit\n\n    z <- rbinom(1, 1, psi) # True Occupancy\n    Yobs[n, k] <- rbinom(1, J, p0[n] * z) # Observed Occupancy\n  }\n}\n\n################################################\n# Format data for model\n################################################\n# X needs to have repeated covariates for each species, long form\nX <- array(0, dim=c(Nsite*Nspecies, Ncov))\nt <- 1; i <- 1\nTT <- Nsite\nwhile(i <= Nspecies){\n  X[t:TT, ] <- Xcov\n  t <- t+Nsite\n  TT <- TT + Nsite\n  i <- i+1\n}\n\n# Species\nSpecies <- rep(c(1:Nspecies), each=Nsite)\n\n# Observations/data:\nY <- NULL\nfor(i in 1:Nspecies){\n  Y <- c(Y, Yobs[i, ])\n}\n\n# All sites surveyed same # times:\nJ <- rep(J, times=Nspecies*Nsite)\n\n# Number of total observations\nNobs <- Nspecies*Nsite\n\n\nEach species has a coefficient for each covariate that describes how the\nprobability of occurrence responds. In one case, all species have the same\nresponse:\n\n\nbeta_df <- melt(Beta, varnames = c('Species', 'Covariate'))\nggplot(beta_df) + \n  geom_point(aes(value, Species)) + \n  facet_wrap(~Covariate, nrow = 1) + \n  xlab('Coefficient value') + \n  ggtitle('Species-level coefficients for each covariate (facets)') + \n  theme_minimal()\n\n\n\nWe fit the following model with JAGS with vague priors.\n\nmodel {\n  # Priors\n  psi.mean ~ dbeta(1,1)\n  p.detect.mean ~ dbeta(1,1)\n\n  sd.psi ~ dunif(0,10)\n  psi.tau <- pow(sd.psi, -2)\n\n  sd.p.detect ~ dunif(0,10)\n  p.detect.tau <- pow(sd.p.detect, -2)\n\n  for(i in 1:Nspecies){\n    alpha[i] ~ dnorm(logit(psi.mean), psi.tau)T(-12,12)\n    lp.detect[i] ~ dnorm(logit(p.detect.mean), p.detect.tau)T(-12,12)\n    p.detect[i] <- exp(lp.detect[i]) / (1 + exp(lp.detect[i]))\n  }\n\n  for(j in 1:Ncov){\n    mean.beta[j] ~ dnorm(0, 0.01)\n    sd.beta[j] ~ dunif(0, 10)\n    tau.beta[j] <- pow(sd.beta[j]+0.001, -2)\n    for(i in 1:Nspecies){\n      betas[i,j] ~ dnorm(mean.beta[j], tau.beta[j])\n    }\n  }\n\n  # Likelihood\n  for(i in 1:Nobs){\n    logit(psi[i]) <- alpha[Species[i]] + inprod(betas[Species[i],], X[i, ])\n    z[i] ~ dbern(psi[i])\n    Y[i] ~ dbinom(z[i] * p.detect[Species[i]], J[i])\n  }\n}\n\nUsing information theory, specifically Watanabe-Akaike information criteria\n(WAIC),\nwe compared this model, which assumes all covariates have random effects, to\nall combination of models varying whether each covariate has fixed or random\neffects. See\nthis GitHub repository for\nall model statements and code.\nA model that assumes all covariates have random effects, and the data-generating\nmodel, in which only covariates 1 and 2 have random effects, performed the best, but were indistinguishable from one another:\nThis result makes sense because the model with all random effects is able to\nrecover species-specific responses to site-level covariates very well:\nHowever, this model estimates that the 95% HDI of \\(\\sigma_{slope}\\) of covariate\n3 includes zero, indicating that this covariate effectively has a fixed, rather\nthan random effect among species.\nThus, we could conclude that the first two covariates have random effects, while\nthe third covariate has a fixed effect. This means that composition shifts along\ngradients of covariates 1 and 2. We can visualize the relative contribution of\ncovariate 1 and 2’s random effects to composition using ordination, as discussed\nin Jackson et al. (2012). To do this, we compare the linear predictor\n(i.e. \\(\\text{logit}^{-1}(\\psi_q)\\)) of the best model that includes only significant\nrandom effects to a model that does not have any random effects.\nThe code to extract linear predictors and ordinate the community is provided on\nGitHub.\n\n\n\nJackson, Michelle M, Monica G Turner, Scott M Pearson, and Anthony R Ives. 2012. “Seeing the Forest and the Trees: Multilevel Models Reveal Both Species and Community Patterns.” Ecosphere 3 (9): 1–16. https://doi.org/10.1890/ES12-00116.1.\n\n\n\n\n",
    "preview": "posts/2018-12-27-multilevel-modeling-of-community-composition-with-imperfect-detection/multilevel-modeling-of-community-composition-with-imperfect-detection_files/figure-html5/plot-coefs-1.png",
    "last_modified": "2022-11-01T21:55:48-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-shiny-variance-inflation-factor-sandbox/",
    "title": "Shiny variance inflation factor sandbox",
    "description": "Exploring how correlation among covariates inflates uncertainty in coefficient estmates.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2014-04-03",
    "categories": [
      "shiny",
      "teaching"
    ],
    "contents": "\nIn multiple regression, strong correlations among covariates increases the uncertainty or variance in estimated regression coefficients.\nVariance inflation factors (VIFs) are one tool that has been used as an indicator of problematic covariate collinearity.\nIn teaching students about VIFs, it may be useful to have some interactive supplementary material so that they can manipulate factors affecting the uncertainty in slope terms in real-time.\nHere’s a little R shiny app that could be used as a starting point for such a supplement: https://mbjoseph.shinyapps.io/vif-sandbox/\nCurrently it only includes two covariates for simplicity, and gives the user control over the covariate \\(R^2\\) value, the residual variance, and the variance of both covariates.\nCode is on GitHub: https://github.com/mbjoseph/vif\nScreenshot:\n\n\n\n\n\n\n",
    "preview": "posts/2018-12-27-shiny-variance-inflation-factor-sandbox/shiny-variance-inflation-factor-sandbox_files/figure-html5/plot-robot-1.png",
    "last_modified": "2022-11-01T21:56:00-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-stochastic-search-variable-selection-in-jags/",
    "title": "Stochastic search variable selection in JAGS",
    "description": "Using spike and slab priors to shrink coefficients toward zero.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2014-03-22",
    "categories": [
      "jags"
    ],
    "contents": "\nStochastic search variable selection (SSVS) identifies promising subsets of\nmultiple regression covariates via Gibbs sampling (George and McCulloch 1993). Here’s\na short SSVS demo with JAGS and\nR.\nAssume we have a multiple regression problem:\n\\[\\boldsymbol{Y} \\sim N_n(\\boldsymbol{X \\beta}, \\sigma^2 \\boldsymbol{I})\\]\nWe suspect only a subset of the elements of \\(\\boldsymbol{\\beta}\\) are non-zero,\ni.e. some of the covariates have no effect.\nAssume \\(\\boldsymbol{\\beta}\\) arises from one of two normal mixture components,\ndepending on a latent variable \\(\\gamma_i\\):\n\\[\n\\beta_i \\mid \\gamma_i  \\sim \\left\\{\n  \\begin{array}{lr}\n    N(0, \\tau^2_i) &  \\gamma_i = 0\\\\\n    N(0, c^2_i \\tau^2_i) &  \\gamma_i = 1\n  \\end{array}\n\\right.\n\\]\n\\(\\tau_i\\) is positive but small s.t. \\(\\beta_i\\) is close to zero when\n\\(\\gamma_i = 0\\). \\(c_i\\) is large enough to allow reasonable deviations from zero\nwhen \\(\\gamma_i = 1\\). The prior probability that covariate \\(i\\) has a nonzero\neffect is \\(Pr(\\gamma_i = 1) = p_i\\).\nLet’s simulate a dataset in which some covariates have strong effects on the\nlinear predictor, and other don’t.\n\n\nlibrary(gridExtra)\nlibrary(runjags)\nlibrary(ggmcmc)\nlibrary(coda)\nlibrary(knitr)\n\nncov <- 20\nnobs <- 60\nvar_beta <- .004\nc <- 1000\np_inclusion <- .5\nsigma_y <- 1\n\n# generate covariates\nX <- array(dim=c(nobs, ncov))\nfor (i in 1:ncov){\n  X[, i] <- rnorm(nobs, 0, 1)\n}\n\nincluded <- rbinom(ncov, 1, p_inclusion)\ncoefs <- rnorm(n=ncov,\n               mean=0,\n               sd=ifelse(included==1,\n                         sqrt(var_beta * c),\n                         sqrt(var_beta)\n                         )\n               )\ncoefs <- sort(coefs)\nY <- rnorm(nobs, mean=X %*% coefs, sd=sigma_y)\n\n\nSpecifying the model:\n\n\ncat(\"model{\n  alpha ~ dnorm(0, 1)\n  sd_y ~ dunif(0, 10)\n  tau_y <- pow(sd_y, -2)\n\n  # ssvs priors\n  sd_bet ~ dunif(0, 10)\n  tau_in <- pow(sd_bet, -2)\n  tau[1] <- tau_in            # coef effectively zero\n  tau[2] <- tau_in / 1000     # nonzero coef\n  p_ind[1] <- 1/2\n  p_ind[2] <- 1 - p_ind[1]\n\n  for (j in 1:ncov){\n    indA[j] ~ dcat(p_ind[]) # returns 1 or 2\n    gamma[j] <- indA[j] - 1   # returns 0 or 1\n    beta[j] ~ dnorm(0, tau[indA[j]])\n  }\n\n  # likelihood\n  for (i in 1:nobs){\n    Y[i] ~ dnorm(alpha + X[i ,] %*% beta[], tau_y)\n  }\n}\n    \"\n    , file=\"ssvs.txt\")\n\n\nFitting the model:\n\n\ndat <- list(Y=Y, X=X, nobs=nobs, ncov=ncov)\nvars <- c(\"alpha\", \"sd_bet\", \"gamma\", \"beta\", \"tau_in\", \"sd_y\")\nout <- run.jags(\"ssvs.txt\", vars, data=dat, n.chains=3,\n                adapt=10000, burnin=10000)\n\nCompiling rjags model...\nCalling the simulation using the rjags method...\nAdapting the model for 10000 iterations...\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |                                                  |   1%\n  |                                                        \n  |+                                                 |   2%\n  |                                                        \n  |++                                                |   3%\n  |                                                        \n  |++                                                |   4%\n  |                                                        \n  |++                                                |   5%\n  |                                                        \n  |+++                                               |   6%\n  |                                                        \n  |++++                                              |   7%\n  |                                                        \n  |++++                                              |   8%\n  |                                                        \n  |++++                                              |   9%\n  |                                                        \n  |+++++                                             |  10%\n  |                                                        \n  |++++++                                            |  11%\n  |                                                        \n  |++++++                                            |  12%\n  |                                                        \n  |++++++                                            |  13%\n  |                                                        \n  |+++++++                                           |  14%\n  |                                                        \n  |++++++++                                          |  15%\n  |                                                        \n  |++++++++                                          |  16%\n  |                                                        \n  |++++++++                                          |  17%\n  |                                                        \n  |+++++++++                                         |  18%\n  |                                                        \n  |++++++++++                                        |  19%\n  |                                                        \n  |++++++++++                                        |  20%\n  |                                                        \n  |++++++++++                                        |  21%\n  |                                                        \n  |+++++++++++                                       |  22%\n  |                                                        \n  |++++++++++++                                      |  23%\n  |                                                        \n  |++++++++++++                                      |  24%\n  |                                                        \n  |++++++++++++                                      |  25%\n  |                                                        \n  |+++++++++++++                                     |  26%\n  |                                                        \n  |++++++++++++++                                    |  27%\n  |                                                        \n  |++++++++++++++                                    |  28%\n  |                                                        \n  |++++++++++++++                                    |  29%\n  |                                                        \n  |+++++++++++++++                                   |  30%\n  |                                                        \n  |++++++++++++++++                                  |  31%\n  |                                                        \n  |++++++++++++++++                                  |  32%\n  |                                                        \n  |++++++++++++++++                                  |  33%\n  |                                                        \n  |+++++++++++++++++                                 |  34%\n  |                                                        \n  |++++++++++++++++++                                |  35%\n  |                                                        \n  |++++++++++++++++++                                |  36%\n  |                                                        \n  |++++++++++++++++++                                |  37%\n  |                                                        \n  |+++++++++++++++++++                               |  38%\n  |                                                        \n  |++++++++++++++++++++                              |  39%\n  |                                                        \n  |++++++++++++++++++++                              |  40%\n  |                                                        \n  |++++++++++++++++++++                              |  41%\n  |                                                        \n  |+++++++++++++++++++++                             |  42%\n  |                                                        \n  |++++++++++++++++++++++                            |  43%\n  |                                                        \n  |++++++++++++++++++++++                            |  44%\n  |                                                        \n  |++++++++++++++++++++++                            |  45%\n  |                                                        \n  |+++++++++++++++++++++++                           |  46%\n  |                                                        \n  |++++++++++++++++++++++++                          |  47%\n  |                                                        \n  |++++++++++++++++++++++++                          |  48%\n  |                                                        \n  |++++++++++++++++++++++++                          |  49%\n  |                                                        \n  |+++++++++++++++++++++++++                         |  50%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  51%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  52%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  53%\n  |                                                        \n  |+++++++++++++++++++++++++++                       |  54%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  55%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  56%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  57%\n  |                                                        \n  |+++++++++++++++++++++++++++++                     |  58%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  59%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  60%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  61%\n  |                                                        \n  |+++++++++++++++++++++++++++++++                   |  62%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  63%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  64%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  65%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++                 |  66%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  67%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  68%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  69%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++               |  70%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  71%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  72%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  73%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++             |  74%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  75%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  76%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  77%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++           |  78%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  79%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  80%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  81%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++         |  82%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  83%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  84%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  85%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++       |  86%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  87%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  88%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  89%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  91%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  92%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  93%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++   |  94%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  95%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  97%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++++ |  98%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++++|  99%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%\nBurning in the model for 10000 iterations...\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |                                                  |   1%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   3%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |**                                                |   5%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   7%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |****                                              |   9%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  11%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |******                                            |  13%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  15%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |********                                          |  17%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  19%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |**********                                        |  21%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  23%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |************                                      |  25%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  27%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |**************                                    |  29%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  31%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |****************                                  |  33%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  35%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |******************                                |  37%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  39%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |********************                              |  41%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  43%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |**********************                            |  45%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  47%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |************************                          |  49%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  51%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |**************************                        |  53%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  55%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |****************************                      |  57%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  59%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |******************************                    |  61%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  63%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |********************************                  |  65%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  67%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |**********************************                |  69%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  71%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |************************************              |  73%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  75%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |**************************************            |  77%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  79%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |****************************************          |  81%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  83%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |******************************************        |  85%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  87%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |********************************************      |  89%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  91%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |**********************************************    |  93%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  95%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************  |  97%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************|  99%\n  |                                                        \n  |**************************************************| 100%\nRunning the model for 10000 iterations...\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |                                                  |   1%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   3%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |**                                                |   5%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   7%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |****                                              |   9%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  11%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |******                                            |  13%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  15%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |********                                          |  17%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  19%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |**********                                        |  21%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  23%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |************                                      |  25%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  27%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |**************                                    |  29%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  31%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |****************                                  |  33%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  35%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |******************                                |  37%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  39%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |********************                              |  41%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  43%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |**********************                            |  45%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  47%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |************************                          |  49%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  51%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |**************************                        |  53%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  55%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |****************************                      |  57%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  59%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |******************************                    |  61%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  63%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |********************************                  |  65%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  67%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |**********************************                |  69%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  71%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |************************************              |  73%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  75%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |**************************************            |  77%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  79%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |****************************************          |  81%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  83%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |******************************************        |  85%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  87%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |********************************************      |  89%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  91%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |**********************************************    |  93%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  95%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************  |  97%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************|  99%\n  |                                                        \n  |**************************************************| 100%\nSimulation complete\nCalculating summary statistics...\nNote: The monitored variables 'gamma[1]', 'gamma[2]',\n'gamma[3]', 'gamma[19]' and 'gamma[20]' appear to be\nnon-stochastic; they will not be included in the convergence\ndiagnostic\nNote: The monitored variable 'gamma[4]' appears to be\nstochastic in one chain but non-stochastic in another chain;\nit will not be included in the convergence diagnostic\nCalculating the Gelman-Rubin statistic for 44 variables....\nFinished running the simulation\n\noutdf <- ggs(as.mcmc.list(out))\nout_summary <- summary(out)\n\n\nNow, let’s visualize the inclusion probabilities.\n\n\ngamma_rows <- grepl(rownames(out_summary), pattern = \"^gamma\\\\[\")\nprobs <- out_summary[gamma_rows, \"Mean\"]\n\nlabels <- rep(NA, ncov)\nfor (i in 1:ncov){\n  labels[i] <- paste(\"beta[\", i, \"]\", sep=\"\")\n}\nxdf <- data.frame(Parameter = labels, value = 1:ncov)\np1 <- ggs_caterpillar(outdf, \"beta\", X=xdf) +\n  theme_classic() +\n  geom_vline(xintercept = 0, linetype = \"longdash\") +\n  geom_point(data=data.frame(coefs, pos = 1:ncov),\n              aes(x=coefs, y=pos), size=5, col=\"green4\", alpha=.7) +\n  xlab(\"Value\") +\n  ylab(\"Coefficient\") +\n  geom_hline(yintercept = 1:ncov, alpha=.05) +\n  scale_y_continuous(breaks=seq(0, ncov, 1))\n\ndf <- data.frame(probs=probs, coefs = coefs)\np2 <- ggplot(df, aes(x=abs(coefs), y=probs)) +\n  geom_point(size=5, alpha=.7) +\n  theme_classic() +\n  xlab(\"Absolute value of true coefficient\") +\n  ylab(\"Posterior probability of non-zeroness\")\n\ngrid.arrange(p1, p2, ncol=2)\n\n\n\nOn the left, green points indicate true coefficient values, with black posterior\ncredible intervals. The right plot shows the relationship between the true\nmagnitude of the effect and the posterior probability that the coefficient was\nnon-zero, \\(E(\\gamma_i \\mid \\boldsymbol{Y})\\).\n\n\n\nGeorge, Edward I, and Robert E McCulloch. 1993. “Variable Selection via Gibbs Sampling.” Journal of the American Statistical Association 88 (423): 881–89. https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476353.\n\n\n\n\n",
    "preview": "posts/2018-12-27-stochastic-search-variable-selection-in-jags/stochastic-search-variable-selection-in-jags_files/figure-html5/visualize-inclusion-probs-1.png",
    "last_modified": "2022-11-01T21:56:58-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-25-better-living-through-zero-one-inflated-beta-regression/",
    "title": "Better living through zero-one inflated beta regression",
    "description": "Fitting a Bayesian ZOIB regression model in JAGS.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2014-02-06",
    "categories": [
      "jags"
    ],
    "contents": "\nDealing with proportion data on the interval \\([0, 1]\\) is tricky. I realized this\nwhile trying to explain variation in vegetation cover. Unfortunately this is a\ntrue proportion, and can’t be made into a binary response. Further, true 0’s and\n1’s rule out beta regression. You could arcsine square root transform the data\n(but shouldn’t; Warton and Hui 2011). Enter zero-and-one inflated beta\nregression.\nThe zero-and-one-inflated beta distribution facilitates modeling fractional or\nproportional data that contains both 0’s and 1’s (Ospina and Ferrari 2010 -\nhighly recommended). The general idea is to model the response variable (call it\n\\(y\\)) as a mixture of Bernoulli and beta distributions, from which the true 0’s\nand 1’s, and the values between 0 and 1 are generated, respectively. The\nprobability density function is\n\\[\nf_{\\text{ZOIB}}(y; \\alpha, \\gamma, \\mu, \\phi) = \\left\\{\n  \\begin{array}{lr}\n    \\alpha(1 - \\gamma) &  y = 0\\\\\n    \\alpha \\gamma &  y = 1\\\\\n    (1 - \\alpha)f(y; \\mu, \\phi) &  0 < y < 1\n  \\end{array}\n\\right.\n\\]\nwhere \\(0 < \\alpha, \\gamma, \\mu < 1\\), and \\(\\phi>0\\). \\(f(y; \\mu, \\phi)\\) is the\nprobability density function for the beta distribution, parameterized in terms\nof its mean \\(\\mu\\) and precision \\(\\phi\\):\n\\[f_{\\text{beta}}(y; \\mu, \\phi) = \\dfrac{\\Gamma(\\phi)}{\\Gamma(\\mu \\phi) \\Gamma((1 - \\mu)\\phi)} y^{\\mu \\phi - 1} (1 - y)^{(1 - \\mu)\\phi - 1}\\]\n\\(\\alpha\\) is a mixture parameter that determines the extent to which the\nBernoulli or beta component dominates the pdf. \\(\\gamma\\) determines the\nprobability that \\(y=1\\) if it comes from the Bernoulli component. \\(\\mu\\) and\n\\(\\phi\\) are the expected value and the precision for the beta component, which is\nusually parameterized in terms of \\(p\\) and \\(q\\) (Ferrari and Cribari-Neto 2004).\n\\(\\mu = \\frac{p}{p + q}\\), and \\(\\phi=p+q\\).\nAlthough ecologists often deal with proportion data, I haven’t found any\nexamples of 0 & 1 inflated beta regression in the ecological literature.\nClosest thing I’ve found was Nishii and Tanaka (2012) who take a different\napproach, where values between 0 and 1 are modeled as logit-normal.\nHere’s a quick demo in JAGS with simulated data. For simplicity, I’ll assume 1)\nthere is one covariate that increases the expected value at the same rate for\nboth the Bernoulli and beta components s.t. \\(\\mu = \\gamma\\), and 2) the Bernoulli\ncomponent dominates extreme values of the covariate, where the expected value is\nnear 0 or 1.\n\n\nlibrary(rjags)\nlibrary(ggmcmc)\nlibrary(reshape2)\n\nset.seed(1234)\nn <- 60\nx <- runif(n, -3, 3)\na0 <- -3\na1 <- 0\na2 <- 1\nantilogit <- function(x){\n  exp(x) / (1 + exp(x))\n}\nalpha <- antilogit(a0 + a1 * x + a2 * x^2)\n\nb0 <- 0\nb1 <- 2\nmu <- antilogit(b0 + b1 * x)\nphi <- 5\np <- mu * phi\nq <- phi - mu * phi\n\ny.discrete <- rbinom(n, 1, alpha)\ny <- rep(NA, n)\nfor (i in 1:n){\n  if (y.discrete[i] == 1){\n    y[i] <- rbinom(1, 1, mu[i])\n  } else {\n    y[i] <- rbeta(1, p[i], q[i])\n  }\n}\n\n# split the data into discrete and continuous components\ny.d <- ifelse(y == 1 | y == 0, y, NA)\ny.discrete <- ifelse(is.na(y.d), 0, 1)\ny.d <- y.d[!is.na(y.d)]\nx.d <- x[y.discrete == 1]\nn.discrete <- length(y.d)\n\nwhich.cont <- which(y < 1 & y > 0)\ny.c <- ifelse(y < 1 & y > 0, y, NA)\ny.c <- y.c[!is.na(y.c)]\nn.cont <- length(y.c)\nx.c <- x[which.cont]\n\n\nNow we can specify our model in JAGS, following the factorization of the\nlikelihood given by Ospina and Ferrari (2010), estimate our parameters, and see\nhow well the model performs.\n\n\n# write model\ncat(\n  \"\n  model{\n  # priors\n  a0 ~ dnorm(0, .001)\n  a1 ~ dnorm(0, .001)\n  a2 ~ dnorm(0, .001)\n  b0 ~ dnorm(0, .001)\n  b1 ~ dnorm(0, .001)\n  t0 ~ dnorm(0, .01)\n  tau <- exp(t0)\n\n  # likelihood for alpha\n  for (i in 1:n){\n    logit(alpha[i]) <- a0 + a1 * x[i] + a2 * x[i] ^ 2\n    y.discrete[i] ~ dbern(alpha[i])\n  }\n\n  # likelihood for gamma\n  for (i in 1:n.discrete){\n    y.d[i] ~ dbern(mu[i])\n    logit(mu[i]) <- b0 + b1 * x.d[i]\n  }\n\n  # likelihood for mu and tau\n  for (i in 1:n.cont){\n    y.c[i] ~ dbeta(p[i], q[i])\n    p[i] <- mu2[i] * tau\n    q[i] <- (1 - mu2[i]) * tau\n    logit(mu2[i]) <- b0 + b1 * x.c[i]\n  }\n  }  \n  \", file=\"beinf.txt\"\n)\n\njd <- list(x=x, y.d=y.d, y.c=y.c, y.discrete = y.discrete,\n           n.discrete=n.discrete, n.cont = n.cont,\n           x.d = x.d, x.c=x.c, n=n)\nmod <- jags.model(\"beinf.txt\", data= jd, n.chains=3, n.adapt=1000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 120\n   Unobserved stochastic nodes: 6\n   Total graph size: 849\n\nInitializing model\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |+                                                 |   2%\n  |                                                        \n  |++                                                |   4%\n  |                                                        \n  |+++                                               |   6%\n  |                                                        \n  |++++                                              |   8%\n  |                                                        \n  |+++++                                             |  10%\n  |                                                        \n  |++++++                                            |  12%\n  |                                                        \n  |+++++++                                           |  14%\n  |                                                        \n  |++++++++                                          |  16%\n  |                                                        \n  |+++++++++                                         |  18%\n  |                                                        \n  |++++++++++                                        |  20%\n  |                                                        \n  |+++++++++++                                       |  22%\n  |                                                        \n  |++++++++++++                                      |  24%\n  |                                                        \n  |+++++++++++++                                     |  26%\n  |                                                        \n  |++++++++++++++                                    |  28%\n  |                                                        \n  |+++++++++++++++                                   |  30%\n  |                                                        \n  |++++++++++++++++                                  |  32%\n  |                                                        \n  |+++++++++++++++++                                 |  34%\n  |                                                        \n  |++++++++++++++++++                                |  36%\n  |                                                        \n  |+++++++++++++++++++                               |  38%\n  |                                                        \n  |++++++++++++++++++++                              |  40%\n  |                                                        \n  |+++++++++++++++++++++                             |  42%\n  |                                                        \n  |++++++++++++++++++++++                            |  44%\n  |                                                        \n  |+++++++++++++++++++++++                           |  46%\n  |                                                        \n  |++++++++++++++++++++++++                          |  48%\n  |                                                        \n  |+++++++++++++++++++++++++                         |  50%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  52%\n  |                                                        \n  |+++++++++++++++++++++++++++                       |  54%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  56%\n  |                                                        \n  |+++++++++++++++++++++++++++++                     |  58%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  60%\n  |                                                        \n  |+++++++++++++++++++++++++++++++                   |  62%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  64%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++                 |  66%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  68%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++               |  70%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  72%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++             |  74%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  76%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++           |  78%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  80%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++         |  82%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  84%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++       |  86%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  88%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  92%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++   |  94%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++++ |  98%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%\n\nout <- coda.samples(mod, c(\"a0\", \"a1\", \"a2\", \"b0\", \"b1\", \"tau\"),\n                    n.iter=6000)\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************| 100%\n\nggd <- ggs(out)\na0.post <- subset(ggd, Parameter == \"a0\")$value\na1.post <- subset(ggd, Parameter == \"a1\")$value\na2.post <- subset(ggd, Parameter == \"a2\")$value\nn.stored <- length(a2.post)\nP.discrete <- array(dim=c(n.stored, n))\nfor (i in 1:n){\n  P.discrete[, i] <- antilogit(a0.post + a1.post * x[i] + a2.post * x[i] ^ 2)\n}\npdd <- melt(P.discrete, varnames = c(\"iteration\", \"site\"), \n            value.name = \"Pr.discrete\")\npdd$x <- x[pdd$site]\nb0.post <- subset(ggd, Parameter == \"b0\")$value\nb1.post <- subset(ggd, Parameter == \"b1\")$value\nexpect <- array(dim=c(n.stored, n))\nfor (i in 1:n){\n  expect[, i] <- antilogit(b0.post + b1.post * x[i])\n}\nexd <- melt(expect, varnames=c(\"iteration\", \"site\"), value.name = \"Expectation\")\nexd$x <- x[exd$site]\nobsd <- data.frame(x=x, y=y,\n                   component = factor(ifelse(y < 1 & y > 0, \n                                             \"Continuous\", \"Discrete\")))\ntrued <- data.frame(x=x, mu=mu)\n\nggplot(pdd) +\n  geom_line(aes(x=x, y=Pr.discrete, group=iteration), \n            alpha=0.05, color=\"grey\") +\n  geom_line(aes(x=x, y=Expectation, group=iteration), \n            data=exd, color=\"blue\", alpha=0.05) +\n  geom_point(aes(x=x, y=y, fill=component), \n             data=obsd, size=3, color=\"black\",\n             position = position_jitter(width=0, height=.01), pch=21) +\n  scale_fill_manual(values = c(\"red\", \"white\"), \"Component\") +\n  ylab(\"y\") +\n  geom_line(aes(x=x, y=mu), \n            data=trued, color=\"green\", linetype=\"dashed\") +\n  theme_bw()\n\n\n\nHere the posterior probability that \\(y\\) comes from the discrete Bernoulli\ncomponent is shown in grey, and the posterior expected value for both the\nBernoulli and beta components across values of the covariate are shown in blue.\nThe dashed green line shows the true expected value that was used to generate\nthe data. Finally, the observed data are shown as jittered points, color coded\nas being derived from the continuous beta component, or the discrete Bernoulli\ncomponent.\n\n\n\n",
    "preview": "posts/2018-12-25-better-living-through-zero-one-inflated-beta-regression/better-living-through-zero-one-inflated-beta-regression_files/figure-html5/write-model-1.png",
    "last_modified": "2022-11-01T21:50:35-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-25-errors-in-variables-models-in-stan/",
    "title": "Errors-in-variables models in stan",
    "description": "Fitting a Bayesian regression with covariate uncertainty.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-11-27",
    "categories": [
      "stan"
    ],
    "contents": "\nHere, I’ll describe a Bayesian approach for estimation and correction for\ncovariate measurement error using a latent-variable based\nerrors-in-variables model,\nthat one might use when there is uncertainty in the covariate for a linear model.\nRecall that this matters because error in covariate measurements tends to bias\nslope estimates towards zero.\nFor what follows, we’ll assume a simple linear regression, in which continuous\ncovariates are measured with error.\nTrue covariate values are considered latent variables, with repeated\nmeasurements of covariate values arising from a normal distribution with a mean\nequal to the true value, and some measurement error \\(\\sigma_x\\), such that\n\\(\\epsilon_x \\sim \\text{Normal}(0, \\sigma_x)\\) and\n\\(\\epsilon_y \\sim \\text{Normal}(0, \\sigma_y)\\),\nwhere \\(\\epsilon_x\\) represents error in the covariate, and \\(\\epsilon_y\\)\nrepresents error in the response variable.\nWe assume that for sample unit \\(i\\) and repeat measurement \\(j\\):\n\\[ x^{obs}_{ij} \\sim \\text{Normal}(x_i, \\sigma_x) \\]\n\\[ y_i \\sim \\text{Normal}(\\alpha + \\beta x_i, \\sigma_y) \\]\nThe trick here is to use repeated measurements of the covariates to estimate and\ncorrect for measurement error.\nIn order for this to be valid, the true covariate values cannot vary across\nrepeat measurements.\nIf the covariate was individual weight, you would have to ensure that the true\nweight did not vary across repeat measurements (for me, frogs urinating during\nhandling would violate this assumption).\nBelow, I’ll simulate some data of this type in R. I’m assuming that we randomly\nselect some sampling units to remeasure covariate values, and each is\nremeasured n.reps times.\n\n\nlibrary(rstan)\nrstan_options(auto_write = TRUE)\noptions(mc.cores = parallel::detectCores())\n\nset.seed(1234)\nn.reps <- 3\nn.repeated <- 10\nn <- 30\n\n# true covariate values\nx <- runif(n, -3, 3)\ny <- x + rnorm(n)  # alpha=0, beta=1, sdy=1\n\n# random subset to perform repeat covariate measurements\nwhich.repeated <- sample(n, n.repeated)\nxsd <- .5  # measurement error\nxerr <- rnorm(n + (n.repeated * (n.reps - 1)), 0, xsd)\n\n# indx assigns measurements to sample units\nindx <- c(1:n, rep(which.repeated, each = n.reps - 1))\nindx <- sort(indx)\nnobs <- length(indx)\nxobs <- x[indx] + xerr\nplot(x[indx], xobs,\n    xlab = \"True covariate value\",\n    ylab = \"Observed covariate value\")\nabline(0, 1, lty = 2)\nsegments(x0 = x[indx], x1 = x[indx],\n    y0 = x[indx], y1 = xobs, col = \"red\")\nabline(v = x[which.repeated], col = \"green\", lty = 3)\n\n\n\nHere, the discrepancy due to measurement error is shown as a red segment, and\nthe sample units that were measured three times are highlighted with green\ndashed lines.\nI’ll use stan to estimate the model parameters, because\nI’ll be refitting the model to new data sets repeatedly below, and Stan is\nfaster than JAGS for these models.\n\n\n# write the .stan file\ncat(\"\ndata{\n  int n;\n  int nobs;\n  real xobs[nobs];\n  real y[n];\n  int indx[nobs];\n}\n\nparameters {\n  real alpha;\n  real beta;\n  real<lower=0> sigmay;\n  real<lower=0> sigmax;\n  real x[n];\n}\n\nmodel {\n  // priors\n  alpha ~ normal(0, 1);\n  beta ~ normal(0, 1);\n  sigmay ~ normal(0, 1);\n  sigmax ~ normal(0, 1);\n\n  // model structure  \n  for (i in 1:nobs){\n    xobs[i] ~ normal(x[indx[i]], sigmax);\n  }\n  for (i in 1:n){\n    y[i] ~ normal(alpha + beta*x[i], sigmay);\n  }\n}\n\n  \",\n    file = \"latent_x.stan\")\n\n\nWith the model specified, estimate the parameters.\n\n\nstan_d <- c(\"y\", \"xobs\", \"nobs\", \"n\", \"indx\")\nchains <- 3\niter <- 1000\nthin <- 1\nmod1 <- stan(file = \"latent_x.stan\", data = stan_d,\n    chains = chains, iter = iter,\n    thin = thin)\n\n\nHow did we do? Let’s compare the true vs. estimated covariate values for each\nsample unit.\n\n\nposteriors <- rstan::extract(mod1)\n\n# highest density interval helper function (thanks to Joe Mihaljevic)\nHDI <- function(values, percent = 0.95) {\n    sorted <- sort(values)\n    index <- floor(percent * length(sorted))\n    nCI <- length(sorted) - index\n    width <- rep(0, nCI)\n    for (i in 1:nCI) {\n        width[i] <- sorted[i + index] - sorted[i]\n    }\n    HDImin <- sorted[which.min(width)]\n    HDImax <- sorted[which.min(width) + index]\n    HDIlim <- c(HDImin, HDImax)\n    return(HDIlim)\n}\n\n# comparing estimated true x values to actual x values\nXd <- array(dim = c(n, 3))\nfor (i in 1:n) {\n    Xd[i, 1:2] <- HDI(posteriors$x[, i])\n    Xd[i, 3] <- median(posteriors$x[, i])\n}\n\nlims <- c(min(Xd), max(Xd))\nplot(x, Xd[, 3], xlab = \"True covariate value\",\n    ylab = \"Estimated covariate value\",\n    col = \"purple\", pch = 19, ylim = lims)\nabline(0, 1, lty = 2)\nsegments(x0 = x, x1 = x, y0 = Xd[, 1], y1 = Xd[, 2], col = \"purple\")\n\n\n\nHere purple marks the posterior for the covariate values, and the dashed black\nline shows the one-to-one line that we would expect if the estimates exactly\nmatched the true values.\nIn addition to estimating the true covariate values, we may wish to check to\nsee how well we estimated the standard deviation of the measurement error in\nour covariate.\n\n\nhist(posteriors$sigmax, breaks = 30,\n    main = \"Posterior for measurement error\",\n    xlab = \"Measurement standard deviation\")\nabline(v = xsd, col = \"red\", lwd = 2)\nlegend(\"topright\", legend = \"True value\", col = \"red\",\n    lty = 1, bty = \"n\", lwd = 2)\n\n\n\nHow many sample units need repeat measurements?\nYou may want to know how many sample units need to be repeatedly measured to\nadequately estimate the degree of covariate measurement error.\nFor instance, if \\(\\sigma_x = 1\\), how does the precision in our estimate of\n\\(\\sigma_x\\) improve as more sample units are repeatedly measured?\nLet’s see what happens when we repeatedly measure covariate values for\n\\(1, 2, ..., N\\) randomly selected sampling units.\n\n\nn.repeated <- 1:n\n\n# store the HDI and mode for the estimate of sigmax in an array\npost.sdx <- array(dim = c(length(n.repeated), 3))\nfor (i in n.repeated) {\n    n.repeats <- i\n    which.repeated <- sample(n, n.repeats)\n    xerr <- rnorm(n + (n.repeats * (n.reps - 1)), 0, xsd)\n    indx <- c(1:n, rep(which.repeated, each = n.reps - 1))\n    indx <- sort(indx)\n    nobs <- length(indx)\n    xobs <- x[indx] + xerr\n    stan_d <- c(\"y\", \"xobs\", \"nobs\", \"n\", \"indx\")\n    mod <- stan(fit = mod1, data = stan_d, chains = chains,\n        iter = iter, thin = thin)\n    posteriors <- extract(mod)\n    post.sdx[i, 1:2] <- HDI(posteriors$sigmax)\n    post.sdx[i, 3] <- median(posteriors$sigmax)\n}\n\n\n\n\n# Plot the relationship b/t number of sampling units revisited & sdx\nplot(x = n.repeated, y = rep(xsd, length(n.repeated)),\n    type = \"l\", lty = 2,\n    ylim = c(0, max(post.sdx)),\n    xlab = \"Number of sampling units measured three times\",\n    ylab = \"Estimated measurement error\")\nsegments(x0 = n.repeated, x1 = n.repeated,\n    y0 = post.sdx[, 1], y1 = post.sdx[, 2],\n    col = \"red\")\npoints(x = n.repeated, y = post.sdx[, 3], col = \"red\")\nlegend(\"topright\", legend = c(\"True value\", \"Posterior estimate\"),\n    col = c(\"black\", \"red\"), lty = c(2, 1),\n    pch = c(NA, 1), bty = \"n\")\n\n\n\nLooking at this plot, you could eyeball the number of sample units that should\nbe remeasured when designing a study.\nRealistically, you might want to explore how this number depends on the true\namount of measurement error, and also simulate multiple realizations (rather\nthan just one) for each scenario.\nUsing a similar approach, you might also evaluate whether it’s more efficient\nto remeasure more sample units, or invest in more repeated measurements per\nsample unit.\n\n\n\n",
    "preview": "posts/2018-12-25-errors-in-variables-models-in-stan/errors-in-variables-models-in-stan_files/figure-html5/simulate-data-1.png",
    "last_modified": "2022-11-01T21:55:19-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-25-r-r-markdown-and-my-divorce-from-microsoft-word/",
    "title": "R Markdown and my divorce from Microsoft Word",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-10-30",
    "categories": [
      "rants"
    ],
    "contents": "\nI do a lot of scholarly writing that requires associated or embedded R analyses,\nfigures, and tables, plus bibliographies.\nMicrosoft Word makes this unnecessarily difficult.\nMany tools are now available to break free from\nthe tyranny of Word.\nThe ones I like involve writing an article in\nmarkdown format, integrating all\ndata preparation, analysis, and outputs with the document (e.g. with the\nexcellent and accessible knitr package or with a\ncustom make set up\nlike this one).\nAdd in version control with Git, and you’ve got a nice\nstew going.\nIf you’re involved in the open source/reproducible research blogo-twittersphere,\nthis is probably old hat.\nTo many others, it’s not.\nMost scientists I see in the wild still manually insert figures and results\nfrom statistical analyses in Word documents, perhaps the manufacturing\nequivalent of hand-crafting each document.\nR markdown provides a level of automation that is amenable to creating many\ndocuments or recreating/updating one document many times, the manufacturing\nequivalent of automated robots that increase efficiency (but do require some\nprogramming to function properly).\n\n\n\nI can’t give an authoritative overview, but here are some resources that helped\nme get through my divorce with Microsoft Word:\nR Markdown = knitr + RStudio may be one of the better places to start\nHow to ditch Word by Karthik Ram\nMarkdown and the future of collaborative manuscript writing by Karthik Ram\nGit can facilitate greater reproducibility and increased transparency in science (Ram 2013)\nWhat is scholarly markdown? by Martin Fenner\nMarkdown for scientific writing\nPandoc to convert from markdown to almost any other format\npandoc-citeproc for citations\nCitations in markdown using knitr for another take on citations, from Carl Boettiger\nGetting started with make\n\n\n\nRam, Karthik. 2013. “Git Can Facilitate Greater Reproducibility and Increased Transparency in Science.” Source Code for Biology and Medicine 8 (1): 7. https://doi.org/10.1186/1751-0473-8-7.\n\n\n\n\n",
    "preview": "posts/2018-12-25-r-r-markdown-and-my-divorce-from-microsoft-word/r-r-markdown-and-my-divorce-from-microsoft-word_files/figure-html5/plot-robot-1.png",
    "last_modified": "2022-11-01T21:55:23-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-25-animating-the-metropolis-algorithm/",
    "title": "Animating the Metropolis algorithm",
    "description": "A homemade Metropolis algorithm animation using R and the animation package.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-09-08",
    "categories": [
      "visualization",
      "teaching"
    ],
    "contents": "\nThe Metropolis algorithm, and its generalization\n(Metropolis-Hastings algorithm)\nprovide elegant methods for obtaining sequences of random samples from complex\nprobability distributions (Beichl and Sullivan 2000). When I first read about\nmodern MCMC methods, I had\ntrouble visualizing the convergence of Markov chains in higher dimensional\ncases. So, I thought I might put together a visualization in a two-dimensional\ncase.\nI’ll use a simple example: estimating a population mean and standard deviation.\nWe’ll define some population level parameters, collect some data, then use the\nMetropolis algorithm to simulate the joint posterior of the mean and standard\ndeviation.\n\n\nlibrary(sm)\n\n# population level parameters\nmu <- 7\nsigma <- 3\n\n# collect some data (e.g. a sample of heights)\nn <- 50\nx <- rnorm(n, mu, sigma)\n\n# log-likelihood function\nll <- function(x, muhat, sigmahat){\n  sum(dnorm(x, muhat, sigmahat, log=T))\n}\n\n# prior densities\npmu <- function(mu){\n  dnorm(mu, 0, 100, log=T)\n}\n\npsigma <- function(sigma){\n  dunif(sigma, 0, 10, log=T)\n}\n\n# posterior density function (log scale)\npost <- function(x, mu, sigma){\n  ll(x, mu, sigma) + pmu(mu) + psigma(sigma)\n}\n\ngeninits <- function(){\n  list(mu = runif(1, 4, 10),\n       sigma = runif(1, 2, 6))\n}\n\njump <- function(x, dist = .2){ # must be symmetric\n  x + rnorm(1, 0, dist)\n}\n\niter = 10000\nchains <- 3\nposterior <- array(dim = c(chains, 2, iter))\naccepted <- array(dim=c(chains, iter - 1))\n\nset.seed(1234)\nfor (c in 1:chains){\n  theta.post <- array(dim=c(2, iter))\n  inits <- geninits()\n  theta.post[1, 1] <- inits$mu\n  theta.post[2, 1] <- inits$sigma\n  for (t in 2:iter){\n    # theta_star = proposed next values for parameters\n    theta_star <- c(jump(theta.post[1, t-1]), jump(theta.post[2, t-1]))\n    pstar <- post(x, mu = theta_star[1], sigma = theta_star[2])  \n    pprev <- post(x, mu = theta.post[1, t-1], sigma = theta.post[2, t-1])\n    lr <- pstar - pprev\n    r <- exp(lr)\n\n    # theta_star is accepted if posterior density is higher w/ theta_star\n    # if posterior density is not higher, it is accepted with probability r\n    # else theta does not change from time t-1 to t\n    accept <- rbinom(1, 1, prob = min(r, 1))\n    accepted[c, t - 1] <- accept\n    if (accept == 1){\n      theta.post[, t] <- theta_star\n    } else {\n      theta.post[, t] <- theta.post[, t-1]\n    }\n  }\n  posterior[c, , ] <- theta.post\n}\n\n\nThen, to visualize the evolution of the Markov chains, we can make plots of the\nchains in 2-parameter space, along with the posterior density at different\niterations, joining these plots together using ImageMagick’s convert command\n(in the terminal) to create an animated .gif:\n\n\nsequence <- unique(round(exp(seq(0, log(iter), length.out = 150))))\n\nxlims <- c(4, 10)\nylims <- c(1, 6)\n\nfor (i in sequence){\n  par(mfrow=c(1, 2))\n  plot(posterior[1, 1, 1:i], posterior[1, 2, 1:i],\n       type=\"l\", xlim=xlims, ylim=ylims, col=\"blue\",\n       xlab=\"mu\", ylab=\"sigma\", main=\"Markov chains\")\n  lines(posterior[2, 1, 1:i], posterior[2, 2, 1:i],\n        col=\"purple\")\n  lines(posterior[3, 1, 1:i], posterior[3, 2, 1:i],\n        col=\"red\")\n  text(x=7, y=1.2, paste(\"Iteration \", i), cex=1.5)\n  sm.density(x=cbind(c(posterior[, 1, 1:i]), c(posterior[, 2, 1:i])),\n             xlab=\"mu\", ylab=\"sigma\",\n             zlab=\"\", zlim=c(0, .7),\n             xlim=xlims, ylim=ylims, col=\"white\", \n             verbose=0)\n  title(\"Posterior density\")\n}\n\n\n\n\n\n\nBeichl, Isabel, and Francis Sullivan. 2000. “The Metropolis Algorithm.” Computing in Science & Engineering 2 (1): 65–69. https://doi.org/10.1109/5992.814660.\n\n\n\n\n",
    "preview": "posts/2018-12-25-animating-the-metropolis-algorithm/animating-the-metropolis-algorithm_files/figure-html5/create-gif.gif",
    "last_modified": "2022-11-01T21:49:37-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-24-quantifying-uncertainty-around-r-squared-for-generalized-linear-models/",
    "title": "Quantifying uncertainty around R-squared for generalized linear models",
    "description": "How to propage posterior uncertainty to R-squared in R and JAGS.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-08-22",
    "categories": [
      "jags"
    ],
    "contents": "\nPeople love \\(R^2\\). As such, when Nakagawa and Schielzeth published\nA general and simple method for obtaining \\(R^2\\) from generalized linear\nmixed-effects models in Methods in Ecology and Evolution earlier this year\n(Nakagawa and Schielzeth 2013),\necologists (amid increasing use of generalized linear mixed models (GLMMs))\nrejoiced. Now there’s\nan R function that automates \\(R^2\\) calculations for GLMMs fit with the lme4 package.\n\\(R^2\\) is usually reported as a point estimate of the variance explained by a\nmodel, using the maximum likelihood estimates of the model parameters and\nignoring uncertainty around these estimates. Nakagawa and Schielzeth (2013)\nnoted that it may be desirable to quantify the uncertainty around \\(R^2\\) using\nMCMC sampling. So, here we are.\nBackground\n\\(R^2\\) quantifies the proportion of observed variance explained by a statistical\nmodel. When it is large (near 1), much of the variance in the data is explained\nby the model.\nNakagawa and Schielzeth (2013) present two \\(R^2\\) statistics for generalized\nlinear mixed models:\nMarginal \\(R^2_{GLMM(m)}\\), which represents the proportion of variance\nexplained by fixed effects:\n\\[R^2_{GLMM(m)} = \\frac{\\sigma^2_f}{\\sigma^2_f + \\sum_{l=1}^{u}\\sigma^2_l + \\sigma^2_d + \\sigma^2_e}\\]\nwhere \\(\\sigma^2_f\\) represents the variance in the fitted values (on a link\nscale) based on the fixed effects:\n\\[ \\sigma^2_f = var(\\boldsymbol{X \\beta}) \\]\n\\(\\boldsymbol{X}\\) is the design matrix of the fixed effects, and\n\\(\\boldsymbol{\\beta}\\) is the vector of fixed effects estimates.\n\\(\\sum_{l=1}^{u}\\sigma^2_l\\) represents the sum the variance components for all\nof \\(u\\) random effects. \\(\\sigma^2_d\\) is the distribution-specific variance\n(Nakagawa and Schielzeth 2010), and \\(\\sigma^2_e\\) represents added dispersion.\nConditional \\(R^2_{GLMM(c)}\\) represents the proportion of variance explained\nby the fixed and random effects combined:\n\\[ R^2_{GLMM(c)} = \\frac{\\sigma^2_f + \\sum_{l=1}^{u}\\sigma^2_l}{\\sigma^2_f + \\sum_{l=1}^{u}\\sigma^2_l + \\sigma^2_d + \\sigma^2_e} \\]\nPoint-estimation of \\(R^2_{GLMM}\\)\nHere, I’ll follow the example of an overdispersed Poisson GLMM provided in the\nsupplement to Nakagawa & Schielzeth (Nakagawa and Schielzeth 2013). This is their most\ncomplicated example, and the simpler ones ought to be relatively straightforward\nfor those that are interested in normal or binomial GLMMs.\n\n\nlibrary(arm)\nlibrary(ggmcmc)\nlibrary(lme4)\nlibrary(rjags)\n\n# First, simulate data (code adapted from Nakagawa & Schielzeth 2013):\nn_population <- 8\nn <- 100\nPopulation <- gl(n_population, k = n / n_population, n)\n\nn_container <- 10\nContainer <- gl(n_container, n / n_container, n)\n\n# Sex of the individuals. Uni-sex within each container (individuals are\n# sorted at the pupa stage)\nSex <- factor(sample(c(\"Female\", \"Male\"), n, replace = TRUE))\n\n# Habitat at the collection site: dry or wet soil (four indiviudal from\n# each Habitat in each container)\nHabitat <- factor(sample(c(\"dry\", \"wet\"), n, replace = TRUE))\n\n# Food treatment at the larval stage: special food ('Exp') or standard\n# food ('Cont')\nTreatment <- factor(sample(c(\"Cont\", \"Exp\"), n, replace = TRUE))\n\n# Data combined in a dataframe\nData <- data.frame(Population = Population,\n    Container = Container, Sex = Sex,\n    Habitat = Habitat, Treatment = Treatment)\n\n# Subset the design matrix (only females express colour morphs)\nDataF <- Data[Data$Sex == \"Female\", ]\n\n# random effects\nPopulationE <- rnorm(n_population, 0, sqrt(0.4))\nContainerE <- rnorm(n_container, 0, sqrt(0.05))\n\n# generation of response values on link scale (!) based on fixed effects,\n# random effects and residual errors\nEggLink <- with(DataF,\n                  1.1 +\n                  0.5 * (as.numeric(Treatment) - 1) +\n                  0.1 * (as.numeric(Habitat) - 1) +\n                  PopulationE[Population] +\n                  ContainerE[Container])\n\n# data generation (on data scale!) based on Poisson distribution\nDataF$Egg <- rpois(length(EggLink), exp(EggLink))\n\n\nHaving simulated a dataset, calculate the \\(R^2\\) point-estimates, using the lme4 package to fit the model.\n\n\n# Creating a dummy variable that allows estimating additive dispersion in\n# glmer This triggers a warning message when fitting the model\nUnit <- factor(1:length(DataF$Egg))\n\n# Fit null model without fixed effects (but including all random effects)\nm0 <- glmer(Egg ~ 1 + (1 | Population) + (1 | Container) + (1 | Unit),\n    family = \"poisson\", data = DataF)\n\n# Fit alternative model including fixed and all random effects\nmF <- glmer(Egg ~ Treatment + Habitat + (1 | Population) + (1 | Container) +\n    (1 | Unit), family = \"poisson\", data = DataF)\n\n# View model fits for both models\nsummary(m0)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: Egg ~ 1 + (1 | Population) + (1 | Container) + (1 | Unit)\n   Data: DataF\n\n     AIC      BIC   logLik deviance df.resid \n   257.7    265.8   -124.9    249.7       51 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5910 -0.5656 -0.3137  0.5858  1.7928 \n\nRandom effects:\n Groups     Name        Variance  Std.Dev. \n Unit       (Intercept) 4.377e-08 0.0002092\n Container  (Intercept) 5.213e-07 0.0007220\n Population (Intercept) 4.986e-01 0.7061462\nNumber of obs: 55, groups:  Unit, 55; Container, 10; Population, 8\n\nFixed effects:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.5526     0.2604   5.962 2.49e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00622563 (tol = 0.002, component 1)\n\nsummary(mF)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: \nEgg ~ Treatment + Habitat + (1 | Population) + (1 | Container) +  \n    (1 | Unit)\n   Data: DataF\n\n     AIC      BIC   logLik deviance df.resid \n   257.9    269.9   -122.9    245.9       49 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.51389 -0.55034 -0.08826  0.47550  2.07213 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n Unit       (Intercept) 0.0000   0.0000  \n Container  (Intercept) 0.0000   0.0000  \n Population (Intercept) 0.4939   0.7028  \nNumber of obs: 55, groups:  Unit, 55; Container, 10; Population, 8\n\nFixed effects:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.42059    0.27659   5.136 2.81e-07 ***\nTreatmentExp  0.23000    0.11652   1.974   0.0484 *  \nHabitatwet    0.05464    0.11964   0.457   0.6479    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) TrtmnE\nTreatmntExp -0.232       \nHabitatwet  -0.292  0.153\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n# Extraction of fitted value for the alternative model fixef() extracts\n# coefficents for fixed effects model.matrix(mF) returns design matrix\nFixed <- fixef(mF)[2] * model.matrix(mF)[, 2] + fixef(mF)[3] * model.matrix(mF)[, 3]\n\n# Calculation of the variance in fitted values\nVarF <- var(Fixed)\n\n# An alternative way for getting the same result\nVarF <- var(as.vector(fixef(mF) %*% t(model.matrix(mF))))\n\n# R2GLMM(m) - marginal R2GLMM see Equ. 29 and 30 and Table 2 fixef(m0)\n# returns the estimate for the intercept of null model\nR2m <- VarF/(VarF + VarCorr(mF)$Container[1] +\n               VarCorr(mF)$Population[1] + VarCorr(mF)$Unit[1] +\n                log(1 + 1/exp(as.numeric(fixef(m0))))\n            )\n\n# R2GLMM(c) - conditional R2GLMM for full model Equ. XXX, XXX\nR2c <- (VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1])/\n         (VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] +\n           VarCorr(mF)$Unit[1] + log(1 + 1/exp(as.numeric(fixef(m0))))\n         )\n\n# Print marginal and conditional R-squared values\ncbind(R2m, R2c)\n\n           R2m       R2c\n[1,] 0.0181809 0.7251428\n\nHaving stored our point estimates, we can now turn to Bayesian methods instead, and generate \\(R^2\\) posteriors.\nPosterior uncertainty in \\(R^2_{GLMM}\\)\nWe need to fit two models in order to get the needed parameters for \\(R^2_{GLMM}\\). First, a model that includes all random effects, but only an intercept fixed effect is fit to estimate the distribution specific variance \\(\\sigma^2_d\\). Second, we fit a model that includes all random and all fixed effects to estimate the remaining variance components.\nFirst I’ll clean up the data that we’ll feed to JAGS:\n\n\n# Prepare the data\njags_d <- as.list(DataF)[-c(2, 3)]  # redefine container, don't need sex\njags_d$nobs <- nrow(DataF)\njags_d$npop <- length(unique(jags_d$Population))\n\n# renumber containers from 1:ncontainer for ease of indexing\njags_d$Container <- rep(NA, nrow(DataF))\nfor (i in 1:nrow(DataF)) {\n  jags_d$Container[i] <- which(unique(DataF$Container) == DataF$Container[i])\n}\njags_d$ncont <- length(unique(jags_d$Container))\n\n# Convert binary factors to 0's and 1's\njags_d$Habitat <- ifelse(jags_d$Habitat == \"dry\", 0, 1)\njags_d$Treatment <- ifelse(jags_d$Treatment == \"Cont\", 0, 1)\nstr(jags_d)\n\nList of 8\n $ Population: Factor w/ 8 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 2 2 2 2 ...\n $ Habitat   : num [1:55] 0 0 1 1 1 0 1 1 0 1 ...\n $ Treatment : num [1:55] 0 0 0 0 1 0 1 0 1 0 ...\n $ Egg       : int [1:55] 5 5 6 5 7 5 17 9 10 10 ...\n $ nobs      : int 55\n $ npop      : int 8\n $ Container : int [1:55] 1 1 1 1 1 2 2 2 2 2 ...\n $ ncont     : int 10\n\nThen, fitting the intercept model:\n\n\n# intercept model statement:\ncat(\"\nmodel{\n  # priors on precisions (inverse variances)\n  tau.pop ~ dgamma(0.01, 0.01)\n  sd.pop <- sqrt(1/tau.pop)\n  tau.cont ~ dgamma(0.01, 0.01)\n  sd.cont <- sqrt(1/tau.cont)\n  tau.unit ~ dgamma(0.01, 0.01)\n  sd.unit <- sqrt(1/tau.unit)\n  # prior on intercept\n  alpha ~ dnorm(0, 0.01)\n\n  # random effect of container\n  for (i in 1:ncont){\n    cont[i] ~ dnorm(0, tau.cont)\n  }\n\n  # random effect of population\n  for (i in 1:npop){\n    pop[i] ~ dnorm(0, tau.pop)\n  }\n\n  # likelihood\n  for (i in 1:nobs){\n    Egg[i] ~ dpois(mu[i])\n    log(mu[i]) <- cont[Container[i]] + pop[Population[i]] + unit[i]\n    unit[i] ~ dnorm(alpha, tau.unit)\n  }\n}\n    \", fill=T, file=\"pois_intercept.txt\")\n\nnstore <- 2000\nnthin <- 20\nni <- nstore*nthin\n\nint_mod <- jags.model(\"pois_intercept.txt\",\n                      data=jags_d[-c(2, 3)], # exclude unused data\n                      n.chains=3,\n                      n.adapt=5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 55\n   Unobserved stochastic nodes: 77\n   Total graph size: 364\n\nInitializing model\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |+                                                 |   2%\n  |                                                        \n  |++                                                |   4%\n  |                                                        \n  |+++                                               |   6%\n  |                                                        \n  |++++                                              |   8%\n  |                                                        \n  |+++++                                             |  10%\n  |                                                        \n  |++++++                                            |  12%\n  |                                                        \n  |+++++++                                           |  14%\n  |                                                        \n  |++++++++                                          |  16%\n  |                                                        \n  |+++++++++                                         |  18%\n  |                                                        \n  |++++++++++                                        |  20%\n  |                                                        \n  |+++++++++++                                       |  22%\n  |                                                        \n  |++++++++++++                                      |  24%\n  |                                                        \n  |+++++++++++++                                     |  26%\n  |                                                        \n  |++++++++++++++                                    |  28%\n  |                                                        \n  |+++++++++++++++                                   |  30%\n  |                                                        \n  |++++++++++++++++                                  |  32%\n  |                                                        \n  |+++++++++++++++++                                 |  34%\n  |                                                        \n  |++++++++++++++++++                                |  36%\n  |                                                        \n  |+++++++++++++++++++                               |  38%\n  |                                                        \n  |++++++++++++++++++++                              |  40%\n  |                                                        \n  |+++++++++++++++++++++                             |  42%\n  |                                                        \n  |++++++++++++++++++++++                            |  44%\n  |                                                        \n  |+++++++++++++++++++++++                           |  46%\n  |                                                        \n  |++++++++++++++++++++++++                          |  48%\n  |                                                        \n  |+++++++++++++++++++++++++                         |  50%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  52%\n  |                                                        \n  |+++++++++++++++++++++++++++                       |  54%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  56%\n  |                                                        \n  |+++++++++++++++++++++++++++++                     |  58%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  60%\n  |                                                        \n  |+++++++++++++++++++++++++++++++                   |  62%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  64%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++                 |  66%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  68%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++               |  70%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  72%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++             |  74%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  76%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++           |  78%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  80%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++         |  82%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  84%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++       |  86%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  88%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  92%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++   |  94%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++++ |  98%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%\n\nvars <- c(\"sd.pop\", \"sd.cont\", \"sd.unit\", \"alpha\")\nint_out <- coda.samples(int_mod, n.iter=ni, thin=nthin,\n                        variable.names=vars)\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************| 100%\n\nThen, fit the full mixed-model with all fixed and random effects:\n\n\n# covariate model statement:\ncat(\"\nmodel{\n  # priors on precisions (inverse variances)\n  tau.pop ~ dgamma(0.01, 0.01)\n  sd.pop <- sqrt(1/tau.pop)\n  tau.cont ~ dgamma(0.01, 0.01)\n  sd.cont <- sqrt(1/tau.cont)\n  tau.unit ~ dgamma(0.01, 0.01)\n  sd.unit <- sqrt(1/tau.unit)\n  # priors on coefficients\n  alpha ~ dnorm(0, 0.01)\n  beta1 ~ dnorm(0, 0.01)\n  beta2 ~ dnorm(0, 0.01)\n\n  # random effect of container\n  for (i in 1:ncont){\n    cont[i] ~ dnorm(0, tau.cont)\n  }\n\n  # random effect of population\n  for (i in 1:npop){\n    pop[i] ~ dnorm(0, tau.pop)\n  }\n\n  # likelihood\n  for (i in 1:nobs){\n    Egg[i] ~ dpois(mu[i])\n    log(mu[i]) <- cont[Container[i]] + pop[Population[i]] + unit[i]\n    mu_f[i] <- alpha + beta1 * Treatment[i] + beta2 * Habitat[i]\n    unit[i] ~ dnorm(mu_f[i], tau.unit)\n  }\n}\n    \", fill=T, file=\"pois_cov.txt\")\n\ncov_mod <- jags.model(\"pois_cov.txt\",\n                      data=jags_d,\n                      n.chains=3,\n                      n.adapt=5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 55\n   Unobserved stochastic nodes: 79\n   Total graph size: 484\n\nInitializing model\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |+                                                 |   2%\n  |                                                        \n  |++                                                |   4%\n  |                                                        \n  |+++                                               |   6%\n  |                                                        \n  |++++                                              |   8%\n  |                                                        \n  |+++++                                             |  10%\n  |                                                        \n  |++++++                                            |  12%\n  |                                                        \n  |+++++++                                           |  14%\n  |                                                        \n  |++++++++                                          |  16%\n  |                                                        \n  |+++++++++                                         |  18%\n  |                                                        \n  |++++++++++                                        |  20%\n  |                                                        \n  |+++++++++++                                       |  22%\n  |                                                        \n  |++++++++++++                                      |  24%\n  |                                                        \n  |+++++++++++++                                     |  26%\n  |                                                        \n  |++++++++++++++                                    |  28%\n  |                                                        \n  |+++++++++++++++                                   |  30%\n  |                                                        \n  |++++++++++++++++                                  |  32%\n  |                                                        \n  |+++++++++++++++++                                 |  34%\n  |                                                        \n  |++++++++++++++++++                                |  36%\n  |                                                        \n  |+++++++++++++++++++                               |  38%\n  |                                                        \n  |++++++++++++++++++++                              |  40%\n  |                                                        \n  |+++++++++++++++++++++                             |  42%\n  |                                                        \n  |++++++++++++++++++++++                            |  44%\n  |                                                        \n  |+++++++++++++++++++++++                           |  46%\n  |                                                        \n  |++++++++++++++++++++++++                          |  48%\n  |                                                        \n  |+++++++++++++++++++++++++                         |  50%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  52%\n  |                                                        \n  |+++++++++++++++++++++++++++                       |  54%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  56%\n  |                                                        \n  |+++++++++++++++++++++++++++++                     |  58%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  60%\n  |                                                        \n  |+++++++++++++++++++++++++++++++                   |  62%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  64%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++                 |  66%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  68%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++               |  70%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  72%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++             |  74%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  76%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++           |  78%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  80%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++         |  82%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  84%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++       |  86%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  88%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  92%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++   |  94%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++++ |  98%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%\n\nvars2 <- c(\"sd.pop\", \"sd.cont\", \"sd.unit\", \"alpha\", \"beta1\", \"beta2\")\ncov_out <- coda.samples(cov_mod, n.iter=ni, thin=nthin,\n                        variable.names=vars2)\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************| 100%\n\nFor every MCMC draw, we can calculate \\(R^2_{GLMM}\\), generating posteriors for both the marginal and conditional values.\n\n\n# Step 1: variance in expected values (using fixed effects only)\nd_int <- ggs(int_out)\nd_cov <- ggs(cov_out)\n\nalpha_cov <- subset(d_cov, Parameter == \"alpha\")$value\nalpha_int <- subset(d_int, Parameter == \"alpha\")$value\nb1_cov <- subset(d_cov, Parameter == \"beta1\")$value\nb2_cov <- subset(d_cov, Parameter == \"beta2\")$value\n\nXmat <- cbind(rep(1, jags_d$nobs), jags_d$Treatment, jags_d$Habitat)\nbeta_mat <- cbind(alpha_cov, b1_cov, b2_cov)\n\nfixed_expect <- array(dim = c(nstore, jags_d$nobs))\nvarF <- rep(NA, nstore)\nfor (i in 1:nstore) {\n    fixed_expect[i, ] <- beta_mat[i, ] %*% t(Xmat)\n    varF[i] <- var(fixed_expect[i, ])\n}\n\n# Step 2: calculate remaining variance components\n# among container variance\nvarCont <- subset(d_cov, Parameter == \"sd.cont\")$value^2\n# among population variance\nvarPop <- subset(d_cov, Parameter == \"sd.pop\")$value^2\n# overdispersion variance\nvarUnit <- subset(d_cov, Parameter == \"sd.unit\")$value^2\n# distribution variance (Table 2, Nakagawa & Schielzeth 2013)\nvarDist <- log(1/exp(alpha_int) + 1)\n\n# Finally, calculate posterior R-squared values\n# marginal\npostR2m <- varF/(varF + varCont + varPop + varUnit + varDist)\n# conditional\npostR2c <- (varF + varCont + varPop)/\n             (varF + varCont + varPop + varUnit + varDist)\n\n# compare posterior R-squared values to point estimates\npar(mfrow = c(1, 2))\nhist(postR2m, main = \"Marginal R-squared\",\n        ylab = \"Posterior density\",\n        xlab = NULL, breaks = 20)\nabline(v = R2m, col = \"blue\", lwd = 4)\nhist(postR2c, main = \"Conditional R-squared\",\n        ylab = \"Posterior density\",\n        xlab = NULL, breaks = 25)\nabline(v = R2c, col = \"blue\", lwd = 4)\n\n\n\nThis plot shows the posterior \\(R^2_{GLMM}\\) distributions for both the marginal\nand conditional cases, with the point estimates generated with glmer shown as\nvertical blue lines. Personally, I find it to be a bit more informative and\nintuitive to think of \\(R^2\\) as a probability distribution that integrates\nuncertainty in its component parameters. That said, it is unconventional to\nrepresent \\(R^2\\) in this way, which could compromise the ease with which this\nhandy statistic can be explained to the uninitiated (e.g. first year biology\nundergraduates). But, being a derived parameter, those wishing to generate a\nposterior can do so relatively easily.\n\n\n\nNakagawa, Shinichi, and Holger Schielzeth. 2010. “Repeatability for Gaussian and Non-Gaussian Data: A Practical Guide for Biologists.” Biological Reviews 85 (4): 935–56. https://doi.org/10.1111/j.1469-185X.2010.00141.x.\n\n\n———. 2013. “A General and Simple Method for Obtaining R2 from Generalized Linear Mixed-Effects Models.” Methods in Ecology and Evolution 4 (2): 133–42. https://doi.org/10.1111/j.2041-210x.2012.00261.x.\n\n\n\n\n",
    "preview": "posts/2018-12-24-quantifying-uncertainty-around-r-squared-for-generalized-linear-models/quantifying-uncertainty-around-r-squared-for-generalized-linear-models_files/figure-html5/process-posterior-draws-1.png",
    "last_modified": "2022-11-01T21:49:22-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-24-clarifying-vague-interactions/",
    "title": "Clarifying vague interactions",
    "description": "One quick way to improve reporting of interaction effects in linear models.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-08-18",
    "categories": [
      "visualization",
      "rants"
    ],
    "contents": "\nIt is easy to present linear model results with vague or unintelligible\ninteraction effects. One way to be vague when presenting interaction effects is\nto provide only a table of model coefficients, including no information on the\nrange of covariate values observed, and no plots to aid in interpretation.\nHere’s an example:\nSuppose you have discovered a statistically significant interaction effect\nbetween two continous covariates in the context of a linear model.\n\\[ y_i \\sim \\text{Normal}(\\mu_i, \\sigma^2) \\]\n\\[ \\mu_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{1i} x_{2i} \\]\nSuppose also that you have decided to present the model results with the\nfollowing table, and the reviewers requested no additional information:\n\nEstimate\nSE\nP-value\n\\(\\beta_0\\)\n-0.004\n0.037\n0.921\n\\(\\beta_1\\)\n1.055\n0.038\n<0.05\n\\(\\beta_2\\)\n-0.496\n0.037\n<0.05\n\\(\\beta_3\\)\n2.002\n0.040\n<0.05\nRSE\n0.517\n\n\n\n\n\n\nWithout knowing the range of covariate values observed, this table gives an\nincomplete story about relationship between the covariates and the response\nvariable. Assuming the reader has a decent guess about the range of possible\nvalues for the covariates, this is what they can piece together:\n\n\n# parameter estimates\nbeta0 <- -.004\nbeta1 <- 1.055\nbeta2 <- -.496\nbeta3 <- 2.002\n\n# reader's guess: range of possible covariate values\nx1 <- seq(-5, 5, .1)\nx2 <- seq(-5, 5, .1)\nX <- expand.grid(x1=x1, x2=x2)\n\n# reader's attempt to know how the covariates relate to E(y)\nmu <- with(X, beta0 + beta1*x1 + beta2*x2 + beta3*x1*x2)\n\nrequire(ggplot2)\nd <- data.frame(mu=mu, x1=X$x1, x2=X$x2)\np1 <- ggplot(d, aes(x1, x2, z=mu)) + theme_bw() +\n  geom_tile(aes(fill=mu)) +\n  stat_contour(binwidth=1.5, color = 'black', alpha = .3) +\n  scale_fill_gradient2(low=\"blue\", mid=\"white\", high=\"orange\") +\n  xlab(\"Covariate 1\") + ylab(\"Covariate 2\") +\n  ggtitle(\"Contour plot of the linear predictor\")\np1\n\n\n\nIf the reader does not know where the observations fell in this plot, it is difficult to know whether the response variable was increasing or decreasing with each covariate across the range of observed values.\nConsider the following two cases, where the observed covariate combinations are included as points.\n\n\nn_point <- 100\nset.seed(1234)\np1 + \n  geom_point(data = data.frame(x1 = rnorm(n_point), \n                               x2 = rnorm(n_point)), \n             aes(x1, x2), inherit.aes = FALSE)\n\n\n\n\n\nset.seed(1234)\np1 + \n  geom_point(data = data.frame(x1 = runif(n_point, max = 5), \n                               x2 = runif(n_point, max = 5)), \n             aes(x1, x2), inherit.aes = FALSE)\n\n\n\nThese two plots tell different stories despite identical model\nparameters. In the second case, across the range of observed covariates, the\nexpected value of \\(y\\) increases as either covariate increases and the\ninteraction term affects the magnitude this increase. In the first case,\nincreases in covariate 1 or 2 could increase or decrease \\(\\mu\\), depending on the\nvalue of the other covariate.\nI won’t get into the nitty gritty of how to present interaction effects (but if\nyou’re interested, there are articles out there (Lamina et al. 2012). My\nmain goal here is to point out the ambiguity associated with only presenting a\ntable of parameter estimates. My preference would be that authors at least\npresent observed covariate ranges (or better yet values), and provide a plot\nthat illustrates the interaction.\n\n\n\nLamina, Claudia, Gisela Sturm, Barbara Kollerits, and Florian Kronenberg. 2012. “Visualizing Interaction Effects: A Proposal for Presentation and Interpretation.” Journal of Clinical Epidemiology 65 (8): 855–62. https://doi.org/10.1016/j.jclinepi.2012.02.013.\n\n\n\n\n",
    "preview": "posts/2018-12-24-clarifying-vague-interactions/clarifying-vague-interactions_files/figure-html5/simulate-data-1.png",
    "last_modified": "2022-11-01T21:48:55-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-23-split-violin-plots/",
    "title": "Split violin plots",
    "description": "Comparing distributions with split violin plots in R.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-06-24",
    "categories": [
      "visualization"
    ],
    "contents": "\nViolin plots are useful for comparing distributions. When data are\ngrouped by a factor with two levels (e.g. males and females), you can\nsplit the violins in half to see the difference between groups. Consider\na 2 x 2 factorial experiment: treatments A and B are crossed with groups\n1 and 2, with N=1000.\n\n\n\nBoxplots are often used:\n\n\n\nThis gives us a rough comparison of the distribution in each group,\nbut sometimes it’s nice to visualize the kernel density estimates instead.\nI recently ran into this issue and tweaked the vioplot() function from\nthe vioplot\npackage by Daniel Adler to make split violin plots.\nWith vioplot2(), the side\nargument specifies whether to plot the density on “both”, the “left”, or\nthe “right” side.\n\n\n\nLast but not least, Peter Kampstra’s\nbeanplot\npackage uses beanplot() to make split\ndensity plots, but 1) plots a rug rather\nthan a quantile box, 2) includes a line for the overall mean or median,\nand 3) makes it easier to change the kernel function.\n\n\n\nThere are\nmore\nways\nthan\none\nto\nskin\na\ncat,\nand what one uses will probably come to personal preference.\n\n\n\n",
    "preview": "posts/2018-12-23-split-violin-plots/split-violin-plots_files/figure-html5/beanplots-1.png",
    "last_modified": "2022-11-01T21:48:52-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-23-bayesian-model-ii-regression/",
    "title": "Bayesian model II regression in JAGS",
    "description": "Fitting a regression model with uncertainty in the explanatory variable.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-05-27",
    "categories": [
      "jags"
    ],
    "contents": "\nRegression is a mainstay of ecological and evolutionary data analysis. For\nexample, a disease ecologist may use body size (e.g. a weight from a scale with\nmeasurement error) to predict infection. Classical linear regression assumes no\nerror in covariates; they are known exactly. This is rarely the case in ecology,\nand ignoring error in covariates can bias regression coefficient estimates. This\nis where model II (aka errors-in variables and measurement errors) regression\nmodels come in handy. Here I’ll demonstrate how to construct such a model in a\nBayesian framework, where substantive prior knowledge of covariate error\nfacilitates less-biased parameter estimates.\nHere’s a quick illustration of the problem: I’ll generate data from a known\nsimple linear regression model, and fit models that ignore or incorporate error\nin the covariate.\n\n\nlibrary(rjags)\nlibrary(ggmcmc)\n\n# simulate covariate data\nn <- 40\nsdx <- 6\nsdobs <- 5\ntaux <- 1 / (sdobs * sdobs)\ntruex <- rnorm(n, 0, sdx)\nerrorx <- rnorm(n, 0, sdobs)\nobsx <- truex + errorx\n\n# simulate response data\nalpha <- 0\nbeta <- 10\nsdy <- 20\nerrory <- rnorm(n, 0, sdy)\nobsy <- alpha + beta*truex + errory\nobserved_data <- data.frame(obsx = obsx, obsy = obsy)\nparms <- data.frame(alpha, beta)\n\n\nIgnoring error in the covariate:\n\n\n# bundle data\njags_d <- list(x = obsx, y = obsy, n = length(obsx))\n\n# write model\ncat(\"\n    model{\n## Priors\nalpha ~ dnorm(0, .001)\nbeta ~ dnorm(0, .001)\nsdy ~ dunif(0, 100)\ntauy <- 1 / (sdy * sdy)\n\n## Likelihood\n  for (i in 1:n){\n    mu[i] <- alpha + beta * x[i]\n    y[i] ~ dnorm(mu[i], tauy)\n  }\n}\n\",\n    fill=TRUE, file=\"yerror.txt\")\n\n# initiate model\nmod1 <- jags.model(\"yerror.txt\", data=jags_d,\n                   n.chains=3, n.adapt=1000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 40\n   Unobserved stochastic nodes: 3\n   Total graph size: 170\n\nInitializing model\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |+                                                 |   2%\n  |                                                        \n  |++                                                |   4%\n  |                                                        \n  |+++                                               |   6%\n  |                                                        \n  |++++                                              |   8%\n  |                                                        \n  |+++++                                             |  10%\n  |                                                        \n  |++++++                                            |  12%\n  |                                                        \n  |+++++++                                           |  14%\n  |                                                        \n  |++++++++                                          |  16%\n  |                                                        \n  |+++++++++                                         |  18%\n  |                                                        \n  |++++++++++                                        |  20%\n  |                                                        \n  |+++++++++++                                       |  22%\n  |                                                        \n  |++++++++++++                                      |  24%\n  |                                                        \n  |+++++++++++++                                     |  26%\n  |                                                        \n  |++++++++++++++                                    |  28%\n  |                                                        \n  |+++++++++++++++                                   |  30%\n  |                                                        \n  |++++++++++++++++                                  |  32%\n  |                                                        \n  |+++++++++++++++++                                 |  34%\n  |                                                        \n  |++++++++++++++++++                                |  36%\n  |                                                        \n  |+++++++++++++++++++                               |  38%\n  |                                                        \n  |++++++++++++++++++++                              |  40%\n  |                                                        \n  |+++++++++++++++++++++                             |  42%\n  |                                                        \n  |++++++++++++++++++++++                            |  44%\n  |                                                        \n  |+++++++++++++++++++++++                           |  46%\n  |                                                        \n  |++++++++++++++++++++++++                          |  48%\n  |                                                        \n  |+++++++++++++++++++++++++                         |  50%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  52%\n  |                                                        \n  |+++++++++++++++++++++++++++                       |  54%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  56%\n  |                                                        \n  |+++++++++++++++++++++++++++++                     |  58%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  60%\n  |                                                        \n  |+++++++++++++++++++++++++++++++                   |  62%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  64%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++                 |  66%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  68%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++               |  70%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  72%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++             |  74%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  76%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++           |  78%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  80%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++         |  82%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  84%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++       |  86%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  88%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  92%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++   |  94%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++++ |  98%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%\n\n# simulate posterior\nout <- coda.samples(mod1, n.iter=1000, thin=1,\n                    variable.names=c(\"alpha\", \"beta\", \"sdy\"))\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************| 100%\n\n# store parameter estimates\nggd <- ggs(out)\na <- ggd$value[which(ggd$Parameter == \"alpha\")]\nb <- ggd$value[which(ggd$Parameter == \"beta\")]\nd <- data.frame(a, b)\n\n\nIncorporating error in the covariate: I’m assuming that we have substantive\nknowledge about covariate measurement represented in the prior for the precision\nin X. Further, the prior for the true X values reflects knowledge of the\ndistribution of our X value in the population from which the sample was taken.\n\n\n# specify model\ncat(\"\n    model {\n## Priors\nalpha ~ dnorm(0, .001)\nbeta ~ dnorm(0, .001)\nsdy ~ dunif(0, 100)\ntauy <- 1 / (sdy * sdy)\ntaux ~ dunif(.03, .05)\n\n## Likelihood\n  for (i in 1:n){\n    truex[i] ~ dnorm(0, .04)\n    x[i] ~ dnorm(truex[i], taux)\n    y[i] ~ dnorm(mu[i], tauy)\n    mu[i] <- alpha + beta * truex[i]\n  }\n}\n    \", fill=T, file=\"xyerror.txt\")\n\n# bundle data\njags_d <- list(x = obsx, y = obsy, n = length(obsx))\n\n# initiate model\nmod2 <- jags.model(\"xyerror.txt\", data=jags_d,\n                   n.chains=3, n.adapt=1000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 80\n   Unobserved stochastic nodes: 44\n   Total graph size: 214\n\nInitializing model\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |+                                                 |   2%\n  |                                                        \n  |++                                                |   4%\n  |                                                        \n  |+++                                               |   6%\n  |                                                        \n  |++++                                              |   8%\n  |                                                        \n  |+++++                                             |  10%\n  |                                                        \n  |++++++                                            |  12%\n  |                                                        \n  |+++++++                                           |  14%\n  |                                                        \n  |++++++++                                          |  16%\n  |                                                        \n  |+++++++++                                         |  18%\n  |                                                        \n  |++++++++++                                        |  20%\n  |                                                        \n  |+++++++++++                                       |  22%\n  |                                                        \n  |++++++++++++                                      |  24%\n  |                                                        \n  |+++++++++++++                                     |  26%\n  |                                                        \n  |++++++++++++++                                    |  28%\n  |                                                        \n  |+++++++++++++++                                   |  30%\n  |                                                        \n  |++++++++++++++++                                  |  32%\n  |                                                        \n  |+++++++++++++++++                                 |  34%\n  |                                                        \n  |++++++++++++++++++                                |  36%\n  |                                                        \n  |+++++++++++++++++++                               |  38%\n  |                                                        \n  |++++++++++++++++++++                              |  40%\n  |                                                        \n  |+++++++++++++++++++++                             |  42%\n  |                                                        \n  |++++++++++++++++++++++                            |  44%\n  |                                                        \n  |+++++++++++++++++++++++                           |  46%\n  |                                                        \n  |++++++++++++++++++++++++                          |  48%\n  |                                                        \n  |+++++++++++++++++++++++++                         |  50%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  52%\n  |                                                        \n  |+++++++++++++++++++++++++++                       |  54%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  56%\n  |                                                        \n  |+++++++++++++++++++++++++++++                     |  58%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  60%\n  |                                                        \n  |+++++++++++++++++++++++++++++++                   |  62%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  64%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++                 |  66%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  68%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++               |  70%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  72%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++             |  74%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  76%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++           |  78%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  80%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++         |  82%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  84%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++       |  86%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  88%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  92%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++   |  94%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++++ |  98%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%\n\n# simulate posterior\nout <- coda.samples(mod2, n.iter=30000, thin=30,\n                    variable.names=c(\"alpha\", \"beta\", \"tauy\", \"taux\"))\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************| 100%\n\n# store parameter estimates\nggd <- ggs(out)\na2 <- ggd$value[which(ggd$Parameter == \"alpha\")]\nb2 <- ggd$value[which(ggd$Parameter == \"beta\")]\nd2 <- data.frame(a2, b2)\n\n\nNow let’s see how the two models perform.\n\n\nggplot(observed_data, aes(x=obsx, obsy)) +\n  geom_abline(aes(intercept=a, slope=b), data=d, \n              color=\"red\", alpha=0.05) +\n  geom_abline(aes(intercept=a2, slope=b2), data=d2, \n              color=\"dodgerblue\", alpha=0.05) +\n  geom_abline(aes(intercept=alpha, slope=beta),\n              data=parms, color=\"green\", size=1.5, linetype=\"dashed\") +\n  geom_point(shape=19, size=3) +\n  theme_minimal() +\n  xlab(\"X values\") + \n  ylab(\"Observed Y values\") +\n  ggtitle(\"Model results with and without modeling error in X\")\n\n\n\nThe dashed green line shows the model that generated the data, i.e. the “true”\nline. The red lines show the posterior for the naive model ignoring error in X,\nwhile the less-biased blue lines show the posterior for the model incorporating\nerror in X.\n\n\n\n",
    "preview": "posts/2018-12-23-bayesian-model-ii-regression/bayesian-model-ii-regression_files/figure-html5/compare-models-1.png",
    "last_modified": "2022-11-01T21:48:33-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-23-modeling-habitat-diversity-and-species-richness/",
    "title": "Modeling habitat diversity and species richness",
    "description": "Experimenting with an agent based model of habitat diversity and species richness in R.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-04-20",
    "categories": [],
    "contents": "\nHow does habitat diversity affect species richness? Perhaps intuition suggests\nthat habitat diversity increases species richness by facilitating niche or\nresource partitioning among species. But, for a fixed area, as habitat\nheterogeneity increases, the area that can be allocated to each habitat type\ndecreases. A recent paper provides a theoretical and empirical treatment of the\nhabitat area-heterogeneity trade-off’s consequences for species richness\n(Allouche et al. 2012). Both treatments of the subject indicated that the\nrelationship between habitat heterogeneity and species richness may be unimodal,\nrather than strictly increasing.\nConceptually, this is expected to occur when on the left side of the curve,\nincreasing habitat heterogeneity opens up new regions in niche space,\nfacilitating colonization by new species. However, as heterogeneity continues to\nincrease, each species has fewer habitat patches to utilize, population sizes\ndecrease, and local extinction risk increases due to demographic stochasticity.\nTo explore this idea theoretically, Allouche et al. (2012) developed an\nindividually based model using a continuous time Markov process. The details of\ntheir modeling approach can be found in the\nsupplement to their article, which I\nrecommend. In this post, I’ll demonstrate how to implement a discrete time\nversion of their model in R. Thanks to the agent-based modeling working group at\nthe University of Colorado for providing motivation to code up model in R.\nModel structure\nThis model is spatially implicit, with \\(A\\) equally connected sites. Each site\nfalls on an environmental condition axis, receiving some value \\(E\\) that\ncharacterizes local conditions. The environmental conditions for each site are\nuniformly distributed between two values that dictate the range of environmental\nconditions in a focal area. The local range of environmental conditions is a\nsubset of some global range. There are \\(N\\) species in the regional pool that can\ncolonize habitat patches. Each species has some environmental optimum \\(\\mu_i\\),\nand some niche width \\(\\sigma_i\\), which together define a Gaussian function for\nthe probability of establishment given a colonization attempt and a habitat\npatch environmental condition \\(E\\).\nIt is assumed that all individuals that occupy a patch have the same\nper-timestep probabilities of death and reproduction. If an individual\nreproduces, the number of offspring it produces is a Poisson distributed random\nvariable, and each individual offspring attempts to colonize one randomly\nselected site. At each time-step, every site has an equal probability of a\ncolonization attempt by an individual from each species in the regional pool.\nEvery habitat patch holds only one individual.\nOffspring and immigrants from the regional pool do not displace individuals from\nhabitat patches when they attempt to colonize. In empty sites, offspring receive\ncolonization priority, with regional colonization occurring after breeding. When\nmultiple offspring or immigrants from the regional pool could establish in an\nempty site, one successful individual is randomly chosen to establish regardless\nof species identity.\nParameters\nThe following parameters are supplied to the function alloucheIBM():\nA = number of sites; N = number of species in the regional pool;\nERmin = global environmental conditions minimum; ERmax = global\nenvironmental conditions maximum; Emin = local environmental minimum;\nEmax = local environmental maximum; sig = niche width standard deviation for\nall species; pM = per timestep probability of mortality; pR = per timestep\nprobability of reproduction; R = per capita expected number of offspring; and\nI = per timestep probability of attempted colonization by an immigrant from\nthe regional pool for each patch.\nImplementation in R\nThe function alloucheIBM() does the majority of work for this model:\n\n\n\nThe function returns a list containing a vector of species richness at each\ntimestep, the proportion of sites occupied at each timestep, a state array\ncontaining all occupancy information for each patch, species, and timestep, and\nlastly a dataframe containing information on the niches of each species in the\nregional pool.\nUsing this function we can simulate richness through time:\n\n\n\nFinally, we can address the issue of habitat heterogeneity and its effect on\nspecies richness. There are many ways to approach this issue, and many parameter\ncombinations to consider. Allouche et al. (2012) provides a thorough treatment\nof the subject; I’ll demonstrate just one result: that under certain conditions,\nspecies richness peaks at intermediate levels of habitat heterogeneity.\nTo construct a range of habitat heterogeneity values, let’s construct an\ninterval and take subsequently narrower intervals centered around the middle of\nthe original interval.\n\n\n\nNow, for each interval, we can iteratively run the model and track species\nrichness. Because species richness tends to vary through time, let’s take the\nmean of the final 100 timesteps as a measure of species richness for each model\nrun, and record the standard deviation to track variability.\n\n\n\nOf course, the shape of this relationship is sensitive to the parameters. As an\nexample, changing niche width to increase or decrease niche overlap will mediate\nthe strength of interspecific competition for space. Also, increasing\nreproductive rates may buffer each species from stochastic extinction so that\nthe relationship between environmental heterogeneity and richness is\nmonotonically increasing. Furthermore, here I centered all intervals around the\nsame value, but the exact position of the environmental heterogeneity interval\nwill affect the net establishment probability for each site, depending on how\nthe interval relates to species niches. The parameter space is yours to explore.\nThese types of stochastic simulation models are fairly straightforward to\nimplement in R. Indeed there’s a package dedicated to facilitating the\nimplementation of such models: simecol.\nThere’s even a book (Soetaert and Herman 2008).\n\n\n\nAllouche, Omri, Michael Kalyuzhny, Gregorio Moreno-Rueda, Manuel Pizarro, and Ronen Kadmon. 2012. “Area–Heterogeneity Tradeoff and the Diversity of Ecological Communities.” Proceedings of the National Academy of Sciences 109 (43): 17495–500. https://doi.org/10.1073/pnas.1208652109.\n\n\nSoetaert, Karline, and Peter MJ Herman. 2008. A Practical Guide to Ecological Modelling: Using r as a Simulation Platform. Springer Science & Business Media.\n\n\n\n\n",
    "preview": "posts/2018-12-23-modeling-habitat-diversity-and-species-richness/modeling-habitat-diversity-and-species-richness_files/figure-html5/run-model-1.png",
    "last_modified": "2022-11-01T21:48:49-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-22-interactive-two-host-sir-model/",
    "title": "Interactive two host SIR model",
    "description": "Creating an interactive two host SIR model in R and shiny.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-02-20",
    "categories": [
      "shiny",
      "teaching"
    ],
    "contents": "\nThis is an example of interfacing R, shiny, and deSolve to produce an\ninteractive environment where users can explore model behavior by altering\nparameters in an easy to use GUI.\nThe model tracks the number of susceptible, infectious, and recovered\nindividuals in two co-occuring host species. The rates of change for each class\nare represented as a system of differential equations:\n\\[\\dot{S_1} = (b_1 - \\Delta_1N_1)N_1 - d_1S_1 - S_1 (\\beta_{11} I_1 + \\beta_{12} I_2)\\]\n\\[\\dot{I_1} =  S_1 (\\beta_{11} I_1 + \\beta_{12} I_2) - (d_1 + \\alpha_1 + \\sigma_1)I_1\\]\n\\[\\dot{R_1} = \\sigma_1 I_1 - d_1R_1\\]\n\\[\\dot{S_2} = (b_2 - \\Delta_2N_2)N_2 - d_2S_2 - S_2 (\\beta_{22} I_2 + \\beta_{21} I_1)\\]\n\\[\\dot{I_2} = S_2 (\\beta_{22} I_2 + \\beta_{21} I_1) - (d_2 + \\alpha_2 + \\sigma_2)I_2\\]\n\\[\\dot{R_2} = \\sigma_2 I_2 - d_2R_2\\]\nWhere \\(S_i\\), \\(I_i\\), and \\(R_i\\) represent the density of susceptible, infectious,\nand recovered individuals respectively of species \\(i\\). The total number of\nindividuals of each species is \\(N_i\\). Per capita birth and death rates are\nrepresented by \\(b_i\\) and \\(d_i\\), and the strength of density dependence in\npopulation growth is \\(\\delta_i\\). Transmission rates from species \\(j\\) to species\n\\(i\\) are given by \\(\\beta_{ij}\\). The pathogen imposes additional mortality for\ninfected individuals at rate \\(\\alpha_{i}\\), and infected individuals recover\nat rate \\(\\sigma_{i}\\) so that the average infectious period is\n\\(\\frac{1}{\\sigma_{i}}\\). Here, it is assumed that the pathogen does not\ncastrate its hosts. Thus, susceptible, infectious, and recovered individuals\nreproduce at the same rate.\nEpidemiological models often differentiate between two transmission dynamics.\nWith density-dependent transmission, the number of host contacts and\ntransmission events increases with the density of individuals (as shown in the\nabove system of equations). In contrast, with\nfrequency-dependent transmission, hosts have a constant contact rate so that\nthe transmission rate depends on the relative proportion of infectious\nindividuals. As an example, models of sexually transmitted infections often\nassume frequency dependent transmission, implying that the number of sexual\npartners one has is independent of population density. To incorporate frequency\ndependent transmission into the above model, it is necessary to divide the\ntransmission term \\(S\\sum{(\\beta I)}\\) by \\(N\\).\nBased on this system of equations, a criterion for pathogen invasion called\n\\(R_0\\) can be derived based on the dominant eigenvalue of the next generation\nmatrix (Dobson 2004). If \\(R_0 < 1\\), the pathogen does not invade; if \\(R_0>1\\),\nthe pathogen invades.\nBuilding the R shiny app\nShiny requires two files to run: a file containing all of the calculations,\nplotting functionality, etc., and a file defining a user interface.\nHere is the file\ndefining what you want the server to do. Note the use of ifelse() to have\neither density- or frequency-dependent transmission.\nHere is the file\ndefining the user interface.\nHere is a link to the resulting app.\n\n\nlibrary(imager)\nim <- load.image(\"https://raw.githubusercontent.com/mbjoseph/2hostSIR/master/tests/test-expected/001.png\")\nplot(im, axes = FALSE)\n\n\n\n\n\n\n",
    "preview": "posts/2018-12-22-interactive-two-host-sir-model/interactive-two-host-sir-model_files/figure-html5/plot-img-1.png",
    "last_modified": "2022-11-01T21:48:25-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-20-interactive-stage-structured-population-model/",
    "title": "Interactive stage-structured population model",
    "description": "Building an interactive stage-structured population model in R with shiny.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-02-16",
    "categories": [
      "teaching",
      "shiny"
    ],
    "contents": "\nThis is an example of interfacing R and shiny to allow users to explore a\nbiological model often encountered in an introductory ecology class.\nWe are interested the growth of a population that is composed of multiple,\ndiscrete stages or age classes. Patrick H. Leslie provides an in-depth\nderivation of the model in a 1945 paper (Leslie 1945).\nThe population at time \\(t\\) is represented by a vector \\(\\bar{x}_t\\), where each\nelement of the vector represents the number of individuals in each age class\n(e.g. if a population has \\(n\\) age classes, then \\(\\bar{x}_t\\) has \\(n\\) elements).\nTime is considered discrete and we assume that the population is censused prior\nto breeding. We assume that individuals within each age class are identical,\nand that each has some probability of maturing to the next age class, surviving\n(staying in the same age class), and reproduction. Changes in the population\nfrom one timestep to another are represented as:\n\\[\\bar{x}_{t+1}=L\\bar{x}_t\\]\nwhere \\(L\\) is an \\(n\\) x \\(n\\) Leslie matrix (or more generally, a projection\nmatrix) that describes the contribution of each age class to the population at\ntime \\(t+1\\).\nSuppose we are tasked with modeling the annual dynamics of a population with\nfour age classes, and \\(t\\) represents years. For simplicity, we model only\nfemales and assume that plenty of males are available for breeding.\nIndividuals in the first age class survive to class 2 with probability 0.1,\nclass 2 individuals survive to class 3 with probability 0.5, class 3\nindividuals survive to class 4 with probability 0.9, and class four\nindividuals survive each year with probability 0.7. Only the fourth age class\nis reproductive, with individuals producing 100 class 1 individuals per year.\nEquivalently, as a Leslie matrix:\n\\[\\left[\\begin{array}{rrrr}\n    0 & 0 & 0 & 100 \\\\\n    .1 & 0 & 0 & 0 \\\\\n    0 & .5 & 0 & 0 \\\\\n    0 & 0 & .9 & .7\n  \\end{array}\\right]\\]\nThe long term population growth rate is related to the dominant eigenvalue\n\\(\\lambda_{1}\\) of \\(L\\). If \\[\\lambda_{1} < 1\\], the population declines to\nextinction, and if \\[\\lambda_{1} > 1\\] the population increases.\nFrom a management perspective, it is often useful to know how limited resources\nmay be allocated to increase population growth or prevent extinction.\nIn other words, if an element \\[l_{ij}\\] such as fecundity or survival could be\nmanipulated by managers, how much would the long term population growth rate\nchange? To this end, one can calculate the sensitivity of the dominant\neigenvalue to small changes in \\(l_{ij}\\):\n\\[\n\\frac{\\partial \\lambda_{1}}{\\partial l_{ij}} = \\frac{(w_{1}){i}(v_{1})_{j}}{\\bar{w}_{1}^{T} \\bar{v}_{1}}\n\\]\nwhere \\(w_1\\) and \\(v_1\\) are left and right eigenvectors, respectively,\nassociated with the dominant eigenvalue. Because survival and fecundity are on\ndifferent scales, sensitivity is often scaled by a factor of\n\\(\\frac{L_{ij}}{\\lambda_1}\\) for a measure of elasticity.\nThe shiny app\nFiles are accessible in this repository.\nPlease feel free to clone for your own use and/or contribute.\nHere is a link to the resulting app.\n\n\n\n\n\n\nLeslie, Patrick H. 1945. “On the Use of Matrices in Certain Population Mathematics.” Biometrika, 183–212. http://dx.doi.org/10.2307/2332297.\n\n\n\n\n",
    "preview": "posts/2018-12-20-interactive-stage-structured-population-model/interactive-stage-structured-population-model_files/figure-html5/plot-img-1.png",
    "last_modified": "2022-11-01T21:48:05-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-22-dynamic-community-occupancy-model-jags/",
    "title": "Dynamic community occupancy modeling with R and JAGS",
    "description": "Multi-species, multi-timestep occupancy model in R and JAGS",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-02-04",
    "categories": [
      "jags"
    ],
    "contents": "\nThis post is intended to provide a simple example of how to construct and make\ninferences on a multi-species multi-year occupancy model using R, JAGS, and the\n‘rjags’ package. This is not intended to be a standalone tutorial on dynamic\ncommunity occupancy modeling (MacKenzie et al. 2002; Royle and Kéry 2007; Kéry and Royle 2008; Dorazio et al. 2010).\nRoyle and Dorazio’s\nHierarchichal Modeling and Inference in Ecology also provides a clear\nexplanation of simple one species occupancy models, multispecies occupancy\nmodels, and dynamic (multiyear) occupancy models, among other things (Royle and Dorazio 2008).\nThere’s also a wealth of code provided\nhere by Elise Zipkin,\nJ. Andrew Royle, and others.\nBefore getting started, we can define two convenience functions:\n\n\nlibrary(rjags)\n\nlogit <- function(x) {\n    log(x/(1 - x))\n}\n\nantilogit <- function(x) {\n    exp(x)/(1 + exp(x))\n}\n\n\nThen, initializing the number of sites, species, years, and repeat surveys\n(i.e. surveys within years, where the occupancy status of a site is assumed to\nbe constant),\n\n\nnsite <- 20\nnspec <- 3\nnyear <- 4\nnrep <- 2\n\n\nwe can begin to consider occupancy. We’re interested in making inferences about\nthe rates of colonization and population persistence for each species in a\ncommunity, while estimating and accounting for imperfect detection.\nOccupancy status at site \\(j\\), by species \\(i\\), in year \\(t\\) is represented by\n\\(z(j,i,t)\\). For occupied sites \\(z=1\\); for unoccupied sites \\(z=0\\).\nHowever, \\(Z\\) is incompletely observed: it is possible that a species \\(i\\) is\npresent at a site \\(j\\) in some year \\(t\\) (\\(z(j,i,t)=1\\)) but species \\(i\\) was never\nseen at at site \\(j\\) in year \\(t\\) across all \\(k\\) repeat surveys because of\nimperfect detection. These observations are represented by \\(x(j,i,t,k)\\).\nHere we assume that there are no “false positive” observations.\nIn other words, if \\(\\sum_{1}^{k}x(j,i,t,k)>0\\) , then \\(z(j,i,t)=1\\).\nIf a site is occupied, the probability that \\(x(j,i,t,k)=1\\) is represented as a\nBernoulli trial with probability of detection \\(p(j,i,t,k)\\), such that\n\\[\nx(j,i,t,k) \\sim \\text{Bernoulli}(z(j,i,t)p(j,i,t,k))\n\\]\nThe occupancy status \\(z\\) of species \\(i\\) at site \\(j\\) in year \\(t\\) is modeled as a\nMarkov Bernoulli trial. In other words whether a species is present at a site\nin year \\(t\\) is influenced by whether it was present at year \\(t−1\\).\n\\[\nz(j,i,t) \\sim \\text{Bernoulli}(\\psi(j,i,t))\n\\]\nwhere for \\(t>1\\)\n\\[\n\\text{logit}(\\psi_{j,i,t})=\\beta_i + \\rho_i z(i, j, t-1)\n\\]\nand in year one \\((t=1)\\)\n\\[\n\\text{logit}(\\psi_{j,i,1})=\\beta_i + \\rho_i z_0(i, j)\n\\]\nwhere the occupancy status in year 0, \\(z_0(i,j) \\sim \\text{Bernoulli}(\\rho_{0i})\\), and\n\\(\\rho_{0i} \\sim \\text{Uniform}(0,1)\\). \\(\\beta_i\\) and \\(\\rho_i\\) are parameters that\ncontrol the probabilities of colonization and persistence.\nIf a site was unoccupied by species \\(i\\) in a previous year \\(z(i,j,t−1)=0\\), then\nthe probability of colonization is given by the antilogit of \\(\\beta_i\\).\nIf a site was previously occupied \\(z(i,j,t−1)=1\\), the probability of population\npersistence is given by the anitlogit of \\(\\beta_i + \\rho_i\\). We assume that\nthe distributions of species specific parameters are defined by community level\nhyperparameters such that \\(\\beta_i \\sim \\text{Normal}(\\mu_\\beta, \\sigma_\\beta)\\) and\n\\(rho_i \\sim \\text{Normal}(\\mu_\\rho, \\sigma_\\rho)\\). We can generate occupancy data as\nfollows:\n\n\n# community level hyperparameters\nmubeta <- 1\nsdbeta <- 0.2\n\nmurho <- -2\nsdrho <- .1\n\n# species specific random effects\nset.seed(1)  # for reproducibility\nbeta <- rnorm(nspec, mubeta, sdbeta)\nset.seed(1008)\nrho <- rnorm(nspec, murho, sdrho)\n\n# initial occupancy states\nset.seed(237)\nrho0 <- runif(nspec, 0, 1)\nz0 <- array(dim = c(nsite, nspec))\nfor (i in 1:nspec) {\n    z0[, i] <- rbinom(nsite, 1, rho0[i])\n}\n\n# subsequent occupancy\nz <- array(dim = c(nsite, nspec, nyear))\nlpsi <- array(dim = c(nsite, nspec, nyear))\npsi <- array(dim = c(nsite, nspec, nyear))\nfor (j in 1:nsite) {\n    for (i in 1:nspec) {\n        for (t in 1:nyear) {\n            if (t == 1) {\n                lpsi[j, i, t] <- beta[i] + rho[i] * z0[j, i]\n                psi[j, i, t] <- antilogit(lpsi[j, i, t])\n                z[j, i, t] <- rbinom(1, 1, psi[j, i, t])\n            } else {\n                lpsi[j, i, t] <- beta[i] + rho[i] * z[j, i, t - 1]\n                psi[j, i, t] <- antilogit(lpsi[j, i, t])\n                z[j, i, t] <- rbinom(1, 1, psi[j, i, t])\n            }\n        }\n    }\n}\n\n\nFor simplicity, we’ll assume that there are no differences in species\ndetectability among sites, years, or repeat surveys, but that detectability\nvaries among species. We’ll again use hyperparameters to specify a distribution\nof detection probabilities in our community, such that\n\\(\\text{logit}(p_i) \\sim \\text{Normal}(\\mu_p, \\sigma_p)\\).\n\n\np_p <- 0.7\nmup <- logit(p_p)\nsdp <- 1.5\nset.seed(222)\nlp <- rnorm(nspec, mup, sdp)\np <- antilogit(lp)\n\n\nWe can now generate our observations based on occupancy states and detection\nprobabilities. Although this could be vectorized for speed, let’s stick with\nnested for loops in the interest of clarity.\n\n\nx <- array(dim = c(nsite, nspec, nyear, nrep))\nfor (j in 1:nsite) {\n    for (i in 1:nspec) {\n        for (t in 1:nyear) {\n            for (k in 1:nrep) {\n                x[j, i, t, k] <- rbinom(1, 1, p[i] * z[j, i, t])\n            }\n        }\n    }\n}\n\n\nNow that we’ve collected some data, we can specify our model:\n\n\ncat(\"\nmodel{\n  #### priors\n  # beta hyperparameters\n  p_beta ~ dbeta(1, 1)\n  mubeta <- log(p_beta / (1 - p_beta))\n  sigmabeta ~ dunif(0, 10)\n  taubeta <- (1 / (sigmabeta * sigmabeta))\n\n  # rho hyperparameters\n  p_rho ~ dbeta(1, 1)\n  murho <- log(p_rho / (1 - p_rho))\n  sigmarho~dunif(0,10)\n  taurho<-1/(sigmarho*sigmarho)\n\n  # p hyperparameters\n  p_p ~ dbeta(1, 1)\n  mup <- log(p_p / (1 - p_p))\n  sigmap ~ dunif(0,10)\n  taup <- (1 / (sigmap * sigmap))\n\n  #### occupancy model\n  # species specific random effects\n  for (i in 1:(nspec)) {\n    rho0[i] ~ dbeta(1, 1)\n    beta[i] ~ dnorm(mubeta, taubeta)\n    rho[i] ~ dnorm(murho, taurho)\n  }\n\n  # occupancy states\n  for (j in 1:nsite) {\n    for (i in 1:nspec) {\n      z0[j, i] ~ dbern(rho0[i])\n      logit(psi[j, i, 1]) <- beta[i] + rho[i] * z0[j, i]\n      z[j, i, 1] ~ dbern(psi[j, i, 1])\n      for (t in 2:nyear) {\n        logit(psi[j, i, t]) <- beta[i] + rho[i] * z[j, i, t-1]\n        z[j, i, t] ~ dbern(psi[j, i, t])\n      }\n    }\n  }\n\n  #### detection model\n  for(i in 1:nspec){\n    lp[i] ~ dnorm(mup, taup)\n    p[i] <- (exp(lp[i])) / (1 + exp(lp[i]))\n  }\n\n  #### observation model\n  for (j in 1:nsite){\n    for (i in 1:nspec){\n      for (t in 1:nyear){\n        mu[j, i, t] <- z[j, i, t] * p[i]\n        for (k in 1:nrep){\n          x[j, i, t, k] ~ dbern(mu[j, i, t])\n        }\n      }\n    }\n  }\n}\n\", fill=TRUE, file=\"com_occ.txt\")\n\n\nNext, bundle up the data.\n\n\ndata <- list(x = x, nrep = nrep, nsite = nsite, nspec = nspec, nyear = nyear)\n\n\nProvide initial values.\n\n\nzinit <- array(dim = c(nsite, nspec, nyear))\nfor (j in 1:nsite) {\n    for (i in 1:nspec) {\n        for (t in 1:nyear) {\n            zinit[j, i, t] <- max(x[j, i, t, ])\n        }\n    }\n}\n\ninits <- function() {\n    list(p_beta = runif(1, 0, 1), p_rho = runif(1, 0, 1), sigmarho = runif(1,\n        0, 1), sigmap = runif(1, 0, 10), sigmabeta = runif(1, 0, 10), z = zinit)\n}\n\n\nAs a side note, it is helpful in JAGS to provide initial values for the\nincompletely observed occupancy state \\(z\\) that are consistent with observed\npresences, as provided in this example with zinit. In other words if\n\\(x(j,i,t,k)=1\\), provide an intial value of 1 for \\(z(j,i,t)\\). Unlike WinBUGS and\nOpenBUGS, if you do not do this, you’ll often (but not always) encounter an\nerror message such as:\n# Error in jags.model(file = 'com_occ.txt', data = data, n.chains = 3) :\n# Error in node x[1,1,2,3] Observed node inconsistent with unobserved\n# parents at initialization\nNow we’re ready to monitor and make inferences about some parameters of\ninterest using JAGS.\n\n\nparams <- c(\"lp\", \"beta\", \"rho\")\nocmod <- jags.model(file = \"com_occ.txt\", inits = inits, data = data, \n                    n.chains = 2)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 480\n   Unobserved stochastic nodes: 318\n   Total graph size: 1789\n\nInitializing model\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |+                                                 |   2%\n  |                                                        \n  |++                                                |   4%\n  |                                                        \n  |+++                                               |   6%\n  |                                                        \n  |++++                                              |   8%\n  |                                                        \n  |+++++                                             |  10%\n  |                                                        \n  |++++++                                            |  12%\n  |                                                        \n  |+++++++                                           |  14%\n  |                                                        \n  |++++++++                                          |  16%\n  |                                                        \n  |+++++++++                                         |  18%\n  |                                                        \n  |++++++++++                                        |  20%\n  |                                                        \n  |+++++++++++                                       |  22%\n  |                                                        \n  |++++++++++++                                      |  24%\n  |                                                        \n  |+++++++++++++                                     |  26%\n  |                                                        \n  |++++++++++++++                                    |  28%\n  |                                                        \n  |+++++++++++++++                                   |  30%\n  |                                                        \n  |++++++++++++++++                                  |  32%\n  |                                                        \n  |+++++++++++++++++                                 |  34%\n  |                                                        \n  |++++++++++++++++++                                |  36%\n  |                                                        \n  |+++++++++++++++++++                               |  38%\n  |                                                        \n  |++++++++++++++++++++                              |  40%\n  |                                                        \n  |+++++++++++++++++++++                             |  42%\n  |                                                        \n  |++++++++++++++++++++++                            |  44%\n  |                                                        \n  |+++++++++++++++++++++++                           |  46%\n  |                                                        \n  |++++++++++++++++++++++++                          |  48%\n  |                                                        \n  |+++++++++++++++++++++++++                         |  50%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  52%\n  |                                                        \n  |+++++++++++++++++++++++++++                       |  54%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  56%\n  |                                                        \n  |+++++++++++++++++++++++++++++                     |  58%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  60%\n  |                                                        \n  |+++++++++++++++++++++++++++++++                   |  62%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  64%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++                 |  66%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  68%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++               |  70%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  72%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++             |  74%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  76%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++           |  78%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  80%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++         |  82%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  84%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++       |  86%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  88%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  92%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++   |  94%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++++ |  98%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%\n\nnburn <- 10000\nupdate(ocmod, n.iter = nburn)\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************| 100%\n\nout <- coda.samples(ocmod, n.iter = 10000, variable.names = params)\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************| 100%\n\nsummary(out)\n\n\nIterations = 11001:21000\nThinning interval = 1 \nNumber of chains = 2 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean     SD Naive SE Time-series SE\nbeta[1]  0.7679 0.3173 0.002244       0.007803\nbeta[2]  1.0692 0.5820 0.004115       0.025065\nbeta[3]  0.8474 0.3079 0.002177       0.006865\nlp[1]    4.3725 1.0684 0.007555       0.012143\nlp[2]    0.5402 0.2887 0.002041       0.004442\nlp[3]    3.6732 0.7496 0.005301       0.007816\nrho[1]  -1.8426 0.4896 0.003462       0.011560\nrho[2]  -2.4753 0.7493 0.005298       0.025575\nrho[3]  -2.1264 0.4960 0.003507       0.010948\n\n2. Quantiles for each variable:\n\n            2.5%     25%     50%     75%   97.5%\nbeta[1]  0.13043  0.5613  0.7683  0.9823  1.3799\nbeta[2]  0.32801  0.7354  0.9671  1.2474  2.5356\nbeta[3]  0.25697  0.6404  0.8386  1.0473  1.4730\nlp[1]    2.72967  3.6221  4.2195  4.9523  6.8553\nlp[2]   -0.01461  0.3429  0.5351  0.7323  1.1189\nlp[3]    2.41727  3.1468  3.5942  4.1151  5.3536\nrho[1]  -2.78718 -2.1701 -1.8526 -1.5200 -0.8629\nrho[2]  -4.23507 -2.8309 -2.3688 -1.9814 -1.3360\nrho[3]  -3.14440 -2.4444 -2.1116 -1.7889 -1.2037\n\nplot(out)\n\n\n\n\n\n\nDorazio, Robert M, Marc Kery, J Andrew Royle, and Matthias Plattner. 2010. “Models for Inference in Dynamic Metacommunity Systems.” Ecology 91 (8): 2466–75. https://doi.org/10.1890/09-1033.1.\n\n\nKéry, M, and J Andrew Royle. 2008. “Hierarchical Bayes Estimation of Species Richness and Occupancy in Spatially Replicated Surveys.” Journal of Applied Ecology 45 (2): 589–98. https://doi.org/10.1111/j.1365-2664.2007.01441.x.\n\n\nMacKenzie, Darryl I, James D Nichols, Gideon B Lachman, Sam Droege, J Andrew Royle, and Catherine A Langtimm. 2002. “Estimating Site Occupancy Rates When Detection Probabilities Are Less Than One.” Ecology 83 (8): 2248–55. https://doi.org/10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2.\n\n\nRoyle, J Andrew, and Robert M Dorazio. 2008. Hierarchical Modeling and Inference in Ecology: The Analysis of Data from Populations, Metapopulations and Communities. Elsevier. https://doi.org/10.1016/B978-0-12-374097-7.50001-5.\n\n\nRoyle, J Andrew, and Marc Kéry. 2007. “A Bayesian State-Space Formulation of Dynamic Occupancy Models.” Ecology 88 (7): 1813–23. https://doi.org/10.1890/06-0669.1.\n\n\n\n\n",
    "preview": "posts/2018-12-22-dynamic-community-occupancy-model-jags/dynamic-community-occupancy-model-jags_files/figure-html5/fit-model-1.png",
    "last_modified": "2022-11-01T21:48:22-06:00",
    "input_file": {}
  }
]
