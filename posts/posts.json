[
  {
    "path": "posts/2020-04-28-a-step-by-step-guide-to-marginalizing-over-discrete-parameters-for-ecologists-using-stan/",
    "title": "A step-by-step guide to marginalizing over discrete parameters for ecologists using Stan",
    "description": "Everything you might have been afraid to ask about implementing models with \ndiscrete parameters in Stan. Written for ecologists that know BUGS, JAGS, or NIMBLE, and want to use Stan. Provides an example by marginalizing\nover partly observed presence/absence states in a simple occupancy model.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2020-04-28",
    "categories": [],
    "contents": "\nDiscrete parameters can be a major stumbling block for ecologists using Stan, because you need to marginalize over the latent discrete parameters (e.g., “alive/dead”, “occupied/not occupied”, “infected/not infected”, etc.).\nThis post demonstrates how to do it, step by step for a simple example.\nI’ll try to make it clear what we are doing along the way, as we work towards a model that we can represent in Stan.\nConsider a Bayesian site occupancy model.\nWe want to estimate occurrence states (presence or absence) using observed detection/non-detection data (MacKenzie et al. 2002).\nFor sites \\(i=1, ...,N\\) we have \\(K\\) replicate sampling occasions.\nOn each sampling occasion, we visit site \\(i\\) and look for a critter (a bug, a bird, a plant, etc.).\nRepresent the number of sampling occasions where we detected the critter at site \\(i\\) as \\(y_i\\).\nSo if we see the critter on two surveys, \\(y_i = 2\\).\nWe assume that if a site is occupied (\\(z_i=1\\)), we detect the animal with probability \\(p\\).\nIf a site is not occupied (\\(z_i=0\\)), we can’t observe it.\nHow a BUGS/JAGS/NIMBLE user might write the model\nWe can write the observation model for site \\(i\\) as:\n\\[y_i \\sim \\text{Binomial}(K, p z_i).\\]\n\nThis assumes that detections are independent across surveys \\(k=1, ..., K\\).\nAnd our prior for the occupancy state of site \\(i\\) is:\n\\[z_i \\sim \\text{Bernoulli}(\\psi),\\]\nwhere \\(\\psi\\) is the probability of occupancy.\nA Bayesian model specification is completed by assigning priors to the remaining parameters:\n\\[p \\sim \\text{Uniform}(0, 1),\\]\n\\[\\psi \\sim \\text{Uniform}(0, 1).\\]\nSquare bracket probability notation\nLet’s rewrite the model in a different way.\nFirst, I want to introduce a different notation for our observation model:\n\\[[y_i \\mid p, z_i] = \\text{Binomial}(y_i \\mid K, p z_i).\\]\nIn this notation, square brackets represent probability mass or density functions (for discrete or continuous quantities, respectively).\nHere, \\([y_i \\mid p, z_i]\\) is the probability mass function of \\(y_i\\) conditioned on the parameters \\(p\\) and \\(z_i\\).\nIf we assume that the observations \\(y_{1:N} = y_1, ..., y_N\\) for each site are conditionally independent, we can write the observation model for all sites \\(1:N\\) as:\n\\[[y_{1:N} \\mid p, z_{1:N}] = \\prod_{i=1}^N [y_i \\mid p, z_i],\\]\nwhich follows from the definition of the joint distribution of independent random variables.\nWe can rewrite the rest of the model in the same notation.\nThe state model for site \\(i\\) becomes:\n\\[[z_i \\mid \\psi] = \\text{Bernoulli}(z_i \\mid \\psi).\\]\nAnd, again if we assume that the occupancy states for site \\(i=1, ..., N\\) are conditionally independent, then we can write down the state model for all sites as:\n\\[[z_{1:N} \\mid \\psi] = \\prod_{i=1}^N \\text{Bernoulli}(z_i \\mid \\psi).\\]\nFinally, we can write the priors in square bracket notation:\n\\[[\\psi] = \\text{Uniform}(\\psi \\mid 0, 1),\\]\n\\[[p] = \\text{Uniform}(p \\mid 0, 1).\\]\nWriting the joint distribution\nWe are working towards a model specification that we can use in Stan, which means we need the log of the joint distribution of data and parameters.\n\nIf you just mouthed the words “what the ****”, stay with me.\nThe “joint distribution of data and parameters” is the numerator in Bayes’ theorem, which gives us an expression for the posterior probability distribution of parameters \\(\\theta\\) given data \\(y\\):\n\\[[\\theta \\mid y] = \\dfrac{[y, \\theta]}{[y]}.\\]\nIn this example, our parameters \\(\\theta\\) are:\n\\(z_{1:N}\\): the occupancy states\n\\(p\\): the detection probability\n\\(\\psi\\): the occupancy probability\nThe data consist of the counts \\(y_{1:N}\\).\nSo our joint distribution is:\n\\[[y_{1:N}, \\theta] = [y_{1:N}, z_{1:N}, p, \\psi],\\]\nWe can factor this using the rules of conditional probability and the components we worked out in the previous section.\nFirst, recognize that:\n\\(y\\) depends on \\(p\\) and \\(z\\),\n\\(z\\) depends on \\(\\psi\\), and\n\\(p\\) and \\(\\psi\\) don’t depend on any other parameters:\n\\[[y_{1:N}, z_{1:N}, p, \\psi] = [y_{1:N} \\mid p, z_{1:N}] \\times [z_{1:N} \\mid \\psi] \\times [p, \\psi].\\]\nThen, recall that we can represent the joint probability distribution for the capture histories and states as a product of site-specific terms:\n\\[[y_{1:N}, z_{1:N}, p, \\psi] = \\prod_{i=1}^N [y_i \\mid p, z_i] \\times \\prod_{i=1}^N [z_i \\mid \\psi] \\times [p, \\psi].\\]\nWe can simplify this a little:\n\\[[y_{1:N}, z_{1:N}, p, \\psi] = \\prod_{i=1}^N [y_i \\mid p, z_i] [z_i \\mid \\psi] \\times [p, \\psi].\\]\nLast, we have independent priors for \\(p\\) and \\(\\psi\\), so we can write the joint distribution as:\n\\[[y_{1:N}, z_{1:N}, p, \\psi] = \\prod_{i=1}^N [y_i \\mid p, z_i] [z_i \\mid \\psi] \\times [p] [\\psi].\\]\n\nThis follows from the definition of independent random variables. If \\(A\\) and \\(B\\) are independent, \\([A, B] = [A] \\times [B]\\).\nIn case that is confusing, it can also be useful to visualize this same dependence structure graphically.\n\n\n\nFigure 1: A directed acyclic graph for an occupancy model. Arrows represent dependence (e.g., p -> y means y depends on p).\n\n\n\nThis is almost in a form that we can use in Stan.\nBut we need to get rid of \\(z\\) from the model.\nIt’s a discrete parameter, and Stan needs continuous parameters.\nMarginalizing over discrete parameters\nTo get rid of our discrete parameter \\(z\\), we need to marginalize it out of the model.\nIn general, if you have a joint distribution for \\(y\\) and \\(z\\) that depends on \\(\\theta\\), you obtain the marginal distribution of \\(y\\) by summing the joint distribution over all possible values of \\(z\\):\n\\[[y \\mid \\theta] = \\sum_{z} [y, z \\mid \\theta].\\]\n\nThis is sometimes referred to as “summing out the responsibility parameter”. See why?\nIn our case, for the \\(i^{th}\\) site, this means that we need to marginalize over \\(z_i\\) as follows:\n\\[[y_i \\mid p, \\psi] = \\sum_{z_i=0}^1 [y_i, z_i \\mid p, \\psi].\\]\nWe are summing over all possible values of \\(z_i\\). In this case there are two (\\(z_i\\) can be 0 or 1).\nWe can factor the joint distribution:\n\\[[y_i \\mid p, \\psi] = \\sum_{z_i=0}^1 [y_i \\mid p, z_i] [z_i \\mid \\psi].\\]\nThis is:\n\\[[y_i \\mid p, \\psi] = [y_i \\mid p, z_i=0] [z_i=0 \\mid \\psi] + [y_i \\mid p, z_i=1] [z_i=1 \\mid \\psi].\\]\nEarlier we said \\(\\psi\\) is the probability that \\(z_i = 1\\).\nSo, \\(1-\\psi\\) is the probability that \\(z_i=0\\):\n\\[[y_i \\mid p, \\psi] = (1 - \\psi) [y_i \\mid p, z_i=0] + \\psi [y_i \\mid p, z_i=1].\\]\n\nReplace \\([z_i=0 \\mid \\psi]\\) with \\(1-\\psi\\), and \\([z_i=1 \\mid \\psi]\\) with \\(\\psi\\).\nWe can also simplify the observation model for unoccupied sites.\nBecause we assume that there are no false positive detections, unoccupied sites (\\(z_i=0\\)) can only generate zero counts for \\(y_i\\) (or, you could say that \\(y_i\\) is identically zero if \\(z_i=0\\)).\nWe can then write this as:\n\\[[y_i \\mid p, \\psi] = (1 - \\psi) I(y_i=0) + \\psi [y_i \\mid p, z_i=1],\\]\nwhere \\(I(y_i=0)\\) is an indicator function that is equal to one if \\(y_i=0\\), and otherwise is equal to zero.\nIt might be more intuitive to write this as:\n\\[[y_i \\mid p, \\psi] = \\begin{cases}\n        \\psi [y_i \\mid p, z_i=1], & \\text{for } y_i > 0\\\\\n        \\psi [y_i \\mid p, z_i=1] + 1 - \\psi, & \\text{for } y_i = 0\n        \\end{cases}\\]\nWe can make this even more explicit by bringing back in the fact that our probability model for \\(y_i\\) is Binomial:\n\\[[y_i \\mid p, \\psi] = \\begin{cases}\n        \\psi \\text{Binomial}(y_i \\mid p), & \\text{for } y_i > 0\\\\\n        \\psi \\text{Binomial}(y_i \\mid p) + 1 - \\psi, & \\text{for } y_i = 0\n        \\end{cases}\\]\nGreat - we just marginalized \\(z_i\\) out of the model.\nLet’s circle back to the joint distribution and see what it looks like now.\nPreviously we had:\n\\[[y_{1:N}, z_{1:N}, p, \\psi] = \\prod_{i=1}^N [y_i \\mid p, z_i] [z_i \\mid \\psi] \\times [p] [\\psi].\\]\nNow, if we marginalize over \\(z\\) for every site, we’d be computing:\n\\[[y_{1:N}, p, \\psi] = \\prod_{i=1}^N \\Big( \\sum_{z_i=0}^1 [y_i \\mid p, z_i] [z_i \\mid \\psi] \\Big) \\times [p] [\\psi],\\]\n\\[ = \\prod_{i=1}^N [y_i \\mid p, \\psi] \\times [p] [\\psi].\\]\nThis joint distribution is something we can work with in Stan.\nThe last thing we need to do is write down the log of the joint distribution, and translate that into Stan’s syntax.\nThe log of the joint distribution\nWe are going to specify the joint distribution in Stan on the log scale.\nTake the log of our joint distribution:\n\\[\\log([y_{1:N}, p, \\psi]) = \\log \\Bigg(\\prod_{i=1}^N [y_i \\mid p, \\psi] \\times [p] [\\psi] \\Bigg),\\]\nand by “\\(\\log\\)” I mean the natural log.\nRecall that the log of a product is the sum of logs: \\(\\log(ab) = \\log(a) + \\log(b)\\)).\nWe can apply this rule and find that:\n\\[\\log([y_{1:N}, p, \\psi]) = \\sum_{i=1}^N \\log [y_i \\mid p, \\psi] + \\log[p] + \\log[\\psi].\\]\nLet’s think about how to represent \\(\\log [y_i \\mid p, \\psi]\\).\nRecall from before that:\n\\[[y_i \\mid p, \\psi] = \\begin{cases}\n        \\psi \\text{Binomial}(y_i \\mid p), & \\text{for } y_i > 0\\\\\n        \\psi \\text{Binomial}(y_i \\mid p) + 1 - \\psi, & \\text{for } y_i = 0\n        \\end{cases}\\]\nTaking logarithms, we get:\n\\[\\log [y_i \\mid p, \\psi] = \\begin{cases}\n        \\log \\big( \\psi \\text{Binomial}(y_i \\mid p) \\big), & \\text{for } y_i > 0\\\\\n        \\log \\big( \\psi \\text{Binomial}(y_i \\mid p) + 1 - \\psi \\big), & \\text{for } y_i = 0\n        \\end{cases}\\]\nRecalling rules about logs of products, we can rewrite this as:\n\\[\\log [y_i \\mid p, \\psi] = \\begin{cases}\n        \\log \\psi + \\log(\\text{Binomial}(y_i \\mid p)), & \\text{for } y_i > 0\\\\\n        \\log \\big( \\psi \\text{Binomial}(y_i \\mid p) + 1 - \\psi \\big), & \\text{for } y_i = 0\n        \\end{cases}\\]\nStan has a function called binomial_lpmf (“binomial log probability mass function”) that gives us exactly what we need to compute \\(\\log(\\text{Binomial}(y_i \\mid p))\\) above.\nTo make this connection clear, let’s rewrite this as:\n\\[\\log [y_i \\mid p, \\psi] = \\begin{cases}\n        \\log \\psi + \\text{binomial_lpmf}(y_i \\mid p), & \\text{for } y_i > 0\\\\\n        \\log \\big( \\psi \\text{Binomial}(y_i \\mid p) + 1 - \\psi \\big), & \\text{for } y_i = 0\n        \\end{cases}\\]\nThen, let’s re-write the case where \\(y_i=0\\) as:\n\\[\\log [y_i \\mid p, \\psi] = \\begin{cases}\n        \\log \\psi + \\text{binomial_lpmf}(y_i \\mid p), & \\text{for } y_i > 0\\\\\n        \\log \\big( e^{\\log(\\psi \\text{Binomial}(y_i \\mid p))} + e^{\\log(1 - \\psi)} \\big), & \\text{for } y_i = 0\n        \\end{cases}\\]\n\nIt may seem silly to do this, but trust me it will be useful.\nThis is true because \\(e^{\\log(x)}=x\\).\nThen, apply the rule about \\(\\log(ab) = \\log a + \\log b\\) again:\n\\[\\log [y_i \\mid p, \\psi] = \\begin{cases}\n        \\log \\psi + \\text{binomial_lpmf}(y_i \\mid p), & \\text{for } y_i > 0\\\\\n        \\log \\big( e^{\\log \\psi + \\text{binomial_lpmf}(y_i \\mid p)} + e^{\\log(1 - \\psi)} \\big), & \\text{for } y_i = 0\n        \\end{cases}\n\\]\nAt this point, we are going to bring in the LogSumExp trick, which gives us a computationally stable way to compute terms like \\(\\log(\\sum_i \\exp(x_i))\\).\nStan has a function called log_sum_exp that does this for us, and it takes the terms to sum on the log scale as inputs.\nLet’s rewrite the model for the data with this function:\n\\[\\log [y_i \\mid p, \\psi] = \\begin{cases}\n        \\log \\psi + \\text{binomial_lpmf}(y_i \\mid p), &  y_i > 0\\\\\n        \\text{log_sum_exp}(\\log \\psi + \\text{binomial_lpmf}(y_i \\mid p),\\; \\log(1 - \\psi)), &  y_i = 0\n        \\end{cases}\n\\]\nTranslating our model to Stan\nHere’s the Stan model (written for clarity, not computational efficiency):\ndata {\n  int<lower = 1> N;\n  int<lower = 1> K;\n  int<lower = 0, upper = K> y[N];\n}\n\nparameters {\n  real<lower = 0, upper = 1> p;\n  real<lower = 0, upper = 1> psi;\n}\n\ntransformed parameters {\n  vector[N] log_lik;\n  \n  for (i in 1:N) {\n    if (y[i] > 0) {\n      log_lik[i] = log(psi) + binomial_lpmf(y[i] | K, p);\n    } else {\n      log_lik[i] = log_sum_exp(\n        log(psi) + binomial_lpmf(y[i] | K, p), \n        log1m(psi)\n      );\n    }\n  }\n}\n\nmodel {\n  target += sum(log_lik);\n  target += uniform_lpdf(p | 0, 1);\n  target += uniform_lpdf(psi | 0, 1);\n}\nThe observation model\nWe symbolically represented the observation model as:\n\\[\\log [y_i \\mid p, \\psi] = \\begin{cases}\n        \\log \\psi + \\text{binomial_lpmf}(y_i \\mid p), &  y_i > 0\\\\\n        \\text{log_sum_exp}(\\log \\psi + \\text{binomial_lpmf}(y_i \\mid p),\\; \\log(1 - \\psi)), &  y_i = 0\n        \\end{cases}\\]\nIn Stan syntax, we are storing \\(\\log [y_i \\mid p, \\psi]\\) in the \\(i^{th}\\) element of the vector log_lik.\nWe used an if-statement to deal with the different cases:\n...\n    if (y[i] > 0) {\n      log_lik[i] = log(psi) + binomial_lpmf(y[i] | K, p);\n    } else {\n      log_lik[i] = log_sum_exp(\n        log(psi) + binomial_lpmf(y[i] | K, p), \n        log1m(psi)\n      );\n    }\n...\nThe log joint distribution and target +=\nNotice how in the model block, we used target += syntax to add things to the log joint distribution:\n...\nmodel {\n  target += sum(log_lik);\n  target += uniform_lpdf(p | 0, 1);\n  target += uniform_lpdf(psi | 0, 1);\n}\n...\nThese terms correspond to the log joint distribution that we represented symbolically as\n\\[\\log([y_{1:N}, p, \\psi]) = \\sum_{i=1}^N \\log [y_i \\mid p, \\psi] + \\log[p] + \\log[\\psi].\\]\nBringing it all together\nTo recap, to go from our model specification with discrete parameters to a model that we can use in Stan, we did the following:\nWrote the joint distribution of our model\nMarginalized discrete parameter(s) out of the joint distribution\nTook the log of the joint distribution\nTranslated the log of the joint distribution into Stan code\nThis general approach applies to a variety of models, but occupancy models provide a simple example.\nMore resources\nThe Stan documentation has some great content on marginalization of discrete parameters: https://mc-stan.org/docs/2_18/stan-users-guide/latent-discrete-parameterization.html\nBob Carpenter put together a great case study for multi-species occupancy models in Stan, which provides a step up in complexity from this single-species model: https://mc-stan.org/users/documentation/case-studies/dorazio-royle-occupancy.html\nThere are a ton of ecological models with discrete parameters that Hiroki Itô has translated to Stan from the book “Bayesian Population Analysis using WinBUGS — A Hierarchical Perspective” (2012) by Marc Kéry and Michael Schaub (Kéry and Schaub 2011): https://github.com/stan-dev/example-models/tree/master/BPA\nMost of the example spatial capture-recapture models from the 2013 book Spatial Capture-Recapture by Royle, Chandler, Gardner, and Sollmann (Royle et al. 2013) have also been translated to Stan: https://github.com/mbjoseph/scr-stan\nThis post demonstrated how to marginalize symbolically (or “on paper” I guess), but there is another great blog post by Jacob Socolar that focuses on JAGS to Stan code translation (and includes a marginal model implementation in JAGS): https://github.com/jsocolar/occupancyModels\n\n\n\nKéry, Marc, and Michael Schaub. 2011. Bayesian Population Analysis Using WinBUGS: A Hierarchical Perspective. Academic Press.\n\n\nMacKenzie, Darryl I, James D Nichols, Gideon B Lachman, Sam Droege, J Andrew Royle, and Catherine A Langtimm. 2002. “Estimating Site Occupancy Rates When Detection Probabilities Are Less Than One.” Ecology 83 (8): 2248–55.\n\n\nRoyle, J Andrew, Richard B Chandler, Rahel Sollmann, and Beth Gardner. 2013. Spatial Capture-Recapture. Academic Press.\n\n\n\n\n",
    "preview": "posts/2020-04-28-a-step-by-step-guide-to-marginalizing-over-discrete-parameters-for-ecologists-using-stan/dag.png",
    "last_modified": "2022-11-01T21:19:18-06:00",
    "input_file": "a-step-by-step-guide-to-marginalizing-over-discrete-parameters-for-ecologists-using-stan.knit.md"
  },
  {
    "path": "posts/2020-01-13-neural-hierarchical-models/",
    "title": "Behind the paper: Neural hierarchical models of ecological populations",
    "description": "A high-level overview, an example, and a call to action.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2020-01-23",
    "categories": [
      "papers"
    ],
    "contents": "\nThis post gives some background and a demo for the paper “Neural hierarchical models of ecological populations” out today in Ecology Letters (Joseph 2020).\nDeep learning and model-based ecological inference may seem like totally separate pursuits. Yet, if you think about deep learning as a set of tools to approximate functions, it’s not much of a leap to begin seeing opportunities to unite deep learning with some standard ecological modeling approaches.\nHierarchical models\nHierarchical models have been around for a while, and are now one of the workhorse methods of modern quantitative ecology (e.g., occupancy models, capture-recapture models, N-mixture models, animal movement models, state-space models, etc.). Hierarchical models combine:\nA data model \\([y \\mid z, \\theta]\\) where we observe \\(y\\), that depends on a process \\(z\\), and parameter(s) \\(\\theta\\),\nA process model \\([z \\mid \\theta]\\), and\nA parameter model \\([\\theta]\\).\nA posterior distribution of the unknowns, conditioned on the data is:\n\\[[z, \\theta \\mid y] = \\dfrac{[y \\mid z, \\theta] \\times [z \\mid \\theta] \\times [\\theta]}{[y]}.\\]\nWe might also have some explanatory variables \\(x\\) that might tell us something about \\(z\\), \\(\\theta\\), and/or \\(y\\).\nNeural networks\nNeural networks approximate functions. Off the shelf neural networks usually just map \\(x\\) to \\(y\\), and allow us to predict new values of \\(y\\) for new values of \\(x\\). Sometimes, predicting \\(y\\) is not really what we care about - we really want to learn something about a process \\(z\\) or some parameters \\(\\theta\\).\nNeural hierarchical models\nWe can parameterize a hierarchical model with a neural network to learn about \\(z\\). So, for example, if \\(\\theta\\) represents the parameters of a neural network, then we can construct a process model \\([z \\mid \\theta]\\) where our input \\(x\\) is mapped to a process \\(z\\) by way of some neural network:\n\\[[z \\mid \\theta] = f(x, \\theta),\\]\nwhere \\(f\\) is a neural network that maps \\(x\\) and \\(\\theta\\) to some probabilistic model for \\(z\\) (here because \\(x\\) is observed, I’m not going to condition \\(z\\) on it on the left hand side of the equation - \\(x\\) is assumed to be constant and known without error).\nGraphically, you might consider a state-space model where some inputs \\(x\\) are mapped to a state transition matrix (for an example with an animal movement model, see Appendix S2 in the paper):\n\n\n\nFigure 1: Using a convolutional neural network to map some input raster to a state transition matrix in a hidden Markov model\n\n\n\nAn example: a neural N-mixture model\nAn N-mixture model can be used to estimate latent integer-valued abundance when unmarked populations are repeatedly surveyed and it is assumed that detection of individuals is imperfect (Royle 2004). Assume that \\(J\\) spatial locations are each surveyed \\(K\\) times, in a short time interval for which it is reasonable to assume that the number of individuals is constant within locations \\(j=1, ..., J\\). Each spatial location has some continuous covariate value represented by \\(x_j\\), that relates to detection probabilities and expected abundance.\nObservation model\nObservations at site \\(j\\) in survey \\(k\\) yield counts of the number of unique individuals detected, denoted \\(y_{j, k}\\) for all \\(j\\) and all \\(k\\). Assuming that the detection of each individual is conditionally independent, and that each individual is detected with site-specific probability \\(p_j\\), the observations can be modeled with a Binomial distribution where the number of trials is the true (latent) population abundance \\(n_j\\):\n\\[y_{j, k} \\sim \\text{Binomial}(p_j, n_j).\\]\nProcess model\nThe true population abundance \\(n_j\\) is treated as a Poisson random variable with expected value \\(\\lambda_j\\):\n\\[n_j \\sim \\text{Poisson}(\\lambda_j).\\]\nParameter model\nHeterogeneity among sites was accounted for using a single layer neural network that ingests the one dimensional covariate \\(x_i\\) for site \\(i\\), passes it through a single hidden layer, and outputs a two dimensional vector containing a detection probability \\(p_i\\) and the expected abundance \\(\\lambda_i\\):\n\\[\n\\begin{bmatrix}\n   \\lambda_i \\\\\n   p_i\n \\end{bmatrix} = f(x_i),\n\\]\nwhere \\(f\\) is a neural network with two dimensional output activations \\(\\vec{h}(x_i)\\) computed via:\n\\[\\vec{h}(x_i) = \\vec{W}^{(2)} g(\\vec{W}^{(1)} x_i ),\\] and final outputs computed using the log and logit link functions for expected abundance and detection probability:\n\\[f(x_i) = \\begin{bmatrix}\n   \\text{exp}(h_1(x_i)) \\\\\n   \\text{logit}^{-1}(h_2(x_i))\n \\end{bmatrix}.\\]\nHere too \\(\\vec{W}^{(1)}\\) is a parameter matrix that generates activations from the inputs, \\(g\\) is an activation function, and \\(\\vec{W}^{(2)}\\) is a parameter matrix that maps the hidden layer to the outputs. Additionally \\(h_1(x_i)\\) is the first element of the output activation vector, and \\(h_2(x_i)\\) the second element.\nLoss function\nThe negative log likelihood was used as the loss function, enumerating over a large range of potential values of the true abundance (from \\(\\min(y_j.)\\) to \\(5 \\times \\max(y_j.)\\), where \\(y_{j.}\\) is a vector of counts of length \\(K\\)) to approximate the underlying infinite mixture model implied by the Poisson model of abundance (Royle 2004).\n\n\n\nSimulating some data\nFirst, load some python dependencies.\n\n\nimport matplotlib.pyplot as plt\nimport multiprocessing\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.distributions import Binomial, Poisson\nfrom torch.utils.data import DataLoader, TensorDataset\n\nSimulate data at nsite sites, with nrep repeat surveys. Here it’s assumed that there is one continuous site-level covariate \\(x\\) that has some nonlinear relationship with the expected number of individuals at a site.\n\n\nnp.random.seed(123456)\nnsite = 200\nnrep = 5\nx = np.linspace(-2.5, 2.5, nsite, dtype=np.float32).reshape(-1,1)\n\n# Draw f(x) from a Gaussian process\ndef kernel(x, theta):\n    m, n = np.meshgrid(x, x)\n    sqdist = abs(m-n)**2\n    return np.exp(- theta * sqdist)\n\nK = kernel(x, theta=.2)\nL = np.linalg.cholesky(K + 1e-5* np.eye(nsite))\nf_prior = np.dot(L, np.random.normal(size=(nsite, 1)))\n\nGenerate some abundance values from a Poisson distribution:\n\n\noffset = 3\nlam = np.exp(f_prior + offset)\nn = np.random.poisson(lam)\n\nplt.scatter(x, n, c='k', alpha=.3)\nplt.plot(x, lam)\nplt.xlabel('Covariate value')\nplt.ylabel('True (latent) abundance')\n\n\nFigure 2: True relationship between latent abundance and the covariate, with sampled points.\n\n\n\nFor simplicity, assume that the probability of detection is constant across all sites and independent of \\(x\\).\n\n\npr_detection = np.array([0.5])\ny = np.random.binomial(n=n, \n                       p=pr_detection, \n                       size=(nsite, nrep)).astype(np.float32)\n\nplt.plot(x, lam)\nfor i in range(nrep):\n    plt.scatter(x, y[:, i], c='b', s=4, alpha=.3)    \nplt.xlabel('Covariate value')\nplt.ylabel('Observed counts')\n\n\n(#fig:gen_data)Observed counts as a function of the covariate value.\n\n\n\nDefining a neural network\nWe will define a torch.nn.Module class for our neural network. This ingests \\(x\\) and outputs a value for \\(\\lambda\\) and \\(p\\):\n\n\nclass Net(nn.Module):\n    \"\"\" Neural N-mixture model \n    \n    This is a neural network that ingests x and outputs:\n    - lam(bda): expected abundance\n    - p: detection probability\n    \"\"\"\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.fc1 = nn.Linear(1, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, 2)\n\n    def forward(self, x):\n        hidden_layer = torch.sigmoid(self.fc1(x))\n        output = self.fc2(hidden_layer)\n        lam = torch.exp(output[:, [0]])\n        p = torch.sigmoid(output[:, [1]])\n        return lam, p\n\nDefining a loss function\nWe will use the negative log likelihood as our loss function:\n\n\ndef nmix_loss(y_obs, lambda_hat, p_hat, n_max):\n    \"\"\" N-mixture loss.\n    \n    Args:\n      y_obs (tensor): nsite by nrep count observation matrix\n      lambda_hat (tensor): poisson abundance expected value\n      p_hat (tensor): individual detection probability\n      n_max (int): maximum abundance to consider\n    \n    Returns:\n      negative log-likelihood (tensor)\n    \"\"\"\n    batch_size, n_rep = y_obs.shape\n    \n    possible_n_vals = torch.arange(n_max).unsqueeze(0).float()\n    n_logprob = Poisson(lambda_hat).log_prob(possible_n_vals)\n    assert n_logprob.shape == (batch_size, n_max)\n    \n    y_dist = Binomial(possible_n_vals.view(1, 1, -1), probs=p_hat.view(-1, 1, 1))\n    y_obs = y_obs.unsqueeze(-1).repeat(1, 1, n_max)\n    y_logprob = y_dist.log_prob(y_obs).sum(dim=1) # sum over repeat surveys\n    assert y_logprob.shape == (batch_size, n_max)\n    \n    log_lik = torch.logsumexp(n_logprob + y_logprob, -1)\n    return -log_lik\n\nPreparing to train\nInstantiate a model.\n\n\nnet = Net(hidden_size=32)\nnet\n\nNet(\n  (fc1): Linear(in_features=1, out_features=32, bias=True)\n  (fc2): Linear(in_features=32, out_features=2, bias=True)\n)\n\nCreate a data loader.\n\n\ndataset = TensorDataset(torch.tensor(x).float(), torch.tensor(y).float())\ndataloader = DataLoader(dataset, \n                        batch_size=16,\n                        shuffle=True, \n                        num_workers=multiprocessing.cpu_count())\n\nInstantiate an optimizer and choose the number of training epochs:\n\n\nn_epoch = 1000\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01, weight_decay=1e-6)\nrunning_loss = []\n\nTraining the model\nFinally, train the model, visualizing the estimated relationship between \\(x\\) and \\(N\\) after every gradient update.\n\n\n_ = plt.scatter(x, n, c='k')\n_ = plt.xlabel('Covariate value')\n_ = plt.ylabel('Abundance')\ncolors = plt.cm.viridis(np.linspace(0,1,n_epoch))\nfor i in range(n_epoch):\n    for i_batch, xy in enumerate(dataloader):\n        x_i, y_i = xy\n        optimizer.zero_grad()\n        lambda_i, p_i = net(x_i)\n        nll = nmix_loss(y_i, lambda_i, p_i, n_max = 200)\n        loss = torch.mean(nll)        \n        loss.backward()\n        optimizer.step()    # Does the update\n        running_loss.append(loss.data.detach().numpy())\n    lam_hat, p_hat = net(torch.from_numpy(x))\n    lam_hat = lam_hat.detach().numpy()\n    _ = plt.plot(x, lam_hat, color=colors[i], alpha=.1)\nplt.show()\n\n\nFigure 3: Estimated relationships between x and abundance as training progresses. Dark blue lines represent predictions from early training iterations, and green/yellow represent middle/late training iterations.\n\n\n\nVisualize loss:\n\n\nn_step = len(running_loss)\n_ = plt.scatter(x=np.arange(n_step), y=running_loss, s=2,\n                color=plt.cm.viridis(np.linspace(0,1,n_step)))\n_ = plt.xlabel('Number of training iterations')\n_ = plt.ylabel('N-mixture loss')\nplt.show()\n\n\nFigure 4: Training loss over time. Each point corresponds to the loss after a gradient update.\n\n\n\nMore on implementing hierarchical models\nIf you are interested in digging into the details of how to build these models, check out the companion repository on GitHub, which has all of the code required to reproduce the paper, as well as links to Jupyter Notebooks (thanks Binder!) to play with some toy occupancy, capture-recapture, and N-mixture models: https://github.com/mbjoseph/neuralecology\nParting thoughts\nDeep learning is somewhat of a mystery to many ecologists. Those who are currently applying it have done some amazing things (like counting plants and animals in imagery). But, I worry that as a community, ecologists are thinking about deep learning too narrowly.\nWe can look to hydrology and physics to get a sense for how we can advance science with deep learning. Here are a few key papers that are shaping my thinking around this topic, which might help motivate future work in science-based deep learning for ecology:\nKarpatne, Anuj, et al. “Physics-guided neural networks (pgnn): An application in lake temperature modeling.” arXiv preprint arXiv:1710.11431 (2017). link\nRaissi, Maziar. “Deep hidden physics models: Deep learning of nonlinear partial differential equations.” The Journal of Machine Learning Research 19.1 (2018): 932-955. link\nRangapuram, Syama Sundar, et al. “Deep state space models for time series forecasting.” Advances in neural information processing systems. 2018. link\nRackauckas, Christopher, et al. “Universal Differential Equations for Scientific Machine Learning.” arXiv preprint arXiv:2001.04385 (2020). link\n\n\nJoseph, Maxwell B. 2020. “Neural Hierarchical Models of Ecological Populations.” Ecology Letters in press. https://doi.org/10.1111/ele.13462.\n\n\nRoyle, J Andrew. 2004. “N-Mixture Models for Estimating Population Size from Spatially Replicated Counts.” Biometrics 60 (1): 108–15.\n\n\n\n\n",
    "preview": "posts/2020-01-13-neural-hierarchical-models/neural-hierarchical-models_files/figure-html5/train-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-30-yes-but-does-it-still-run/",
    "title": "Yes, but does it (still) run?",
    "description": "Migrating from Jekyll to distill, with some reflections on the past 6 years.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2018-12-30",
    "categories": [],
    "contents": "\nI haven’t blogged recently. It was useful as a PhD student to wrap my head around new methods and track my path from a code-naive field ecologist to a not entirely incompetent R programmer in 2016 when I graduated. But, much has changed in the 5-6 years since I began the blog. I’ve developed new skills, and the tools around R programming have matured. It’s easier now to publish content with R markdown with tools like blogdown and distill than it was six years ago.\nMy old workflow involved writing posts in R markdown, then generating markdown and publishing with a static site generator (first Octopress, then Jekyll). This is fine, but I ended up losing track of most of the original R markdown files that generated the markdown being served on the site.\nLooking back on 2018, I’ve come to better appreciate continuous integration through Travis CI, CircleCI, and AppVeyor. It’s nice to know when builds break and code stops working. As I was taking stock of the past year, I realized that I did not know whether the code that I had posted years ago still worked, and I didn’t have the .Rmd files anymore to find out.\n\n\n\nFigure 1: An abandoned car that may have been useful in its time, but no longer runs.\n\n\n\nThis bothered me. First, I didn’t want to be responsible for publishing code that doesn’t run. Second, I also realized that years ago, I did not appreciate portability as much as I do today, e.g., when I felt comfortable assuming that users would need to manually download data from a website and store it in a specific directory on their filesystem to run some code. With time off over the holidays, I figured I would migrate my old site from Jekyll to distill and put the resulting site under continuous integration to be sure that the answer to the question “does it (still) run?” is yes.\nThe result is this site (as of Dec 2018), and unsurprisingly, not all of the code that I had posted previously still ran. But, now with distill I have Travis CI building the posts regularly, so at least if something breaks in the future, I should know sooner.\n\n\n",
    "preview": "posts/2018-12-30-yes-but-does-it-still-run/yes-but-does-it-still-run_files/figure-html5/plot-img-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-gaussian-predictive-process-models-in-stan/",
    "title": "Gaussian predictive process models in Stan",
    "description": "Gaussian processes that scale to larger data.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2016-08-14",
    "categories": [
      "stan",
      "spatial"
    ],
    "contents": "\nGaussian process (GP) models are computationally demanding for large datasets. Much work has been done to avoid expensive matrix operations that arise in parameter estimation with larger datasets via sparse and/or reduced rank covariance matrices (Datta et al. 2016 provide a nice review). What follows is an implementation of a spatial Gaussian predictive process Poisson GLM in Stan, following Finley et al. 2009: Improving the performance of predictive process modeling for large datasets, with a comparison to a full rank GP model in terms of execution time and MCMC efficiency.\nGenerative model\nAssume we have \\(n\\) spatially referenced counts \\(y(s)\\) made at spatial locations \\(s_1, s_2, ..., s_n\\), which depend on a latent mean zero Gaussian process with an isotropic stationary exponential covariance function:\n\\[y(s) \\sim \\text{Poisson}(\\text{exp}(X^T(s) \\beta + w(s)))\\]\n\\[w(s) \\sim GP(0, C(d))\\]\n\\[[C(d)]_{i, j} = \\eta^2 \\text{exp}(- d_{ij} \\phi)) + I(i = j) \\sigma^2\\]\nwhere \\(y(s)\\) is the response at location \\(s\\), \\(X\\) is an \\(n \\times p\\) design matrix, \\(\\beta\\) is a length \\(p\\) parameter vector, \\(\\sigma^2\\) is a “nugget” parameter, \\(\\eta^2\\) is the variance parameter of the Gaussian process, \\(d_{ij}\\) is a spatial distance between locations \\(s_i\\) and \\(s_j\\), and \\(\\phi\\) determines how quickly the correlation in \\(w\\) decays as distance increases. For simplicity, the point locations are uniformly distributed in a 2d unit square spatial region.\nTo estimate this model in a Bayesian context, we might be faced with taking a Cholesky decomposition of the \\(n \\times n\\) matrix \\(C(d)\\) at every iteration in an MCMC algorithm, a costly operation for large \\(n\\).\nGaussian predictive process representation\nComputational benefits of Gaussian predictive process models arise from the estimation of the latent Gaussian process at \\(m << n\\) locations (knots). Instead of taking the Cholesky factorization of the \\(n \\times n\\) covariance matrix, we instead factorize the \\(m \\times m\\) covariance matrix corresponding to the covariance in the latent spatial process among knots. Knot placement is a non-trivial topic, but for the purpose of illustration let’s place knots on a grid over the spatial region of interest. Note that here the knots are fixed, but it is possible to model knot locations stochastically as in Guhaniyogi et al. 2012.\n\n\n\nKnots are shown as stars and the points are observations.\nWe will replace \\(w\\) above with an estimate of \\(w\\) that is derived from a reduced rank representation of the latent spatial process. Below, the vector \\(\\tilde{\\boldsymbol{\\epsilon}}\\) corrects for bias (underestimation of \\(\\eta\\) and overestimation of \\(\\sigma\\)) as an extension of Banerjee et al. 2008, and \\(\\mathcal{C}^T(\\theta) \\mathcal{C}^{*-1}(\\theta)\\) relates \\(w\\) at desired point locations to the value of the latent GP at the knot locations, where \\(\\mathcal{C}^T(\\theta)\\) is an \\(n \\times m\\) matrix that gets multiplied by the inverse of the \\(m \\times m\\) covariance matrix for the latent spatial process at the knots. For a complete and general description see Finley et al. 2009, but here is the jist of the univariate model:\n\\[\\boldsymbol{Y} \\sim \\text{Poisson}(\\boldsymbol{X} \\beta + \\mathcal{C}^T(\\theta) \\mathcal{C}^{*-1}(\\theta)\\boldsymbol{w^*} + \\tilde{\\boldsymbol{\\epsilon}})\\]\n\\[\\boldsymbol{w^*} \\sim GP(0, \\mathcal{C}^*(\\theta))\\]\n\\[\\tilde{\\epsilon}(s) \\stackrel{indep}{\\sim} N(0, C(s, s) - \\mathcal{c}^T(\\theta) \\mathcal{C}^{*-1}(\\theta))\\mathcal{c}(\\theta)\\]\nwith priors for \\(\\eta\\), \\(\\sigma\\), and \\(\\phi\\) completing a Bayesian specification.\nThis approach scales well for larger datasets relative to a full rank GP model. Comparing the number of effective samples per unit time for the two approaches across a range of sample sizes, the GPP executes more quickly and is more efficient for larger datasets. Code for this simulation is available on GitHub: https://github.com/mbjoseph/gpp-speed-test.\n\n\n\nRelevant papers\nFinley, Andrew O., et al. “Improving the performance of predictive process modeling for large datasets.” Computational statistics & data analysis 53.8 (2009): 2873-2884.\nBanerjee, Sudipto, et al. “Gaussian predictive process models for large spatial data sets.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 70.4 (2008): 825-848.\nDatta, Abhirup, et al. “Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets.” Journal of the American Statistical Association. Accepted (2015).\nGuhaniyogi, Rajarshi, et al. “Adaptive Gaussian predictive process models for large spatial datasets.” Environmetrics 22.8 (2011): 997-1007.\n\n\n",
    "preview": "posts/2018-12-27-gaussian-predictive-process-models-in-stan/gaussian-predictive-process-models-in-stan_files/figure-html5/plot-img2-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-the-five-elements-ninjas-approach-to-teaching-design-matrices/",
    "title": "The five elements ninjas approach to teaching design matrices",
    "description": "In-class activities to teach design matrices from multiple perspectives.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2016-04-25",
    "categories": [
      "teaching"
    ],
    "contents": "\nDesign matrices unite seemingly disparate statistical methods, including linear regression, ANOVA, multiple regression, ANCOVA, and generalized linear modeling. As part of a hierarchical Bayesian modeling course that we offered this semester, we wanted our students to learn about design matrices to facilitate model specification and parameter interpretation. Naively, I thought that I could spend a few minutes in class reviewing matrix multiplication and a design matrix for simple linear regression, and if students wanted more, they might end up on Wikipedia’s Design matrix page.\nIt quickly became clear that this approach was not effective, so I started to think about how students could construct their own understanding of design matrices. About the same time, I watched a pretty incredible kung fu movie called Five Element Ninjas, and it occurred to me that the “five elements” concept could be an effective device for getting my students to think about model specification and design matrices.\nLearning goals\nStudents should be able to specify design matrices for many different types of models (e.g., linear models and generalized linear models), and they should be able to interpret the parameters.\nApproach\nThe broad idea was to get the students to think about model specification from five perspectives:\nModel specification via a design matrix\nModel specification via R syntax (e.g., the formula argument to lm)\nModel specification via “long form” equations\nGraphical model specification\nVerbal model specification (along with an interpretation of each of the parameter estimates)\nThis leverages what students already know, and encourages them to connect new concepts to their existing knowledge. In our case, the students were all students in CU Boulder’s Ecology and Evolutionary Biology graduate program. Most of them had a strong grasp of perspective 2 (model specification in R syntax), but relatively weak understanding of the remaining perspectives.\nGetting the students started\nBefore we asked them to do anything, I demonstrated this five elements approach on a simple model: the model of the mean.\n1. Design matrix specification\n\\[y \\sim N(\\mu, \\sigma_y)\\]\n\\[\\mu = X \\beta\\]\n\\[X = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\\\ 1 \\end{bmatrix}\\]\n2. R syntax\nThe formula for a model of the mean is y ~ 1\n3. Long form equations\n\\[y_1 = \\beta + \\epsilon_1\\]\n\\[y_2 = \\beta + \\epsilon_2\\]\n\\[ \\vdots \\]\n\\[y_n = \\beta + \\epsilon_n\\]\n4. Graphical interpretation\n\n5. Verbal description\nI asked for a student to take a stab at a verbal description of the model specification, and also to explain the interpretation of the parameter \\(\\beta\\). If they’re having a hard time understanding the task, you can tell them to pretend that they are talking to a classmate on the phone and trying to describe the model.\nThe activity\nWe provided students with a very simple data set that does not include the “response” variable. This was printed ahead of time, so that each student had a paper copy that they could also use as scratch paper.\nCovariate 1\nCovariate 2\n1.0\nA\n2.0\nB\n3.0\nA\n4.0\nB\nThe omission of the response variable is deliberate, reinforcing the idea that one can construct a design matrix without knowing the outcome variable (this is useful later in our class for prior and posterior predictive simulations).\nWe organized the students into groups of three or four and had each group come up to the blackboard, which we partitioned ahead of time to have a space for each group to work. Then, we proceeded to work through incrementally more complex models with our five-pronged approach:\nA model that includes an effect of covariate 1.\nA model that includes an effect of covariate 2.\nA model that includes additive effects of covariate 1 and 2 (no interactions).\nA model that includes additive effects and an interaction between covariate 1 and 2.\nEach of these exercises took about 15 minutes, and once all the groups were done we checked in with each group as a class to see what they came up with. Some groups opted for effects parameterizations, while others opted for means parameterizations, which lead to a useful discussion of the default treatment of intercepts in R model formulas and the manual suppression of intercepts (e.g., y ~ 0 + x).\nThe outcome\nThis in-class activity was surprisingly well-received, and it seemed to provide the context and practice necessary for the students to understand design matrices on a deeper level. Throughout the rest of the semester, model matrices were preferred over other specifications by many of the students - a far cry from the confusion at the beginning of the semester.\n\n\n\nFigure 1: Katsushika Hokusai [Public domain]. Use different perspectives to improve concept delivery.\n\n\n\n\n\n",
    "preview": "posts/2018-12-27-the-five-elements-ninjas-approach-to-teaching-design-matrices/the-five-elements-ninjas-approach-to-teaching-design-matrices_files/figure-html5/plot-img-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-first-year-books/",
    "title": "First year books",
    "description": "10 books I wish I had entering graduate school.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2015-09-08",
    "categories": [
      "rants"
    ],
    "contents": "\nI had to read a lot of books in graduate school. Some were life-changing, and others were forgettable.\nIf I could bring a reading list back in time for my ‘first year’ graduate self, it would include the following:\nBayesian Data Analysis\nThird Edition, by Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin\nProbably the most useful book I’ve ever owned. Has staying power - can be used as a reference.\nThe Art of R Programming\nby Norman Matloff\nThis book made me less bad at programming in R early on.\nCausality\nSecond Edition, by Judea Pearl\nEcology is complicated. We often lack replicated controlled experiments with random treatment assignment. This book helped me organize my thinking around how to translate mechanistic knowledge to statistical models.\nStatistics for Spatio-Temporal Data\nby Noel Cressie and Christopher Wikle\nA thoughtful treatment of hierarchical modeling in a spatial, temporal, and spatiotemporal context. Has breadth with a healthy dose of outside references for depth. My dog ate this one, but it was great to have.\nEcological Models and Data in R\nby Benjamin M. Bolker\nCovers fundamental ideas about likelihood and process-oriented modeling while building R proficiency.\nBayesian Models: A Statistical Primer for Ecologists\nby N. Thompson Hobbs & Mevin B. Hooten\nAn introduction to the process of model building and estimation for non-math/stats oriented readers. Thoughtful treatment of notation, helped me to better understand how to communicate models.\nData Analysis Using Regression and Multilevel/Hierarchical Models\nby Andrew Gelman and Jennifer Hill\nA gentle introduction to multilevel modeling, with plenty of graphics and integration with R.\nStatistical Inference\nSecond Edition, by George Casella and Roger L. Berger\nEssential for understanding the mathematical and probabilistic foundations of statistics. Read it after brushing up on calculus. Checked this out from the library, and my dog ate it. Had to buy a copy to replace, which was not cheap. But, later found a pdf online.\nLinear Algebra\nby George Shilov\nI wish I had taken a class in linear algebra as an undergraduate, but I instead had to catch up in my first year of grad school. This book made it relatively painless. Literally found it on the shelf of the office I moved into on day one.\nSingle and Multivariable Calculus\nby David Guichard and friends\nBecause I took a few calculus classes in high school and college and didn’t know why.\nMathematical Tools for Understanding Infectious Disease Dynamics\nby Odo Diekmann, Hans Heesterbeek & Tom Britton\nMathematical epidemiology is a huge topic. This book introduces common models and approaches from first principles, with plenty of problems along the way to make sure you’re following along. Read it with a notebook and pencil handy.\n\n\n\nFigure 1: Credit: Wellcome Library, London. Wellcome Images images@wellcome.ac.uk http://wellcomeimages.org Plague treatise - a full page woodcut representing a man at a table studying a book, he is surrounded by four persons, - one holding a urine glass, Woodcut Darin durch sechs kurtzer Buchlin vil Heimlichkeiten der Natur beschriben werden Albertus Magnus Published: 1551\n\n\n\n\n\n",
    "preview": "posts/2018-12-27-first-year-books/first-year-books_files/figure-html5/plot-tape-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-the-iquit-r-video-series/",
    "title": "The IQUIT R video series",
    "description": "A series of introductory R videos.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2015-08-28",
    "categories": [
      "teaching"
    ],
    "contents": "\nI’ve uploaded 20+ R tutorials to YouTube for a new undergraduate course in Ecology and Evolutionary Biology at CU developed by Andrew Martin and Brett Melbourne, which in jocular anticipation was named IQUIT: an introduction to quantitative inference and thinking.\nWe made the videos to address the most common R programming problems that arose for students in the first iteration of the course. These short tutorials may be of use elsewhere:\nIntroduction to R\neverything is an object\naddition, subtraction, multiplication\nassignment\nNumeric vectors: 1\nvectors vs. scalars\ncreate vectors with c()\nNumeric vectors: 2\nhow to explore the structure of a vector\nclass, length, str\nFunctions in R\ninput and output\nsingle argument functions: sqrt, log, exp\nmulti-argument functions: round\nCreating special vectors: sequences and repetition\ngenerate integer sequence: :\ncreate sequence seq (hit args)\nrepeat something rep (also note argument structure)\nRelational operators and logical data types\nlogical types (intro to relational operators)\n==, !=, >, <, >=, <=\nTRUE and FALSE\nCharacter data\ncharacter objects\ncharacter vectors\nrelational operators on character vectors\n2-d data structures: matrices and data frames\ndata frames can hold lots of different data types\nmatrix elements must be of the same type\nIntro to indexing: matrices and vectors\nindexing and subsetting with [\nreview str\na bit with relational operators\nData frame subsetting and indexing\nindexing with relational operators\n3 ways to subset data frame: df[c(\"column names\")], df$column, df[, 1]\nR style & other secrets to happiness\nbasics of R style: spacing, alignment,\nbreaking up run-on lines\nworkspace management\nls, rm\nchoosing good names for files and objects\ncommenting\nWorking with data in R: 1\nreading in data with read.csv\nautomatic conversion of missing values to NA\nWorking with data in R: 2\nmixed type errors (numbers read in as characters because one cell has a letter)\nsearch path errors\nis.na\nVisualization part 1: intro to plot()\nplot\narguments: xlab, ylab, col\nVisualization part 2: other types of plots\nhistograms, jitter plots, line graphs\nVisualization part 3: adding data to plots\nadding points\nadding lines, and segments (also abline)\nVisualization part 4: annotation and legends\nannotation via text\nadding legends\nVisualization part 5: graphical parameters\ncommonly used parameters\nfor points: col, cex, pch (see ?points for pch options)\nfor lines: col, lwd, lty\nLooping repetitive tasks\nthe power of the for loop\ncreating objects to hold results ahead of time, rather than growing objects\nSummarizing data\nmean, sd, var, median\nRandomization & sampling distributions\nsample and rep\nDebugging R code 1: letting R find your data\nworking directory errors when reading in data\nproblems with typos, using objects that don’t exist\nDebugging R code 2: unreported errors\nerrors do not always bring error messages\nsteps to finding & fixing errors\nReplication and sample size\nexplore the effect of n on the uncertainty in a sample mean\nConveying uncertainty with confidence intervals while not obscuring the data\nconstructing confidence intervals\nplot CIs using the segments function\nDifferences in means\ngiven two populations, simulate the null sampling distribution of the difference in means\nrandomly assign individuals to a group using sample or some other scheme, then iteratively simulate differences in means with CIs\n\n\n\nFigure 1: Toby Hudson [CC BY-SA 2.5 au (https://creativecommons.org/licenses/by-sa/2.5/au/deed.en)], from Wikimedia Commons\n\n\n\n\n\n",
    "preview": "posts/2018-12-27-the-iquit-r-video-series/the-iquit-r-video-series_files/figure-html5/plot-tape-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-plotting-spatial-neighbors-in-ggplot2/",
    "title": "Plotting spatial neighbors in ggplot2",
    "description": "How to visualize spatial neighbors using ggplot2, spdep, and sf.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2015-06-15",
    "categories": [
      "visualization",
      "spatial"
    ],
    "contents": "\nThe R package spdep has great utilities to define spatial neighbors (e.g. dnearneigh, knearneigh, with a nice vignette to boot), but the plotting functionality is aimed at base graphics.\nSo, to save others some trouble, I thought I’d share a little snippet to convert a spatial neighbors object (of class nb) to an sf data frame.\n\n\nlibrary(sf)\nlibrary(spdep)\nlibrary(ggplot2)\n\nfname <- system.file(\"shape/nc.shp\", package=\"sf\")\nnc <- st_read(fname, quiet = TRUE)\n\nnc_sp <- as(nc, 'Spatial')\nneighbors <- poly2nb(nc_sp)\nneighbors_sf <- as(nb2lines(neighbors, coords = coordinates(nc_sp)), 'sf')\nneighbors_sf <- st_set_crs(neighbors_sf, st_crs(nc))\n\nggplot(nc) + \n  geom_sf(fill = 'salmon', color = 'white') +\n  geom_sf(data = neighbors_sf) +\n  theme_minimal() +\n  ylab(\"Latitude\") +\n  xlab(\"Longitude\")\n\n\n\n\n",
    "preview": "posts/2018-12-27-plotting-spatial-neighbors-in-ggplot2/plotting-spatial-neighbors-in-ggplot2_files/figure-html5/plot-neighbors-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-why-i-think-twice-before-editing-plots-in-powerpoint-illustrator-inkscape-etc/",
    "title": "Why I think twice before editing plots in Powerpoint, Illustrator, Inkscape, etc.",
    "description": "TLDR: scripting plots is more reproducible and efficient long term",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2015-02-26",
    "categories": [
      "visualization",
      "rants"
    ],
    "contents": "\nThanks to a nice post by Meghan Duffy on the Dynamic Ecology blog (How do you make figures?), we have some empirical evidence that many figures made in R by ecologists are secondarily edited in other programs including MS Powerpoint, Adobe Illustrator, Inkscape, and Photoshop. I do not do this for two reasons: reproducibility and bonus learning.\nReproducibility\nR is nice because results are relatively easy to reproduce. It’s free, and your code serves as a written record of what was done. When figures are edited outside of R, they can be much more difficult to reproduce. Independent of whether I am striving to maximize the reproducibility of my work for others, it behooves me to save time for my future self, ensuring that we (I?) can quickly update my own figures throughout the process of paper writing, submission, rewriting, resubmission, and so on.\nI had to learn this the hard way. The following figure was my issue: initially I created a rough version in R, edited it in Inkscape (~30 minutes invested), and ended up with a “final” version for submission.\n\n\n\nFigure 1: Figure from Does life history mediate changing disease risk when communities disassemble? (Joseph et al. 2013)\n\n\n\nTurns out that I had to remake the figure three times throughout the revision process (for the better). Eventually I realized I should to make the plot in R than to process it outside of R.\nIn retrospect, two things are clear:\nMy energy allocation strategy was not conducive to the revision process. I wasted time trying to make my “final” version look good in Inkscape, when I could have invested time to figure out how to make the figure as I wanted it in R. The payoff from this time investment will be a function of how much manipulation is done outside R, how hard it is to get the desired result in R, and how many times a figure will be re-made.\nI probably could have found a better way to display the data. Another post perhaps.\nLearning\nForcing myself to remake the figure exactly as I wanted it using only R had an unintended side effect: I learned more about base graphics in R. Now, when faced with similar situations, I can make similar plots much faster, because I know more graphical parameters and plotting functions. In contrast, point-and-click programs are inherently slow because I’m manually manipulating elements, usually with a mouse, and my mouse isn’t getting any faster.\n\n\nJoseph, Maxwell B, Joseph R Mihaljevic, Sarah A Orlofske, and Sara H Paull. 2013. “Does Life History Mediate Changing Disease Risk When Communities Disassemble?” Ecology Letters 16 (11): 1405–12. https://doi.org/10.1111/ele.12180.\n\n\n\n\n",
    "preview": "posts/2018-12-27-why-i-think-twice-before-editing-plots-in-powerpoint-illustrator-inkscape-etc/why-i-think-twice-before-editing-plots-in-powerpoint-illustrator-inkscape-etc_files/figure-html5/plot-tape-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-notes-on-shrinkage-and-prediction-in-hierarchical-models/",
    "title": "Notes on shrinkage and prediction in hierarchical models",
    "description": "Partial pooling and the best NBA free throw shooters of all time.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2014-12-13",
    "categories": [
      "teaching"
    ],
    "contents": "\nEcologists increasingly use mixed effects models, where some intercepts or slopes are fixed, and others are random (or varying). Often, confusion exists around whether and when to use fixed vs. random intercepts/slopes, which is understandable given their multiple definitions.\nIn an attempt to help clarify the utility of varying intercept models (and more generally, hierarchical modeling), specifically in terms of shrinkage and prediction, here is a GitHub repo with materials and a slideshow from our department’s graduate QDT (quantitative (th)ink tank) group.\nFor fun, I’ve included a toy example demonstrating the value of shrinkage when trying to rank NBA players by their free throw shooting ability, a situation with wildly varying amounts of information (free throw attempts) on each player.\n\n\n\nFigure 1: Kobe_Bryant_7144.jpg: Sgt. Joseph A. Leederivative work: JoeJohnson2 [Public domain], via Wikimedia Commons\n\n\n\nThe example admittedly is not ecological, and sensitive readers may replace free throw attempts with prey capture attempts for topical consistency. Many if not most ecological datasets suffer from similar issues, with varying amounts of information from different sites, species, individuals, etc., so even without considering predation dynamics of NBA players, the example’s relevance should be immediate.\n\n\n",
    "preview": "posts/2018-12-27-notes-on-shrinkage-and-prediction-in-hierarchical-models/notes-on-shrinkage-and-prediction-in-hierarchical-models_files/figure-html5/plot-tape-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-dynamic-occupancy-models-in-stan/",
    "title": "Dynamic occupancy models in Stan",
    "description": "Dynamic multi-year occupancy models, marginalizing over latent occurrence states.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2014-11-14",
    "categories": [
      "stan"
    ],
    "contents": "\nOccupancy modeling is possible in Stan as shown here, despite the lack of support for integer parameters (without marginalization). In many Bayesian applications of occupancy modeling, the true occupancy states (0 or 1) are directly modeled, but this can be avoided by marginalizing out the true occupancy state. The Stan manual (pg. 96) gives an example of this kind of marginalization for a discrete change-point model.\nFor a Stan implementation of a dynamic (multi-year) occupancy model (MacKenzie et al. 2003), see: https://github.com/stan-dev/example-models/tree/master/BPA/Ch.13\n\n\nMacKenzie, Darryl I, James D Nichols, James E Hines, Melinda G Knutson, and Alan B Franklin. 2003. “Estimating Site Occupancy, Colonization, and Local Extinction When a Species Is Detected Imperfectly.” Ecology 84 (8): 2200–2207. https://doi.org/10.1890/02-3090.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-categorical-spatial-data-extraction-around-buffered-points-in-r/",
    "title": "Categorical spatial data extraction around buffered points in R",
    "description": "Computing the proportion of land cover types using R and the raster package.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2014-11-08",
    "categories": [
      "spatial"
    ],
    "contents": "\nQuantifying categorical spatial data (e.g. land cover) around points can be done in a variety of ways, some of which require considerable amounts of patience, clicking around, and/or cash for a license. Here’s a bit of code that I cobbled together to quickly extract land cover data from the National Land Cover Database for a buffered point between Denver and Boulder - two cities in the state of Colorado.\nFirst, get data using the FedData package:\n\n\nlibrary(knitr)\nlibrary(raster)\nlibrary(FedData)\nlibrary(rgdal)\n\nsite_df <- data.frame(city = c('Denver', 'Boulder'), \n                      lat = c(39.7392, 40.0150), \n                      lon = c(-104.9903, -105.2705))\nmidpoint <- data.frame(lat = mean(site_df$lat), \n                       lon = mean(site_df$lon))\n\n# create spatial point data frame\ncoordinates(site_df) <- ~lon + lat\nproj4string(site_df) <- CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84\")\n\ncoordinates(midpoint) <- ~lon + lat\nproj4string(midpoint) <- CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84\")\n\nnlcd_raster <- get_nlcd(site_df, \n                        label = 'categorial-extraction', \n                        year = 2011, \n                        extraction.dir = '.')\n\n# reproject midpoint to raster's crs\nmidpoint <- spTransform(midpoint, projection(nlcd_raster))\n\nbuffer_distance_meters <- 5000\n\n# visualize buffered point and land cover data\nbuff_shp <- buffer(midpoint, buffer_distance_meters)\nplot(buff_shp) # I will plot over this, but it sets the plotting extent\nplot(nlcd_raster, add = TRUE)\nplot(buff_shp, add = TRUE)\n\n\nNow, we can use raster::extract to extract raster data around our point within some buffer, specified in meters.\n\n\nlandcover <- extract(nlcd_raster, midpoint, buffer = buffer_distance_meters)\n\nBut this object does not immediately provide the proportions of each cover type. Instead, it contains values from the cells within the buffer:\n\n\nstr(landcover)\n\nList of 1\n $ : num [1:87263] 21 21 21 21 21 21 21 22 21 21 ...\n\nWe can get the proportions of each class within the buffer as follows:\n\n\nlandcover_proportions <- lapply(landcover, function(x) {\n  counts_x <- table(x)\n  proportions_x <- prop.table(counts_x)\n  sort(proportions_x)\n  })\nsort(unlist(landcover_proportions))\n\n          41           42           31           81           82 \n0.0001260557 0.0001833538 0.0004125460 0.0066236549 0.0072997720 \n          24           90           95           52           23 \n0.0103480284 0.0119638335 0.0237213939 0.0713589952 0.0745562266 \n          21           11           22           71 \n0.1202456941 0.1244857500 0.1698543483 0.3788203477 \n\nResources\nLarge .img file processing in R (GIS) on Stack Overflow by Israel Del Toro\nNLCD website\n",
    "preview": "posts/2018-12-27-categorical-spatial-data-extraction-around-buffered-points-in-r/categorical-spatial-data-extraction-around-buffered-points-in-r_files/figure-html5/get-data-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-multilevel-modeling-of-community-composition-with-imperfect-detection/",
    "title": "Multilevel modeling of community composition with imperfect detection",
    "description": "A guest post by Joe Mihaljevic.",
    "author": [
      {
        "name": "Joseph Mihaljevic",
        "url": {}
      }
    ],
    "date": "2014-06-19",
    "categories": [
      "jags"
    ],
    "contents": "\nThis is a guest post generously provided by Joe Mihaljevic.\nA common goal of community ecology is to understand how and why species composition shifts across space. Common techniques to determine which environmental covariates might lead to such shifts typically rely on ordination of community data to reduce the amount of data. These techniques include redundancy analysis (RDA), canonical correspondence analysis (CCA), and nonmetric multi-dimensional scaling (NMDS), each paired with permutation tests. However, these ordination techniques do not discern species-level covariate effects, making it difficult to attribute community-level pattern shifts to species-level changes (Jackson et al. 2012). Jackson et al. (2012) propose a hierarchical modeling framework as an alternative, which we extend in this post to correct for imperfect detection.\nMultilevel models can estimate species-level random and fixed covariate effects to determine the relative contribution of environmental covariates to changing composition across space (Jackson et al. 2012). For presence/absence data, such models are often formulated as:\n\\[y_{q} \\sim \\text{Bernoulli}(\\psi_{q})\\]\n\\[\\psi_{q} = logit^{-1}(\\alpha_{spp[q]} + b_{spp[q]}  x_{site[q]})\\]\n\\[\\alpha_{spp[q]} \\sim N(\\mu_{\\alpha}, \\sigma_{intercept}^2)\\]\n\\[b_{spp[q]} \\sim N(\\mu_{b}, \\sigma_{slope}^2)\\]\nHere \\(y_q\\) is a vector of presences/absence of each species at each site (\\(q=1, ... , nm,\\) where \\(n\\) is the number of species and \\(m\\) the number of sites). This model can be extended to incorporate multiple covariates.\nWe are interested in whether species respond differently to environmental gradients (e.g. elevation, temperature, precipitation). If this is the case, then we expect community composition changes along such gradients. Concretely, we are interested in whether \\(\\sigma_{slope}^2\\) for any covariate differs from zero.\nJackson et al. (2012) provide code for a maximum likelihood implementation of their model with data from Southern Appalachian understory herbs using the R package lme4. Here we present a simple extension of Jackson and colleague’s work, correcting for detection error with repeat surveys (i.e. multi-species occupancy modeling). Specifically, the above model could be changed slightly to:\n\\[y_{q} \\sim \\text{Binomial}(z_q p_{spp[q]}, j_q)\\]\n\\[z_q \\sim \\text{Bernoulli}(\\psi_q)\\]\n\\[\\psi_{q} = logit^{-1}(\\alpha_{spp[q]} + b_{spp[q]}  x_{site[q]})\\]\n\\[\\alpha_{spp[q]} \\sim N(\\mu_{\\alpha}, \\sigma_{intercept}^2)\\]\n\\[b_{spp[q]} \\sim N(\\mu_{b}, \\sigma_{slope}^2)\\]\nNow \\(y_q\\) is the number of times each species is observed at each site over \\(j\\) surveys. \\(p_{spp[q]}\\) represents the species-specific probability of detection when the species is present, and \\(z_q\\) represents the ‘true’ occurence of the species, a Bernoulli random variable with probability, \\(\\psi_q\\).\nTo demonstrate the method, we simulate data for a 20 species community across 100 sites with 4 repeat surveys. We assume that three site-level environmental covariates were measured, two of which have variable affects on occurrence probabilities (i.e. random effects), and one of which has consistent effects for all species (i.e. a fixed effect). We also assumed that species-specific detection probabilities varied, but were independent of environmental covariates.\n\n\nlibrary(reshape2)\nlibrary(ggplot2)\n\n################################################\n# Simulate data\n################################################\n\nNsite <- 100\nNcov <- 3\nNspecies <- 20\nJ <- 4\nset.seed(1234)\n\n# species-specific intercepts:\nalpha <- rnorm(Nspecies, 0, 1)\n\n# covariate values\nXcov <- matrix(rnorm(Nsite*Ncov, 0, 2),\n               nrow=Nsite, ncol=Ncov)\n\n# I'll assume 2 of the 3 covariates have effects that vary among species\nBeta <- array(c(rnorm(Nspecies, 0, 2),\n                rnorm(Nspecies, -1, 1),\n                rep(1, Nspecies)\n                ),\n              dim=c(Nspecies, Ncov)\n              )\n\n# species-specific detection probs\np0 <- plogis(rnorm(Nspecies, 1, 0.5))\n\n#### Occupancy states ####\nYobs <- array(0, dim = c(Nspecies, Nsite)) # Simulated observations\n\nfor(n in 1:Nspecies){\n  for(k in 1:Nsite){\n    lpsi <- alpha[n] + Beta[n, ] %*% Xcov[k, ] # Covariate effects on occurrence\n    psi <- 1/(1+exp(-lpsi)) #anti-logit\n\n    z <- rbinom(1, 1, psi) # True Occupancy\n    Yobs[n, k] <- rbinom(1, J, p0[n] * z) # Observed Occupancy\n  }\n}\n\n################################################\n# Format data for model\n################################################\n# X needs to have repeated covariates for each species, long form\nX <- array(0, dim=c(Nsite*Nspecies, Ncov))\nt <- 1; i <- 1\nTT <- Nsite\nwhile(i <= Nspecies){\n  X[t:TT, ] <- Xcov\n  t <- t+Nsite\n  TT <- TT + Nsite\n  i <- i+1\n}\n\n# Species\nSpecies <- rep(c(1:Nspecies), each=Nsite)\n\n# Observations/data:\nY <- NULL\nfor(i in 1:Nspecies){\n  Y <- c(Y, Yobs[i, ])\n}\n\n# All sites surveyed same # times:\nJ <- rep(J, times=Nspecies*Nsite)\n\n# Number of total observations\nNobs <- Nspecies*Nsite\n\nEach species has a coefficient for each covariate that describes how the probability of occurrence responds. In one case, all species have the same response:\n\n\nbeta_df <- melt(Beta, varnames = c('Species', 'Covariate'))\nggplot(beta_df) + \n  geom_point(aes(value, Species)) + \n  facet_wrap(~Covariate, nrow = 1) + \n  xlab('Coefficient value') + \n  ggtitle('Species-level coefficients for each covariate (facets)') + \n  theme_minimal()\n\n\nWe fit the following model with JAGS with vague priors.\n\n\nmodel {\n  # Priors\n  psi.mean ~ dbeta(1,1)\n  p.detect.mean ~ dbeta(1,1)\n\n  sd.psi ~ dunif(0,10)\n  psi.tau <- pow(sd.psi, -2)\n\n  sd.p.detect ~ dunif(0,10)\n  p.detect.tau <- pow(sd.p.detect, -2)\n\n  for(i in 1:Nspecies){\n    alpha[i] ~ dnorm(logit(psi.mean), psi.tau)T(-12,12)\n    lp.detect[i] ~ dnorm(logit(p.detect.mean), p.detect.tau)T(-12,12)\n    p.detect[i] <- exp(lp.detect[i]) / (1 + exp(lp.detect[i]))\n  }\n\n  for(j in 1:Ncov){\n    mean.beta[j] ~ dnorm(0, 0.01)\n    sd.beta[j] ~ dunif(0, 10)\n    tau.beta[j] <- pow(sd.beta[j]+0.001, -2)\n    for(i in 1:Nspecies){\n      betas[i,j] ~ dnorm(mean.beta[j], tau.beta[j])\n    }\n  }\n\n  # Likelihood\n  for(i in 1:Nobs){\n    logit(psi[i]) <- alpha[Species[i]] + inprod(betas[Species[i],], X[i, ])\n    z[i] ~ dbern(psi[i])\n    Y[i] ~ dbinom(z[i] * p.detect[Species[i]], J[i])\n  }\n}\n\nUsing information theory, specifically Watanabe-Akaike information criteria (WAIC), we compared this model, which assumes all covariates have random effects, to all combination of models varying whether each covariate has fixed or random effects. See this GitHub repository for all model statements and code.\nA model that assumes all covariates have random effects, and the data-generating model, in which only covariates 1 and 2 have random effects, performed the best, but were indistinguishable from one another:\nThis result makes sense because the model with all random effects is able to recover species-specific responses to site-level covariates very well:\nHowever, this model estimates that the 95% HDI of \\(\\sigma_{slope}\\) of covariate 3 includes zero, indicating that this covariate effectively has a fixed, rather than random effect among species.\nThus, we could conclude that the first two covariates have random effects, while the third covariate has a fixed effect. This means that composition shifts along gradients of covariates 1 and 2. We can visualize the relative contribution of covariate 1 and 2’s random effects to composition using ordination, as discussed in Jackson et al. (2012). To do this, we compare the linear predictor (i.e. \\(\\text{logit}^{-1}(\\psi_q)\\)) of the best model that includes only significant random effects to a model that does not have any random effects.\nThe code to extract linear predictors and ordinate the community is provided on GitHub.\n\n\nJackson, Michelle M, Monica G Turner, Scott M Pearson, and Anthony R Ives. 2012. “Seeing the Forest and the Trees: Multilevel Models Reveal Both Species and Community Patterns.” Ecosphere 3 (9): 1–16. https://doi.org/10.1890/ES12-00116.1.\n\n\n\n\n",
    "preview": "posts/2018-12-27-multilevel-modeling-of-community-composition-with-imperfect-detection/multilevel-modeling-of-community-composition-with-imperfect-detection_files/figure-html5/plot-coefs-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-shiny-variance-inflation-factor-sandbox/",
    "title": "Shiny variance inflation factor sandbox",
    "description": "Exploring how correlation among covariates inflates uncertainty in coefficient estmates.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2014-04-03",
    "categories": [
      "shiny",
      "teaching"
    ],
    "contents": "\nIn multiple regression, strong correlations among covariates increases the uncertainty or variance in estimated regression coefficients. Variance inflation factors (VIFs) are one tool that has been used as an indicator of problematic covariate collinearity. In teaching students about VIFs, it may be useful to have some interactive supplementary material so that they can manipulate factors affecting the uncertainty in slope terms in real-time.\nHere’s a little R shiny app that could be used as a starting point for such a supplement: https://mbjoseph.shinyapps.io/vif-sandbox/ Currently it only includes two covariates for simplicity, and gives the user control over the covariate \\(R^2\\) value, the residual variance, and the variance of both covariates. Code is on GitHub: https://github.com/mbjoseph/vif\nScreenshot:\n\n\n\n\n\n",
    "preview": "posts/2018-12-27-shiny-variance-inflation-factor-sandbox/shiny-variance-inflation-factor-sandbox_files/figure-html5/plot-robot-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-27-stochastic-search-variable-selection-in-jags/",
    "title": "Stochastic search variable selection in JAGS",
    "description": "Using spike and slab priors to shrink coefficients toward zero.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2014-03-22",
    "categories": [
      "jags"
    ],
    "contents": "\nStochastic search variable selection (SSVS) identifies promising subsets of multiple regression covariates via Gibbs sampling (George and McCulloch 1993). Here’s a short SSVS demo with JAGS and R.\nAssume we have a multiple regression problem:\n\\[\\boldsymbol{Y} \\sim N_n(\\boldsymbol{X \\beta}, \\sigma^2 \\boldsymbol{I})\\]\nWe suspect only a subset of the elements of \\(\\boldsymbol{\\beta}\\) are non-zero, i.e. some of the covariates have no effect.\nAssume \\(\\boldsymbol{\\beta}\\) arises from one of two normal mixture components, depending on a latent variable \\(\\gamma_i\\):\n\\[\n\\beta_i \\mid \\gamma_i  \\sim \\left\\{\n  \\begin{array}{lr}\n    N(0, \\tau^2_i) &  \\gamma_i = 0\\\\\n    N(0, c^2_i \\tau^2_i) &  \\gamma_i = 1\n  \\end{array}\n\\right.\n\\]\n\\(\\tau_i\\) is positive but small s.t. \\(\\beta_i\\) is close to zero when \\(\\gamma_i = 0\\). \\(c_i\\) is large enough to allow reasonable deviations from zero when \\(\\gamma_i = 1\\). The prior probability that covariate \\(i\\) has a nonzero effect is \\(Pr(\\gamma_i = 1) = p_i\\).\nLet’s simulate a dataset in which some covariates have strong effects on the linear predictor, and other don’t.\n\n\nlibrary(gridExtra)\nlibrary(runjags)\nlibrary(ggmcmc)\nlibrary(coda)\nlibrary(knitr)\n\nncov <- 20\nnobs <- 60\nvar_beta <- .004\nc <- 1000\np_inclusion <- .5\nsigma_y <- 1\n\n# generate covariates\nX <- array(dim=c(nobs, ncov))\nfor (i in 1:ncov){\n  X[, i] <- rnorm(nobs, 0, 1)\n}\n\nincluded <- rbinom(ncov, 1, p_inclusion)\ncoefs <- rnorm(n=ncov,\n               mean=0,\n               sd=ifelse(included==1,\n                         sqrt(var_beta * c),\n                         sqrt(var_beta)\n                         )\n               )\ncoefs <- sort(coefs)\nY <- rnorm(nobs, mean=X %*% coefs, sd=sigma_y)\n\nSpecifying the model:\n\n\ncat(\"model{\n  alpha ~ dnorm(0, 1)\n  sd_y ~ dunif(0, 10)\n  tau_y <- pow(sd_y, -2)\n\n  # ssvs priors\n  sd_bet ~ dunif(0, 10)\n  tau_in <- pow(sd_bet, -2)\n  tau[1] <- tau_in            # coef effectively zero\n  tau[2] <- tau_in / 1000     # nonzero coef\n  p_ind[1] <- 1/2\n  p_ind[2] <- 1 - p_ind[1]\n\n  for (j in 1:ncov){\n    indA[j] ~ dcat(p_ind[]) # returns 1 or 2\n    gamma[j] <- indA[j] - 1   # returns 0 or 1\n    beta[j] ~ dnorm(0, tau[indA[j]])\n  }\n\n  # likelihood\n  for (i in 1:nobs){\n    Y[i] ~ dnorm(alpha + X[i ,] %*% beta[], tau_y)\n  }\n}\n    \"\n    , file=\"ssvs.txt\")\n\nFitting the model:\n\n\ndat <- list(Y=Y, X=X, nobs=nobs, ncov=ncov)\nvars <- c(\"alpha\", \"sd_bet\", \"gamma\", \"beta\", \"tau_in\", \"sd_y\")\nout <- run.jags(\"ssvs.txt\", vars, data=dat, n.chains=3,\n                adapt=10000, burnin=10000)\n\nCompiling rjags model...\nCalling the simulation using the rjags method...\nAdapting the model for 10000 iterations...\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |                                                  |   1%\n  |                                                        \n  |+                                                 |   2%\n  |                                                        \n  |++                                                |   3%\n  |                                                        \n  |++                                                |   4%\n  |                                                        \n  |++                                                |   5%\n  |                                                        \n  |+++                                               |   6%\n  |                                                        \n  |++++                                              |   7%\n  |                                                        \n  |++++                                              |   8%\n  |                                                        \n  |++++                                              |   9%\n  |                                                        \n  |+++++                                             |  10%\n  |                                                        \n  |++++++                                            |  11%\n  |                                                        \n  |++++++                                            |  12%\n  |                                                        \n  |++++++                                            |  13%\n  |                                                        \n  |+++++++                                           |  14%\n  |                                                        \n  |++++++++                                          |  15%\n  |                                                        \n  |++++++++                                          |  16%\n  |                                                        \n  |++++++++                                          |  17%\n  |                                                        \n  |+++++++++                                         |  18%\n  |                                                        \n  |++++++++++                                        |  19%\n  |                                                        \n  |++++++++++                                        |  20%\n  |                                                        \n  |++++++++++                                        |  21%\n  |                                                        \n  |+++++++++++                                       |  22%\n  |                                                        \n  |++++++++++++                                      |  23%\n  |                                                        \n  |++++++++++++                                      |  24%\n  |                                                        \n  |++++++++++++                                      |  25%\n  |                                                        \n  |+++++++++++++                                     |  26%\n  |                                                        \n  |++++++++++++++                                    |  27%\n  |                                                        \n  |++++++++++++++                                    |  28%\n  |                                                        \n  |++++++++++++++                                    |  29%\n  |                                                        \n  |+++++++++++++++                                   |  30%\n  |                                                        \n  |++++++++++++++++                                  |  31%\n  |                                                        \n  |++++++++++++++++                                  |  32%\n  |                                                        \n  |++++++++++++++++                                  |  33%\n  |                                                        \n  |+++++++++++++++++                                 |  34%\n  |                                                        \n  |++++++++++++++++++                                |  35%\n  |                                                        \n  |++++++++++++++++++                                |  36%\n  |                                                        \n  |++++++++++++++++++                                |  37%\n  |                                                        \n  |+++++++++++++++++++                               |  38%\n  |                                                        \n  |++++++++++++++++++++                              |  39%\n  |                                                        \n  |++++++++++++++++++++                              |  40%\n  |                                                        \n  |++++++++++++++++++++                              |  41%\n  |                                                        \n  |+++++++++++++++++++++                             |  42%\n  |                                                        \n  |++++++++++++++++++++++                            |  43%\n  |                                                        \n  |++++++++++++++++++++++                            |  44%\n  |                                                        \n  |++++++++++++++++++++++                            |  45%\n  |                                                        \n  |+++++++++++++++++++++++                           |  46%\n  |                                                        \n  |++++++++++++++++++++++++                          |  47%\n  |                                                        \n  |++++++++++++++++++++++++                          |  48%\n  |                                                        \n  |++++++++++++++++++++++++                          |  49%\n  |                                                        \n  |+++++++++++++++++++++++++                         |  50%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  51%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  52%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  53%\n  |                                                        \n  |+++++++++++++++++++++++++++                       |  54%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  55%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  56%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  57%\n  |                                                        \n  |+++++++++++++++++++++++++++++                     |  58%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  59%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  60%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  61%\n  |                                                        \n  |+++++++++++++++++++++++++++++++                   |  62%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  63%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  64%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  65%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++                 |  66%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  67%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  68%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  69%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++               |  70%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  71%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  72%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  73%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++             |  74%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  75%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  76%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  77%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++           |  78%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  79%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  80%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  81%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++         |  82%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  83%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  84%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  85%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++       |  86%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  87%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  88%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  89%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  91%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  92%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  93%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++   |  94%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  95%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  97%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++++ |  98%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++++|  99%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%\nBurning in the model for 10000 iterations...\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |                                                  |   1%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   3%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |**                                                |   5%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   7%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |****                                              |   9%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  11%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |******                                            |  13%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  15%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |********                                          |  17%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  19%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |**********                                        |  21%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  23%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |************                                      |  25%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  27%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |**************                                    |  29%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  31%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |****************                                  |  33%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  35%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |******************                                |  37%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  39%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |********************                              |  41%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  43%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |**********************                            |  45%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  47%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |************************                          |  49%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  51%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |**************************                        |  53%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  55%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |****************************                      |  57%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  59%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |******************************                    |  61%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  63%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |********************************                  |  65%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  67%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |**********************************                |  69%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  71%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |************************************              |  73%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  75%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |**************************************            |  77%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  79%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |****************************************          |  81%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  83%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |******************************************        |  85%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  87%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |********************************************      |  89%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  91%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |**********************************************    |  93%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  95%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************  |  97%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************|  99%\n  |                                                        \n  |**************************************************| 100%\nRunning the model for 10000 iterations...\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |                                                  |   1%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   3%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |**                                                |   5%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   7%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |****                                              |   9%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  11%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |******                                            |  13%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  15%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |********                                          |  17%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  19%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |**********                                        |  21%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  23%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |************                                      |  25%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  27%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |**************                                    |  29%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  31%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |****************                                  |  33%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  35%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |******************                                |  37%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  39%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |********************                              |  41%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  43%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |**********************                            |  45%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  47%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |************************                          |  49%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  51%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |**************************                        |  53%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  55%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |****************************                      |  57%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  59%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |******************************                    |  61%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  63%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |********************************                  |  65%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  67%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |**********************************                |  69%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  71%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |************************************              |  73%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  75%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |**************************************            |  77%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  79%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |****************************************          |  81%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  83%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |******************************************        |  85%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  87%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |********************************************      |  89%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  91%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |**********************************************    |  93%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  95%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************  |  97%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************|  99%\n  |                                                        \n  |**************************************************| 100%\nSimulation complete\nCalculating summary statistics...\nNote: The monitored variables 'gamma[1]', 'gamma[2]',\n'gamma[3]', 'gamma[19]' and 'gamma[20]' appear to be\nnon-stochastic; they will not be included in the convergence\ndiagnostic\nNote: The monitored variable 'gamma[4]' appears to be\nstochastic in one chain but non-stochastic in another chain;\nit will not be included in the convergence diagnostic\nCalculating the Gelman-Rubin statistic for 44 variables....\nFinished running the simulation\n\noutdf <- ggs(as.mcmc.list(out))\nout_summary <- summary(out)\n\nNow, let’s visualize the inclusion probabilities.\n\n\ngamma_rows <- grepl(rownames(out_summary), pattern = \"^gamma\\\\[\")\nprobs <- out_summary[gamma_rows, \"Mean\"]\n\nlabels <- rep(NA, ncov)\nfor (i in 1:ncov){\n  labels[i] <- paste(\"beta[\", i, \"]\", sep=\"\")\n}\nxdf <- data.frame(Parameter = labels, value = 1:ncov)\np1 <- ggs_caterpillar(outdf, \"beta\", X=xdf) +\n  theme_classic() +\n  geom_vline(xintercept = 0, linetype = \"longdash\") +\n  geom_point(data=data.frame(coefs, pos = 1:ncov),\n              aes(x=coefs, y=pos), size=5, col=\"green4\", alpha=.7) +\n  xlab(\"Value\") +\n  ylab(\"Coefficient\") +\n  geom_hline(yintercept = 1:ncov, alpha=.05) +\n  scale_y_continuous(breaks=seq(0, ncov, 1))\n\ndf <- data.frame(probs=probs, coefs = coefs)\np2 <- ggplot(df, aes(x=abs(coefs), y=probs)) +\n  geom_point(size=5, alpha=.7) +\n  theme_classic() +\n  xlab(\"Absolute value of true coefficient\") +\n  ylab(\"Posterior probability of non-zeroness\")\n\ngrid.arrange(p1, p2, ncol=2)\n\n\nOn the left, green points indicate true coefficient values, with black posterior credible intervals. The right plot shows the relationship between the true magnitude of the effect and the posterior probability that the coefficient was non-zero, \\(E(\\gamma_i \\mid \\boldsymbol{Y})\\).\n\n\nGeorge, Edward I, and Robert E McCulloch. 1993. “Variable Selection via Gibbs Sampling.” Journal of the American Statistical Association 88 (423): 881–89. https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476353.\n\n\n\n\n",
    "preview": "posts/2018-12-27-stochastic-search-variable-selection-in-jags/stochastic-search-variable-selection-in-jags_files/figure-html5/visualize-inclusion-probs-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-25-better-living-through-zero-one-inflated-beta-regression/",
    "title": "Better living through zero-one inflated beta regression",
    "description": "Fitting a Bayesian ZOIB regression model in JAGS.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2014-02-06",
    "categories": [
      "jags"
    ],
    "contents": "\nDealing with proportion data on the interval \\([0, 1]\\) is tricky. I realized this while trying to explain variation in vegetation cover. Unfortunately this is a true proportion, and can’t be made into a binary response. Further, true 0’s and 1’s rule out beta regression. You could arcsine square root transform the data (but shouldn’t; Warton and Hui 2011). Enter zero-and-one inflated beta regression.\nThe zero-and-one-inflated beta distribution facilitates modeling fractional or proportional data that contains both 0’s and 1’s (Ospina and Ferrari 2010 - highly recommended). The general idea is to model the response variable (call it \\(y\\)) as a mixture of Bernoulli and beta distributions, from which the true 0’s and 1’s, and the values between 0 and 1 are generated, respectively. The probability density function is\n\\[\nf_{\\text{ZOIB}}(y; \\alpha, \\gamma, \\mu, \\phi) = \\left\\{\n  \\begin{array}{lr}\n    \\alpha(1 - \\gamma) &  y = 0\\\\\n    \\alpha \\gamma &  y = 1\\\\\n    (1 - \\alpha)f(y; \\mu, \\phi) &  0 < y < 1\n  \\end{array}\n\\right.\n\\]\nwhere \\(0 < \\alpha, \\gamma, \\mu < 1\\), and \\(\\phi>0\\). \\(f(y; \\mu, \\phi)\\) is the probability density function for the beta distribution, parameterized in terms of its mean \\(\\mu\\) and precision \\(\\phi\\):\n\\[f_{\\text{beta}}(y; \\mu, \\phi) = \\dfrac{\\Gamma(\\phi)}{\\Gamma(\\mu \\phi) \\Gamma((1 - \\mu)\\phi)} y^{\\mu \\phi - 1} (1 - y)^{(1 - \\mu)\\phi - 1}\\]\n\\(\\alpha\\) is a mixture parameter that determines the extent to which the Bernoulli or beta component dominates the pdf. \\(\\gamma\\) determines the probability that \\(y=1\\) if it comes from the Bernoulli component. \\(\\mu\\) and \\(\\phi\\) are the expected value and the precision for the beta component, which is usually parameterized in terms of \\(p\\) and \\(q\\) (Ferrari and Cribari-Neto 2004). \\(\\mu = \\frac{p}{p + q}\\), and \\(\\phi=p+q\\).\nAlthough ecologists often deal with proportion data, I haven’t found any examples of 0 & 1 inflated beta regression in the ecological literature. Closest thing I’ve found was Nishii and Tanaka (2012) who take a different approach, where values between 0 and 1 are modeled as logit-normal.\nHere’s a quick demo in JAGS with simulated data. For simplicity, I’ll assume 1) there is one covariate that increases the expected value at the same rate for both the Bernoulli and beta components s.t. \\(\\mu = \\gamma\\), and 2) the Bernoulli component dominates extreme values of the covariate, where the expected value is near 0 or 1.\n\n\nlibrary(rjags)\nlibrary(ggmcmc)\nlibrary(reshape2)\n\nset.seed(1234)\nn <- 60\nx <- runif(n, -3, 3)\na0 <- -3\na1 <- 0\na2 <- 1\nantilogit <- function(x){\n  exp(x) / (1 + exp(x))\n}\nalpha <- antilogit(a0 + a1 * x + a2 * x^2)\n\nb0 <- 0\nb1 <- 2\nmu <- antilogit(b0 + b1 * x)\nphi <- 5\np <- mu * phi\nq <- phi - mu * phi\n\ny.discrete <- rbinom(n, 1, alpha)\ny <- rep(NA, n)\nfor (i in 1:n){\n  if (y.discrete[i] == 1){\n    y[i] <- rbinom(1, 1, mu[i])\n  } else {\n    y[i] <- rbeta(1, p[i], q[i])\n  }\n}\n\n# split the data into discrete and continuous components\ny.d <- ifelse(y == 1 | y == 0, y, NA)\ny.discrete <- ifelse(is.na(y.d), 0, 1)\ny.d <- y.d[!is.na(y.d)]\nx.d <- x[y.discrete == 1]\nn.discrete <- length(y.d)\n\nwhich.cont <- which(y < 1 & y > 0)\ny.c <- ifelse(y < 1 & y > 0, y, NA)\ny.c <- y.c[!is.na(y.c)]\nn.cont <- length(y.c)\nx.c <- x[which.cont]\n\nNow we can specify our model in JAGS, following the factorization of the likelihood given by Ospina and Ferrari (2010), estimate our parameters, and see how well the model performs.\n\n\n# write model\ncat(\n  \"\n  model{\n  # priors\n  a0 ~ dnorm(0, .001)\n  a1 ~ dnorm(0, .001)\n  a2 ~ dnorm(0, .001)\n  b0 ~ dnorm(0, .001)\n  b1 ~ dnorm(0, .001)\n  t0 ~ dnorm(0, .01)\n  tau <- exp(t0)\n\n  # likelihood for alpha\n  for (i in 1:n){\n    logit(alpha[i]) <- a0 + a1 * x[i] + a2 * x[i] ^ 2\n    y.discrete[i] ~ dbern(alpha[i])\n  }\n\n  # likelihood for gamma\n  for (i in 1:n.discrete){\n    y.d[i] ~ dbern(mu[i])\n    logit(mu[i]) <- b0 + b1 * x.d[i]\n  }\n\n  # likelihood for mu and tau\n  for (i in 1:n.cont){\n    y.c[i] ~ dbeta(p[i], q[i])\n    p[i] <- mu2[i] * tau\n    q[i] <- (1 - mu2[i]) * tau\n    logit(mu2[i]) <- b0 + b1 * x.c[i]\n  }\n  }  \n  \", file=\"beinf.txt\"\n)\n\njd <- list(x=x, y.d=y.d, y.c=y.c, y.discrete = y.discrete,\n           n.discrete=n.discrete, n.cont = n.cont,\n           x.d = x.d, x.c=x.c, n=n)\nmod <- jags.model(\"beinf.txt\", data= jd, n.chains=3, n.adapt=1000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 120\n   Unobserved stochastic nodes: 6\n   Total graph size: 849\n\nInitializing model\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |+                                                 |   2%\n  |                                                        \n  |++                                                |   4%\n  |                                                        \n  |+++                                               |   6%\n  |                                                        \n  |++++                                              |   8%\n  |                                                        \n  |+++++                                             |  10%\n  |                                                        \n  |++++++                                            |  12%\n  |                                                        \n  |+++++++                                           |  14%\n  |                                                        \n  |++++++++                                          |  16%\n  |                                                        \n  |+++++++++                                         |  18%\n  |                                                        \n  |++++++++++                                        |  20%\n  |                                                        \n  |+++++++++++                                       |  22%\n  |                                                        \n  |++++++++++++                                      |  24%\n  |                                                        \n  |+++++++++++++                                     |  26%\n  |                                                        \n  |++++++++++++++                                    |  28%\n  |                                                        \n  |+++++++++++++++                                   |  30%\n  |                                                        \n  |++++++++++++++++                                  |  32%\n  |                                                        \n  |+++++++++++++++++                                 |  34%\n  |                                                        \n  |++++++++++++++++++                                |  36%\n  |                                                        \n  |+++++++++++++++++++                               |  38%\n  |                                                        \n  |++++++++++++++++++++                              |  40%\n  |                                                        \n  |+++++++++++++++++++++                             |  42%\n  |                                                        \n  |++++++++++++++++++++++                            |  44%\n  |                                                        \n  |+++++++++++++++++++++++                           |  46%\n  |                                                        \n  |++++++++++++++++++++++++                          |  48%\n  |                                                        \n  |+++++++++++++++++++++++++                         |  50%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  52%\n  |                                                        \n  |+++++++++++++++++++++++++++                       |  54%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  56%\n  |                                                        \n  |+++++++++++++++++++++++++++++                     |  58%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  60%\n  |                                                        \n  |+++++++++++++++++++++++++++++++                   |  62%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  64%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++                 |  66%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  68%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++               |  70%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  72%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++             |  74%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  76%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++           |  78%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  80%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++         |  82%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  84%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++       |  86%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  88%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  92%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++   |  94%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++++ |  98%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%\n\nout <- coda.samples(mod, c(\"a0\", \"a1\", \"a2\", \"b0\", \"b1\", \"tau\"),\n                    n.iter=6000)\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************| 100%\n\nggd <- ggs(out)\na0.post <- subset(ggd, Parameter == \"a0\")$value\na1.post <- subset(ggd, Parameter == \"a1\")$value\na2.post <- subset(ggd, Parameter == \"a2\")$value\nn.stored <- length(a2.post)\nP.discrete <- array(dim=c(n.stored, n))\nfor (i in 1:n){\n  P.discrete[, i] <- antilogit(a0.post + a1.post * x[i] + a2.post * x[i] ^ 2)\n}\npdd <- melt(P.discrete, varnames = c(\"iteration\", \"site\"), \n            value.name = \"Pr.discrete\")\npdd$x <- x[pdd$site]\nb0.post <- subset(ggd, Parameter == \"b0\")$value\nb1.post <- subset(ggd, Parameter == \"b1\")$value\nexpect <- array(dim=c(n.stored, n))\nfor (i in 1:n){\n  expect[, i] <- antilogit(b0.post + b1.post * x[i])\n}\nexd <- melt(expect, varnames=c(\"iteration\", \"site\"), value.name = \"Expectation\")\nexd$x <- x[exd$site]\nobsd <- data.frame(x=x, y=y,\n                   component = factor(ifelse(y < 1 & y > 0, \n                                             \"Continuous\", \"Discrete\")))\ntrued <- data.frame(x=x, mu=mu)\n\nggplot(pdd) +\n  geom_line(aes(x=x, y=Pr.discrete, group=iteration), \n            alpha=0.05, color=\"grey\") +\n  geom_line(aes(x=x, y=Expectation, group=iteration), \n            data=exd, color=\"blue\", alpha=0.05) +\n  geom_point(aes(x=x, y=y, fill=component), \n             data=obsd, size=3, color=\"black\",\n             position = position_jitter(width=0, height=.01), pch=21) +\n  scale_fill_manual(values = c(\"red\", \"white\"), \"Component\") +\n  ylab(\"y\") +\n  geom_line(aes(x=x, y=mu), \n            data=trued, color=\"green\", linetype=\"dashed\") +\n  theme_bw()\n\n\nHere the posterior probability that \\(y\\) comes from the discrete Bernoulli component is shown in grey, and the posterior expected value for both the Bernoulli and beta components across values of the covariate are shown in blue. The dashed green line shows the true expected value that was used to generate the data. Finally, the observed data are shown as jittered points, color coded as being derived from the continuous beta component, or the discrete Bernoulli component.\n\n\n",
    "preview": "posts/2018-12-25-better-living-through-zero-one-inflated-beta-regression/better-living-through-zero-one-inflated-beta-regression_files/figure-html5/write-model-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-25-errors-in-variables-models-in-stan/",
    "title": "Errors-in-variables models in stan",
    "description": "Fitting a Bayesian regression with covariate uncertainty.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-11-27",
    "categories": [
      "stan"
    ],
    "contents": "\nHere, I’ll describe a Bayesian approach for estimation and correction for covariate measurement error using a latent-variable based errors-in-variables model, that one might use when there is uncertainty in the covariate for a linear model. Recall that this matters because error in covariate measurements tends to bias slope estimates towards zero.\nFor what follows, we’ll assume a simple linear regression, in which continuous covariates are measured with error. True covariate values are considered latent variables, with repeated measurements of covariate values arising from a normal distribution with a mean equal to the true value, and some measurement error \\(\\sigma_x\\), such that \\(\\epsilon_x \\sim \\text{Normal}(0, \\sigma_x)\\) and \\(\\epsilon_y \\sim \\text{Normal}(0, \\sigma_y)\\), where \\(\\epsilon_x\\) represents error in the covariate, and \\(\\epsilon_y\\) represents error in the response variable.\nWe assume that for sample unit \\(i\\) and repeat measurement \\(j\\):\n\\[ x^{obs}_{ij} \\sim \\text{Normal}(x_i, \\sigma_x) \\]\n\\[ y_i \\sim \\text{Normal}(\\alpha + \\beta x_i, \\sigma_y) \\]\nThe trick here is to use repeated measurements of the covariates to estimate and correct for measurement error. In order for this to be valid, the true covariate values cannot vary across repeat measurements. If the covariate was individual weight, you would have to ensure that the true weight did not vary across repeat measurements (for me, frogs urinating during handling would violate this assumption).\nBelow, I’ll simulate some data of this type in R. I’m assuming that we randomly select some sampling units to remeasure covariate values, and each is remeasured n.reps times.\n\n\nlibrary(rstan)\nrstan_options(auto_write = TRUE)\noptions(mc.cores = parallel::detectCores())\n\nset.seed(1234)\nn.reps <- 3\nn.repeated <- 10\nn <- 30\n\n# true covariate values\nx <- runif(n, -3, 3)\ny <- x + rnorm(n)  # alpha=0, beta=1, sdy=1\n\n# random subset to perform repeat covariate measurements\nwhich.repeated <- sample(n, n.repeated)\nxsd <- .5  # measurement error\nxerr <- rnorm(n + (n.repeated * (n.reps - 1)), 0, xsd)\n\n# indx assigns measurements to sample units\nindx <- c(1:n, rep(which.repeated, each = n.reps - 1))\nindx <- sort(indx)\nnobs <- length(indx)\nxobs <- x[indx] + xerr\nplot(x[indx], xobs,\n    xlab = \"True covariate value\",\n    ylab = \"Observed covariate value\")\nabline(0, 1, lty = 2)\nsegments(x0 = x[indx], x1 = x[indx],\n    y0 = x[indx], y1 = xobs, col = \"red\")\nabline(v = x[which.repeated], col = \"green\", lty = 3)\n\n\nHere, the discrepancy due to measurement error is shown as a red segment, and the sample units that were measured three times are highlighted with green dashed lines.\nI’ll use stan to estimate the model parameters, because I’ll be refitting the model to new data sets repeatedly below, and Stan is faster than JAGS for these models.\n\n\n# write the .stan file\ncat(\"\ndata{\n  int n;\n  int nobs;\n  real xobs[nobs];\n  real y[n];\n  int indx[nobs];\n}\n\nparameters {\n  real alpha;\n  real beta;\n  real<lower=0> sigmay;\n  real<lower=0> sigmax;\n  real x[n];\n}\n\nmodel {\n  // priors\n  alpha ~ normal(0, 1);\n  beta ~ normal(0, 1);\n  sigmay ~ normal(0, 1);\n  sigmax ~ normal(0, 1);\n\n  // model structure  \n  for (i in 1:nobs){\n    xobs[i] ~ normal(x[indx[i]], sigmax);\n  }\n  for (i in 1:n){\n    y[i] ~ normal(alpha + beta*x[i], sigmay);\n  }\n}\n\n  \",\n    file = \"latent_x.stan\")\n\nWith the model specified, estimate the parameters.\n\n\nstan_d <- c(\"y\", \"xobs\", \"nobs\", \"n\", \"indx\")\nchains <- 3\niter <- 1000\nthin <- 1\nmod1 <- stan(file = \"latent_x.stan\", data = stan_d,\n    chains = chains, iter = iter,\n    thin = thin)\n\nHow did we do? Let’s compare the true vs. estimated covariate values for each sample unit.\n\n\nposteriors <- rstan::extract(mod1)\n\n# highest density interval helper function (thanks to Joe Mihaljevic)\nHDI <- function(values, percent = 0.95) {\n    sorted <- sort(values)\n    index <- floor(percent * length(sorted))\n    nCI <- length(sorted) - index\n    width <- rep(0, nCI)\n    for (i in 1:nCI) {\n        width[i] <- sorted[i + index] - sorted[i]\n    }\n    HDImin <- sorted[which.min(width)]\n    HDImax <- sorted[which.min(width) + index]\n    HDIlim <- c(HDImin, HDImax)\n    return(HDIlim)\n}\n\n# comparing estimated true x values to actual x values\nXd <- array(dim = c(n, 3))\nfor (i in 1:n) {\n    Xd[i, 1:2] <- HDI(posteriors$x[, i])\n    Xd[i, 3] <- median(posteriors$x[, i])\n}\n\nlims <- c(min(Xd), max(Xd))\nplot(x, Xd[, 3], xlab = \"True covariate value\",\n    ylab = \"Estimated covariate value\",\n    col = \"purple\", pch = 19, ylim = lims)\nabline(0, 1, lty = 2)\nsegments(x0 = x, x1 = x, y0 = Xd[, 1], y1 = Xd[, 2], col = \"purple\")\n\n\nHere purple marks the posterior for the covariate values, and the dashed black line shows the one-to-one line that we would expect if the estimates exactly matched the true values. In addition to estimating the true covariate values, we may wish to check to see how well we estimated the standard deviation of the measurement error in our covariate.\n\n\nhist(posteriors$sigmax, breaks = 30,\n    main = \"Posterior for measurement error\",\n    xlab = \"Measurement standard deviation\")\nabline(v = xsd, col = \"red\", lwd = 2)\nlegend(\"topright\", legend = \"True value\", col = \"red\",\n    lty = 1, bty = \"n\", lwd = 2)\n\n\nHow many sample units need repeat measurements?\nYou may want to know how many sample units need to be repeatedly measured to adequately estimate the degree of covariate measurement error. For instance, if \\(\\sigma_x = 1\\), how does the precision in our estimate of \\(\\sigma_x\\) improve as more sample units are repeatedly measured? Let’s see what happens when we repeatedly measure covariate values for \\(1, 2, ..., N\\) randomly selected sampling units.\n\n\nn.repeated <- 1:n\n\n# store the HDI and mode for the estimate of sigmax in an array\npost.sdx <- array(dim = c(length(n.repeated), 3))\nfor (i in n.repeated) {\n    n.repeats <- i\n    which.repeated <- sample(n, n.repeats)\n    xerr <- rnorm(n + (n.repeats * (n.reps - 1)), 0, xsd)\n    indx <- c(1:n, rep(which.repeated, each = n.reps - 1))\n    indx <- sort(indx)\n    nobs <- length(indx)\n    xobs <- x[indx] + xerr\n    stan_d <- c(\"y\", \"xobs\", \"nobs\", \"n\", \"indx\")\n    mod <- stan(fit = mod1, data = stan_d, chains = chains,\n        iter = iter, thin = thin)\n    posteriors <- extract(mod)\n    post.sdx[i, 1:2] <- HDI(posteriors$sigmax)\n    post.sdx[i, 3] <- median(posteriors$sigmax)\n}\n\n\n\n# Plot the relationship b/t number of sampling units revisited & sdx\nplot(x = n.repeated, y = rep(xsd, length(n.repeated)),\n    type = \"l\", lty = 2,\n    ylim = c(0, max(post.sdx)),\n    xlab = \"Number of sampling units measured three times\",\n    ylab = \"Estimated measurement error\")\nsegments(x0 = n.repeated, x1 = n.repeated,\n    y0 = post.sdx[, 1], y1 = post.sdx[, 2],\n    col = \"red\")\npoints(x = n.repeated, y = post.sdx[, 3], col = \"red\")\nlegend(\"topright\", legend = c(\"True value\", \"Posterior estimate\"),\n    col = c(\"black\", \"red\"), lty = c(2, 1),\n    pch = c(NA, 1), bty = \"n\")\n\n\nLooking at this plot, you could eyeball the number of sample units that should be remeasured when designing a study. Realistically, you might want to explore how this number depends on the true amount of measurement error, and also simulate multiple realizations (rather than just one) for each scenario. Using a similar approach, you might also evaluate whether it’s more efficient to remeasure more sample units, or invest in more repeated measurements per sample unit.\n\n\n",
    "preview": "posts/2018-12-25-errors-in-variables-models-in-stan/errors-in-variables-models-in-stan_files/figure-html5/simulate-data-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-25-r-r-markdown-and-my-divorce-from-microsoft-word/",
    "title": "R Markdown and my divorce from Microsoft Word",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-10-30",
    "categories": [
      "rants"
    ],
    "contents": "\nI do a lot of scholarly writing that requires associated or embedded R analyses, figures, and tables, plus bibliographies.\nMicrosoft Word makes this unnecessarily difficult.\nMany tools are now available to break free from the tyranny of Word. The ones I like involve writing an article in markdown format, integrating all data preparation, analysis, and outputs with the document (e.g. with the excellent and accessible knitr package or with a custom make set up like this one). Add in version control with Git, and you’ve got a nice stew going.\nIf you’re involved in the open source/reproducible research blogo-twittersphere, this is probably old hat. To many others, it’s not.\nMost scientists I see in the wild still manually insert figures and results from statistical analyses in Word documents, perhaps the manufacturing equivalent of hand-crafting each document. R markdown provides a level of automation that is amenable to creating many documents or recreating/updating one document many times, the manufacturing equivalent of automated robots that increase efficiency (but do require some programming to function properly).\n\n\n\nI can’t give an authoritative overview, but here are some resources that helped me get through my divorce with Microsoft Word:\nR Markdown = knitr + RStudio may be one of the better places to start\nHow to ditch Word by Karthik Ram\nMarkdown and the future of collaborative manuscript writing by Karthik Ram\nGit can facilitate greater reproducibility and increased transparency in science (Ram 2013)\nWhat is scholarly markdown? by Martin Fenner\nMarkdown for scientific writing\nPandoc to convert from markdown to almost any other format\npandoc-citeproc for citations\nCitations in markdown using knitr for another take on citations, from Carl Boettiger\nGetting started with make\n\n\nRam, Karthik. 2013. “Git Can Facilitate Greater Reproducibility and Increased Transparency in Science.” Source Code for Biology and Medicine 8 (1): 7. https://doi.org/10.1186/1751-0473-8-7.\n\n\n\n\n",
    "preview": "posts/2018-12-25-r-r-markdown-and-my-divorce-from-microsoft-word/r-r-markdown-and-my-divorce-from-microsoft-word_files/figure-html5/plot-robot-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-25-animating-the-metropolis-algorithm/",
    "title": "Animating the Metropolis algorithm",
    "description": "A homemade Metropolis algorithm animation using R and the animation package.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-09-08",
    "categories": [
      "visualization",
      "teaching"
    ],
    "contents": "\nThe Metropolis algorithm, and its generalization (Metropolis-Hastings algorithm) provide elegant methods for obtaining sequences of random samples from complex probability distributions (Beichl and Sullivan 2000). When I first read about modern MCMC methods, I had trouble visualizing the convergence of Markov chains in higher dimensional cases. So, I thought I might put together a visualization in a two-dimensional case.\nI’ll use a simple example: estimating a population mean and standard deviation. We’ll define some population level parameters, collect some data, then use the Metropolis algorithm to simulate the joint posterior of the mean and standard deviation.\n\n\nlibrary(sm)\n\n# population level parameters\nmu <- 7\nsigma <- 3\n\n# collect some data (e.g. a sample of heights)\nn <- 50\nx <- rnorm(n, mu, sigma)\n\n# log-likelihood function\nll <- function(x, muhat, sigmahat){\n  sum(dnorm(x, muhat, sigmahat, log=T))\n}\n\n# prior densities\npmu <- function(mu){\n  dnorm(mu, 0, 100, log=T)\n}\n\npsigma <- function(sigma){\n  dunif(sigma, 0, 10, log=T)\n}\n\n# posterior density function (log scale)\npost <- function(x, mu, sigma){\n  ll(x, mu, sigma) + pmu(mu) + psigma(sigma)\n}\n\ngeninits <- function(){\n  list(mu = runif(1, 4, 10),\n       sigma = runif(1, 2, 6))\n}\n\njump <- function(x, dist = .2){ # must be symmetric\n  x + rnorm(1, 0, dist)\n}\n\niter = 10000\nchains <- 3\nposterior <- array(dim = c(chains, 2, iter))\naccepted <- array(dim=c(chains, iter - 1))\n\nset.seed(1234)\nfor (c in 1:chains){\n  theta.post <- array(dim=c(2, iter))\n  inits <- geninits()\n  theta.post[1, 1] <- inits$mu\n  theta.post[2, 1] <- inits$sigma\n  for (t in 2:iter){\n    # theta_star = proposed next values for parameters\n    theta_star <- c(jump(theta.post[1, t-1]), jump(theta.post[2, t-1]))\n    pstar <- post(x, mu = theta_star[1], sigma = theta_star[2])  \n    pprev <- post(x, mu = theta.post[1, t-1], sigma = theta.post[2, t-1])\n    lr <- pstar - pprev\n    r <- exp(lr)\n\n    # theta_star is accepted if posterior density is higher w/ theta_star\n    # if posterior density is not higher, it is accepted with probability r\n    # else theta does not change from time t-1 to t\n    accept <- rbinom(1, 1, prob = min(r, 1))\n    accepted[c, t - 1] <- accept\n    if (accept == 1){\n      theta.post[, t] <- theta_star\n    } else {\n      theta.post[, t] <- theta.post[, t-1]\n    }\n  }\n  posterior[c, , ] <- theta.post\n}\n\nThen, to visualize the evolution of the Markov chains, we can make plots of the chains in 2-parameter space, along with the posterior density at different iterations, joining these plots together using ImageMagick’s convert command (in the terminal) to create an animated .gif:\n\n\nsequence <- unique(round(exp(seq(0, log(iter), length.out = 150))))\n\nxlims <- c(4, 10)\nylims <- c(1, 6)\n\nfor (i in sequence){\n  par(mfrow=c(1, 2))\n  plot(posterior[1, 1, 1:i], posterior[1, 2, 1:i],\n       type=\"l\", xlim=xlims, ylim=ylims, col=\"blue\",\n       xlab=\"mu\", ylab=\"sigma\", main=\"Markov chains\")\n  lines(posterior[2, 1, 1:i], posterior[2, 2, 1:i],\n        col=\"purple\")\n  lines(posterior[3, 1, 1:i], posterior[3, 2, 1:i],\n        col=\"red\")\n  text(x=7, y=1.2, paste(\"Iteration \", i), cex=1.5)\n  sm.density(x=cbind(c(posterior[, 1, 1:i]), c(posterior[, 2, 1:i])),\n             xlab=\"mu\", ylab=\"sigma\",\n             zlab=\"\", zlim=c(0, .7),\n             xlim=xlims, ylim=ylims, col=\"white\", \n             verbose=0)\n  title(\"Posterior density\")\n}\n\n\n\n\nBeichl, Isabel, and Francis Sullivan. 2000. “The Metropolis Algorithm.” Computing in Science & Engineering 2 (1): 65–69. https://doi.org/10.1109/5992.814660.\n\n\n\n\n",
    "preview": "posts/2018-12-25-animating-the-metropolis-algorithm/animating-the-metropolis-algorithm_files/figure-html5/create-gif.gif",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-24-quantifying-uncertainty-around-r-squared-for-generalized-linear-models/",
    "title": "Quantifying uncertainty around R-squared for generalized linear models",
    "description": "How to propage posterior uncertainty to R-squared in R and JAGS.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-08-22",
    "categories": [
      "jags"
    ],
    "contents": "\nPeople love \\(R^2\\). As such, when Nakagawa and Schielzeth published A general and simple method for obtaining \\(R^2\\) from generalized linear mixed-effects models in Methods in Ecology and Evolution earlier this year (Nakagawa and Schielzeth 2013), ecologists (amid increasing use of generalized linear mixed models (GLMMs)) rejoiced. Now there’s an R function that automates \\(R^2\\) calculations for GLMMs fit with the lme4 package.\n\\(R^2\\) is usually reported as a point estimate of the variance explained by a model, using the maximum likelihood estimates of the model parameters and ignoring uncertainty around these estimates. Nakagawa and Schielzeth (2013) noted that it may be desirable to quantify the uncertainty around \\(R^2\\) using MCMC sampling. So, here we are.\nBackground\n\\(R^2\\) quantifies the proportion of observed variance explained by a statistical model. When it is large (near 1), much of the variance in the data is explained by the model.\nNakagawa and Schielzeth (2013) present two \\(R^2\\) statistics for generalized linear mixed models:\nMarginal \\(R^2_{GLMM(m)}\\), which represents the proportion of variance explained by fixed effects:\n\\[R^2_{GLMM(m)} = \\frac{\\sigma^2_f}{\\sigma^2_f + \\sum_{l=1}^{u}\\sigma^2_l + \\sigma^2_d + \\sigma^2_e}\\]\nwhere \\(\\sigma^2_f\\) represents the variance in the fitted values (on a link scale) based on the fixed effects:\n\\[ \\sigma^2_f = var(\\boldsymbol{X \\beta}) \\]\n\\(\\boldsymbol{X}\\) is the design matrix of the fixed effects, and \\(\\boldsymbol{\\beta}\\) is the vector of fixed effects estimates.\n\\(\\sum_{l=1}^{u}\\sigma^2_l\\) represents the sum the variance components for all of \\(u\\) random effects. \\(\\sigma^2_d\\) is the distribution-specific variance (Nakagawa and Schielzeth 2010), and \\(\\sigma^2_e\\) represents added dispersion.\nConditional \\(R^2_{GLMM(c)}\\) represents the proportion of variance explained by the fixed and random effects combined:\n\\[ R^2_{GLMM(c)} = \\frac{\\sigma^2_f + \\sum_{l=1}^{u}\\sigma^2_l}{\\sigma^2_f + \\sum_{l=1}^{u}\\sigma^2_l + \\sigma^2_d + \\sigma^2_e} \\]\nPoint-estimation of \\(R^2_{GLMM}\\)\nHere, I’ll follow the example of an overdispersed Poisson GLMM provided in the supplement to Nakagawa & Schielzeth (Nakagawa and Schielzeth 2013). This is their most complicated example, and the simpler ones ought to be relatively straightforward for those that are interested in normal or binomial GLMMs.\n\n\nlibrary(arm)\nlibrary(ggmcmc)\nlibrary(lme4)\nlibrary(rjags)\n\n# First, simulate data (code adapted from Nakagawa & Schielzeth 2013):\nn_population <- 8\nn <- 100\nPopulation <- gl(n_population, k = n / n_population, n)\n\nn_container <- 10\nContainer <- gl(n_container, n / n_container, n)\n\n# Sex of the individuals. Uni-sex within each container (individuals are\n# sorted at the pupa stage)\nSex <- factor(sample(c(\"Female\", \"Male\"), n, replace = TRUE))\n\n# Habitat at the collection site: dry or wet soil (four indiviudal from\n# each Habitat in each container)\nHabitat <- factor(sample(c(\"dry\", \"wet\"), n, replace = TRUE))\n\n# Food treatment at the larval stage: special food ('Exp') or standard\n# food ('Cont')\nTreatment <- factor(sample(c(\"Cont\", \"Exp\"), n, replace = TRUE))\n\n# Data combined in a dataframe\nData <- data.frame(Population = Population,\n    Container = Container, Sex = Sex,\n    Habitat = Habitat, Treatment = Treatment)\n\n# Subset the design matrix (only females express colour morphs)\nDataF <- Data[Data$Sex == \"Female\", ]\n\n# random effects\nPopulationE <- rnorm(n_population, 0, sqrt(0.4))\nContainerE <- rnorm(n_container, 0, sqrt(0.05))\n\n# generation of response values on link scale (!) based on fixed effects,\n# random effects and residual errors\nEggLink <- with(DataF,\n                  1.1 +\n                  0.5 * (as.numeric(Treatment) - 1) +\n                  0.1 * (as.numeric(Habitat) - 1) +\n                  PopulationE[Population] +\n                  ContainerE[Container])\n\n# data generation (on data scale!) based on Poisson distribution\nDataF$Egg <- rpois(length(EggLink), exp(EggLink))\n\nHaving simulated a dataset, calculate the \\(R^2\\) point-estimates, using the lme4 package to fit the model.\n\n\n# Creating a dummy variable that allows estimating additive dispersion in\n# glmer This triggers a warning message when fitting the model\nUnit <- factor(1:length(DataF$Egg))\n\n# Fit null model without fixed effects (but including all random effects)\nm0 <- glmer(Egg ~ 1 + (1 | Population) + (1 | Container) + (1 | Unit),\n    family = \"poisson\", data = DataF)\n\n# Fit alternative model including fixed and all random effects\nmF <- glmer(Egg ~ Treatment + Habitat + (1 | Population) + (1 | Container) +\n    (1 | Unit), family = \"poisson\", data = DataF)\n\n# View model fits for both models\nsummary(m0)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: Egg ~ 1 + (1 | Population) + (1 | Container) + (1 | Unit)\n   Data: DataF\n\n     AIC      BIC   logLik deviance df.resid \n   257.7    265.8   -124.9    249.7       51 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5910 -0.5656 -0.3137  0.5858  1.7928 \n\nRandom effects:\n Groups     Name        Variance  Std.Dev. \n Unit       (Intercept) 4.377e-08 0.0002092\n Container  (Intercept) 5.213e-07 0.0007220\n Population (Intercept) 4.986e-01 0.7061462\nNumber of obs: 55, groups:  Unit, 55; Container, 10; Population, 8\n\nFixed effects:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.5526     0.2604   5.962 2.49e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nconvergence code: 0\nModel failed to converge with max|grad| = 0.00622563 (tol = 0.001, component 1)\n\nsummary(mF)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: \nEgg ~ Treatment + Habitat + (1 | Population) + (1 | Container) +  \n    (1 | Unit)\n   Data: DataF\n\n     AIC      BIC   logLik deviance df.resid \n   257.9    269.9   -122.9    245.9       49 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.51389 -0.55034 -0.08826  0.47550  2.07213 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n Unit       (Intercept) 0.0000   0.0000  \n Container  (Intercept) 0.0000   0.0000  \n Population (Intercept) 0.4939   0.7028  \nNumber of obs: 55, groups:  Unit, 55; Container, 10; Population, 8\n\nFixed effects:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.42059    0.27659   5.136 2.81e-07 ***\nTreatmentExp  0.23000    0.11652   1.974   0.0484 *  \nHabitatwet    0.05464    0.11964   0.457   0.6479    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) TrtmnE\nTreatmntExp -0.232       \nHabitatwet  -0.292  0.153\nconvergence code: 0\nboundary (singular) fit: see ?isSingular\n\n# Extraction of fitted value for the alternative model fixef() extracts\n# coefficents for fixed effects model.matrix(mF) returns design matrix\nFixed <- fixef(mF)[2] * model.matrix(mF)[, 2] + fixef(mF)[3] * model.matrix(mF)[, 3]\n\n# Calculation of the variance in fitted values\nVarF <- var(Fixed)\n\n# An alternative way for getting the same result\nVarF <- var(as.vector(fixef(mF) %*% t(model.matrix(mF))))\n\n# R2GLMM(m) - marginal R2GLMM see Equ. 29 and 30 and Table 2 fixef(m0)\n# returns the estimate for the intercept of null model\nR2m <- VarF/(VarF + VarCorr(mF)$Container[1] +\n               VarCorr(mF)$Population[1] + VarCorr(mF)$Unit[1] +\n                log(1 + 1/exp(as.numeric(fixef(m0))))\n            )\n\n# R2GLMM(c) - conditional R2GLMM for full model Equ. XXX, XXX\nR2c <- (VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1])/\n         (VarF + VarCorr(mF)$Container[1] + VarCorr(mF)$Population[1] +\n           VarCorr(mF)$Unit[1] + log(1 + 1/exp(as.numeric(fixef(m0))))\n         )\n\n# Print marginal and conditional R-squared values\ncbind(R2m, R2c)\n\n           R2m       R2c\n[1,] 0.0181809 0.7251428\n\nHaving stored our point estimates, we can now turn to Bayesian methods instead, and generate \\(R^2\\) posteriors.\nPosterior uncertainty in \\(R^2_{GLMM}\\)\nWe need to fit two models in order to get the needed parameters for \\(R^2_{GLMM}\\). First, a model that includes all random effects, but only an intercept fixed effect is fit to estimate the distribution specific variance \\(\\sigma^2_d\\). Second, we fit a model that includes all random and all fixed effects to estimate the remaining variance components.\nFirst I’ll clean up the data that we’ll feed to JAGS:\n\n\n# Prepare the data\njags_d <- as.list(DataF)[-c(2, 3)]  # redefine container, don't need sex\njags_d$nobs <- nrow(DataF)\njags_d$npop <- length(unique(jags_d$Population))\n\n# renumber containers from 1:ncontainer for ease of indexing\njags_d$Container <- rep(NA, nrow(DataF))\nfor (i in 1:nrow(DataF)) {\n  jags_d$Container[i] <- which(unique(DataF$Container) == DataF$Container[i])\n}\njags_d$ncont <- length(unique(jags_d$Container))\n\n# Convert binary factors to 0's and 1's\njags_d$Habitat <- ifelse(jags_d$Habitat == \"dry\", 0, 1)\njags_d$Treatment <- ifelse(jags_d$Treatment == \"Cont\", 0, 1)\nstr(jags_d)\n\nList of 8\n $ Population: Factor w/ 8 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 2 2 2 2 ...\n $ Habitat   : num [1:55] 0 0 1 1 1 0 1 1 0 1 ...\n $ Treatment : num [1:55] 0 0 0 0 1 0 1 0 1 0 ...\n $ Egg       : int [1:55] 5 5 6 5 7 5 17 9 10 10 ...\n $ nobs      : int 55\n $ npop      : int 8\n $ Container : int [1:55] 1 1 1 1 1 2 2 2 2 2 ...\n $ ncont     : int 10\n\nThen, fitting the intercept model:\n\n\n# intercept model statement:\ncat(\"\nmodel{\n  # priors on precisions (inverse variances)\n  tau.pop ~ dgamma(0.01, 0.01)\n  sd.pop <- sqrt(1/tau.pop)\n  tau.cont ~ dgamma(0.01, 0.01)\n  sd.cont <- sqrt(1/tau.cont)\n  tau.unit ~ dgamma(0.01, 0.01)\n  sd.unit <- sqrt(1/tau.unit)\n  # prior on intercept\n  alpha ~ dnorm(0, 0.01)\n\n  # random effect of container\n  for (i in 1:ncont){\n    cont[i] ~ dnorm(0, tau.cont)\n  }\n\n  # random effect of population\n  for (i in 1:npop){\n    pop[i] ~ dnorm(0, tau.pop)\n  }\n\n  # likelihood\n  for (i in 1:nobs){\n    Egg[i] ~ dpois(mu[i])\n    log(mu[i]) <- cont[Container[i]] + pop[Population[i]] + unit[i]\n    unit[i] ~ dnorm(alpha, tau.unit)\n  }\n}\n    \", fill=T, file=\"pois_intercept.txt\")\n\nnstore <- 2000\nnthin <- 20\nni <- nstore*nthin\n\nint_mod <- jags.model(\"pois_intercept.txt\",\n                      data=jags_d[-c(2, 3)], # exclude unused data\n                      n.chains=3,\n                      n.adapt=5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 55\n   Unobserved stochastic nodes: 77\n   Total graph size: 364\n\nInitializing model\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |+                                                 |   2%\n  |                                                        \n  |++                                                |   4%\n  |                                                        \n  |+++                                               |   6%\n  |                                                        \n  |++++                                              |   8%\n  |                                                        \n  |+++++                                             |  10%\n  |                                                        \n  |++++++                                            |  12%\n  |                                                        \n  |+++++++                                           |  14%\n  |                                                        \n  |++++++++                                          |  16%\n  |                                                        \n  |+++++++++                                         |  18%\n  |                                                        \n  |++++++++++                                        |  20%\n  |                                                        \n  |+++++++++++                                       |  22%\n  |                                                        \n  |++++++++++++                                      |  24%\n  |                                                        \n  |+++++++++++++                                     |  26%\n  |                                                        \n  |++++++++++++++                                    |  28%\n  |                                                        \n  |+++++++++++++++                                   |  30%\n  |                                                        \n  |++++++++++++++++                                  |  32%\n  |                                                        \n  |+++++++++++++++++                                 |  34%\n  |                                                        \n  |++++++++++++++++++                                |  36%\n  |                                                        \n  |+++++++++++++++++++                               |  38%\n  |                                                        \n  |++++++++++++++++++++                              |  40%\n  |                                                        \n  |+++++++++++++++++++++                             |  42%\n  |                                                        \n  |++++++++++++++++++++++                            |  44%\n  |                                                        \n  |+++++++++++++++++++++++                           |  46%\n  |                                                        \n  |++++++++++++++++++++++++                          |  48%\n  |                                                        \n  |+++++++++++++++++++++++++                         |  50%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  52%\n  |                                                        \n  |+++++++++++++++++++++++++++                       |  54%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  56%\n  |                                                        \n  |+++++++++++++++++++++++++++++                     |  58%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  60%\n  |                                                        \n  |+++++++++++++++++++++++++++++++                   |  62%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  64%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++                 |  66%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  68%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++               |  70%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  72%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++             |  74%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  76%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++           |  78%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  80%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++         |  82%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  84%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++       |  86%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  88%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  92%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++   |  94%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++++ |  98%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%\n\nvars <- c(\"sd.pop\", \"sd.cont\", \"sd.unit\", \"alpha\")\nint_out <- coda.samples(int_mod, n.iter=ni, thin=nthin,\n                        variable.names=vars)\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************| 100%\n\nThen, fit the full mixed-model with all fixed and random effects:\n\n\n# covariate model statement:\ncat(\"\nmodel{\n  # priors on precisions (inverse variances)\n  tau.pop ~ dgamma(0.01, 0.01)\n  sd.pop <- sqrt(1/tau.pop)\n  tau.cont ~ dgamma(0.01, 0.01)\n  sd.cont <- sqrt(1/tau.cont)\n  tau.unit ~ dgamma(0.01, 0.01)\n  sd.unit <- sqrt(1/tau.unit)\n  # priors on coefficients\n  alpha ~ dnorm(0, 0.01)\n  beta1 ~ dnorm(0, 0.01)\n  beta2 ~ dnorm(0, 0.01)\n\n  # random effect of container\n  for (i in 1:ncont){\n    cont[i] ~ dnorm(0, tau.cont)\n  }\n\n  # random effect of population\n  for (i in 1:npop){\n    pop[i] ~ dnorm(0, tau.pop)\n  }\n\n  # likelihood\n  for (i in 1:nobs){\n    Egg[i] ~ dpois(mu[i])\n    log(mu[i]) <- cont[Container[i]] + pop[Population[i]] + unit[i]\n    mu_f[i] <- alpha + beta1 * Treatment[i] + beta2 * Habitat[i]\n    unit[i] ~ dnorm(mu_f[i], tau.unit)\n  }\n}\n    \", fill=T, file=\"pois_cov.txt\")\n\ncov_mod <- jags.model(\"pois_cov.txt\",\n                      data=jags_d,\n                      n.chains=3,\n                      n.adapt=5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 55\n   Unobserved stochastic nodes: 79\n   Total graph size: 484\n\nInitializing model\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |+                                                 |   2%\n  |                                                        \n  |++                                                |   4%\n  |                                                        \n  |+++                                               |   6%\n  |                                                        \n  |++++                                              |   8%\n  |                                                        \n  |+++++                                             |  10%\n  |                                                        \n  |++++++                                            |  12%\n  |                                                        \n  |+++++++                                           |  14%\n  |                                                        \n  |++++++++                                          |  16%\n  |                                                        \n  |+++++++++                                         |  18%\n  |                                                        \n  |++++++++++                                        |  20%\n  |                                                        \n  |+++++++++++                                       |  22%\n  |                                                        \n  |++++++++++++                                      |  24%\n  |                                                        \n  |+++++++++++++                                     |  26%\n  |                                                        \n  |++++++++++++++                                    |  28%\n  |                                                        \n  |+++++++++++++++                                   |  30%\n  |                                                        \n  |++++++++++++++++                                  |  32%\n  |                                                        \n  |+++++++++++++++++                                 |  34%\n  |                                                        \n  |++++++++++++++++++                                |  36%\n  |                                                        \n  |+++++++++++++++++++                               |  38%\n  |                                                        \n  |++++++++++++++++++++                              |  40%\n  |                                                        \n  |+++++++++++++++++++++                             |  42%\n  |                                                        \n  |++++++++++++++++++++++                            |  44%\n  |                                                        \n  |+++++++++++++++++++++++                           |  46%\n  |                                                        \n  |++++++++++++++++++++++++                          |  48%\n  |                                                        \n  |+++++++++++++++++++++++++                         |  50%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  52%\n  |                                                        \n  |+++++++++++++++++++++++++++                       |  54%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  56%\n  |                                                        \n  |+++++++++++++++++++++++++++++                     |  58%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  60%\n  |                                                        \n  |+++++++++++++++++++++++++++++++                   |  62%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  64%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++                 |  66%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  68%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++               |  70%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  72%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++             |  74%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  76%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++           |  78%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  80%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++         |  82%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  84%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++       |  86%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  88%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  92%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++   |  94%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++++ |  98%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%\n\nvars2 <- c(\"sd.pop\", \"sd.cont\", \"sd.unit\", \"alpha\", \"beta1\", \"beta2\")\ncov_out <- coda.samples(cov_mod, n.iter=ni, thin=nthin,\n                        variable.names=vars2)\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************| 100%\n\nFor every MCMC draw, we can calculate \\(R^2_{GLMM}\\), generating posteriors for both the marginal and conditional values.\n\n\n# Step 1: variance in expected values (using fixed effects only)\nd_int <- ggs(int_out)\nd_cov <- ggs(cov_out)\n\nalpha_cov <- subset(d_cov, Parameter == \"alpha\")$value\nalpha_int <- subset(d_int, Parameter == \"alpha\")$value\nb1_cov <- subset(d_cov, Parameter == \"beta1\")$value\nb2_cov <- subset(d_cov, Parameter == \"beta2\")$value\n\nXmat <- cbind(rep(1, jags_d$nobs), jags_d$Treatment, jags_d$Habitat)\nbeta_mat <- cbind(alpha_cov, b1_cov, b2_cov)\n\nfixed_expect <- array(dim = c(nstore, jags_d$nobs))\nvarF <- rep(NA, nstore)\nfor (i in 1:nstore) {\n    fixed_expect[i, ] <- beta_mat[i, ] %*% t(Xmat)\n    varF[i] <- var(fixed_expect[i, ])\n}\n\n# Step 2: calculate remaining variance components\n# among container variance\nvarCont <- subset(d_cov, Parameter == \"sd.cont\")$value^2\n# among population variance\nvarPop <- subset(d_cov, Parameter == \"sd.pop\")$value^2\n# overdispersion variance\nvarUnit <- subset(d_cov, Parameter == \"sd.unit\")$value^2\n# distribution variance (Table 2, Nakagawa & Schielzeth 2013)\nvarDist <- log(1/exp(alpha_int) + 1)\n\n# Finally, calculate posterior R-squared values\n# marginal\npostR2m <- varF/(varF + varCont + varPop + varUnit + varDist)\n# conditional\npostR2c <- (varF + varCont + varPop)/\n             (varF + varCont + varPop + varUnit + varDist)\n\n# compare posterior R-squared values to point estimates\npar(mfrow = c(1, 2))\nhist(postR2m, main = \"Marginal R-squared\",\n        ylab = \"Posterior density\",\n        xlab = NULL, breaks = 20)\nabline(v = R2m, col = \"blue\", lwd = 4)\nhist(postR2c, main = \"Conditional R-squared\",\n        ylab = \"Posterior density\",\n        xlab = NULL, breaks = 25)\nabline(v = R2c, col = \"blue\", lwd = 4)\n\n\nThis plot shows the posterior \\(R^2_{GLMM}\\) distributions for both the marginal and conditional cases, with the point estimates generated with glmer shown as vertical blue lines. Personally, I find it to be a bit more informative and intuitive to think of \\(R^2\\) as a probability distribution that integrates uncertainty in its component parameters. That said, it is unconventional to represent \\(R^2\\) in this way, which could compromise the ease with which this handy statistic can be explained to the uninitiated (e.g. first year biology undergraduates). But, being a derived parameter, those wishing to generate a posterior can do so relatively easily.\n\n\nNakagawa, Shinichi, and Holger Schielzeth. 2010. “Repeatability for Gaussian and Non-Gaussian Data: A Practical Guide for Biologists.” Biological Reviews 85 (4): 935–56. https://doi.org/10.1111/j.1469-185X.2010.00141.x.\n\n\n———. 2013. “A General and Simple Method for Obtaining R2 from Generalized Linear Mixed-Effects Models.” Methods in Ecology and Evolution 4 (2): 133–42. https://doi.org/10.1111/j.2041-210x.2012.00261.x.\n\n\n\n\n",
    "preview": "posts/2018-12-24-quantifying-uncertainty-around-r-squared-for-generalized-linear-models/quantifying-uncertainty-around-r-squared-for-generalized-linear-models_files/figure-html5/process-posterior-draws-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-24-clarifying-vague-interactions/",
    "title": "Clarifying vague interactions",
    "description": "One quick way to improve reporting of interaction effects in linear models.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-08-18",
    "categories": [
      "visualization",
      "rants"
    ],
    "contents": "\nIt is easy to present linear model results with vague or unintelligible interaction effects. One way to be vague when presenting interaction effects is to provide only a table of model coefficients, including no information on the range of covariate values observed, and no plots to aid in interpretation. Here’s an example:\nSuppose you have discovered a statistically significant interaction effect between two continous covariates in the context of a linear model.\n\\[ y_i \\sim \\text{Normal}(\\mu_i, \\sigma^2) \\]\n\\[ \\mu_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{1i} x_{2i} \\]\nSuppose also that you have decided to present the model results with the following table, and the reviewers requested no additional information:\n\nEstimate\nSE\nP-value\n\\(\\beta_0\\)\n-0.004\n0.037\n0.921\n\\(\\beta_1\\)\n1.055\n0.038\n<0.05\n\\(\\beta_2\\)\n-0.496\n0.037\n<0.05\n\\(\\beta_3\\)\n2.002\n0.040\n<0.05\nRSE\n0.517\n\n\n\n\n\n\nWithout knowing the range of covariate values observed, this table gives an incomplete story about relationship between the covariates and the response variable. Assuming the reader has a decent guess about the range of possible values for the covariates, this is what they can piece together:\n\n\n# parameter estimates\nbeta0 <- -.004\nbeta1 <- 1.055\nbeta2 <- -.496\nbeta3 <- 2.002\n\n# reader's guess: range of possible covariate values\nx1 <- seq(-5, 5, .1)\nx2 <- seq(-5, 5, .1)\nX <- expand.grid(x1=x1, x2=x2)\n\n# reader's attempt to know how the covariates relate to E(y)\nmu <- with(X, beta0 + beta1*x1 + beta2*x2 + beta3*x1*x2)\n\nrequire(ggplot2)\nd <- data.frame(mu=mu, x1=X$x1, x2=X$x2)\np1 <- ggplot(d, aes(x1, x2, z=mu)) + theme_bw() +\n  geom_tile(aes(fill=mu)) +\n  stat_contour(binwidth=1.5, color = 'black', alpha = .3) +\n  scale_fill_gradient2(low=\"blue\", mid=\"white\", high=\"orange\") +\n  xlab(\"Covariate 1\") + ylab(\"Covariate 2\") +\n  ggtitle(\"Contour plot of the linear predictor\")\np1\n\n\nIf the reader does not know where the observations fell in this plot, it is difficult to know whether the response variable was increasing or decreasing with each covariate across the range of observed values.\nConsider the following two cases, where the observed covariate combinations are included as points.\n\n\nn_point <- 100\nset.seed(1234)\np1 + \n  geom_point(data = data.frame(x1 = rnorm(n_point), \n                               x2 = rnorm(n_point)), \n             aes(x1, x2), inherit.aes = FALSE)\n\n\n\n\nset.seed(1234)\np1 + \n  geom_point(data = data.frame(x1 = runif(n_point, max = 5), \n                               x2 = runif(n_point, max = 5)), \n             aes(x1, x2), inherit.aes = FALSE)\n\n\nThese two plots tell different stories despite identical model parameters. In the second case, across the range of observed covariates, the expected value of \\(y\\) increases as either covariate increases and the interaction term affects the magnitude this increase. In the first case, increases in covariate 1 or 2 could increase or decrease \\(\\mu\\), depending on the value of the other covariate.\nI won’t get into the nitty gritty of how to present interaction effects (but if you’re interested, there are articles out there (Lamina et al. 2012). My main goal here is to point out the ambiguity associated with only presenting a table of parameter estimates. My preference would be that authors at least present observed covariate ranges (or better yet values), and provide a plot that illustrates the interaction.\n\n\nLamina, Claudia, Gisela Sturm, Barbara Kollerits, and Florian Kronenberg. 2012. “Visualizing Interaction Effects: A Proposal for Presentation and Interpretation.” Journal of Clinical Epidemiology 65 (8): 855–62. https://doi.org/10.1016/j.jclinepi.2012.02.013.\n\n\n\n\n",
    "preview": "posts/2018-12-24-clarifying-vague-interactions/clarifying-vague-interactions_files/figure-html5/simulate-data-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-23-split-violin-plots/",
    "title": "Split violin plots",
    "description": "Comparing distributions with split violin plots in R.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-06-24",
    "categories": [
      "visualization"
    ],
    "contents": "\nViolin plots are useful for comparing distributions. When data are grouped by a factor with two levels (e.g. males and females), you can split the violins in half to see the difference between groups. Consider a 2 x 2 factorial experiment: treatments A and B are crossed with groups 1 and 2, with N=1000.\n\n\n\nBoxplots are often used:\n\n\n\nThis gives us a rough comparison of the distribution in each group, but sometimes it’s nice to visualize the kernel density estimates instead.\nI recently ran into this issue and tweaked the vioplot() function from the vioplot package by Daniel Adler to make split violin plots. With vioplot2(), the side argument specifies whether to plot the density on “both”, the “left”, or the “right” side.\n\n\n\nLast but not least, Peter Kampstra’s beanplot package uses beanplot() to make split density plots, but 1) plots a rug rather than a quantile box, 2) includes a line for the overall mean or median, and 3) makes it easier to change the kernel function.\n\n\n\nThere are more ways than one to skin a cat, and what one uses will probably come to personal preference.\n\n\n",
    "preview": "posts/2018-12-23-split-violin-plots/split-violin-plots_files/figure-html5/beanplots-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-23-bayesian-model-ii-regression/",
    "title": "Bayesian model II regression in JAGS",
    "description": "Fitting a regression model with uncertainty in the explanatory variable.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-05-27",
    "categories": [
      "jags"
    ],
    "contents": "\nRegression is a mainstay of ecological and evolutionary data analysis. For example, a disease ecologist may use body size (e.g. a weight from a scale with measurement error) to predict infection. Classical linear regression assumes no error in covariates; they are known exactly. This is rarely the case in ecology, and ignoring error in covariates can bias regression coefficient estimates. This is where model II (aka errors-in variables and measurement errors) regression models come in handy. Here I’ll demonstrate how to construct such a model in a Bayesian framework, where substantive prior knowledge of covariate error facilitates less-biased parameter estimates.\nHere’s a quick illustration of the problem: I’ll generate data from a known simple linear regression model, and fit models that ignore or incorporate error in the covariate.\n\n\nlibrary(rjags)\nlibrary(ggmcmc)\n\n# simulate covariate data\nn <- 40\nsdx <- 6\nsdobs <- 5\ntaux <- 1 / (sdobs * sdobs)\ntruex <- rnorm(n, 0, sdx)\nerrorx <- rnorm(n, 0, sdobs)\nobsx <- truex + errorx\n\n# simulate response data\nalpha <- 0\nbeta <- 10\nsdy <- 20\nerrory <- rnorm(n, 0, sdy)\nobsy <- alpha + beta*truex + errory\nobserved_data <- data.frame(obsx = obsx, obsy = obsy)\nparms <- data.frame(alpha, beta)\n\nIgnoring error in the covariate:\n\n\n# bundle data\njags_d <- list(x = obsx, y = obsy, n = length(obsx))\n\n# write model\ncat(\"\n    model{\n## Priors\nalpha ~ dnorm(0, .001)\nbeta ~ dnorm(0, .001)\nsdy ~ dunif(0, 100)\ntauy <- 1 / (sdy * sdy)\n\n## Likelihood\n  for (i in 1:n){\n    mu[i] <- alpha + beta * x[i]\n    y[i] ~ dnorm(mu[i], tauy)\n  }\n}\n\",\n    fill=TRUE, file=\"yerror.txt\")\n\n# initiate model\nmod1 <- jags.model(\"yerror.txt\", data=jags_d,\n                   n.chains=3, n.adapt=1000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 40\n   Unobserved stochastic nodes: 3\n   Total graph size: 170\n\nInitializing model\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |+                                                 |   2%\n  |                                                        \n  |++                                                |   4%\n  |                                                        \n  |+++                                               |   6%\n  |                                                        \n  |++++                                              |   8%\n  |                                                        \n  |+++++                                             |  10%\n  |                                                        \n  |++++++                                            |  12%\n  |                                                        \n  |+++++++                                           |  14%\n  |                                                        \n  |++++++++                                          |  16%\n  |                                                        \n  |+++++++++                                         |  18%\n  |                                                        \n  |++++++++++                                        |  20%\n  |                                                        \n  |+++++++++++                                       |  22%\n  |                                                        \n  |++++++++++++                                      |  24%\n  |                                                        \n  |+++++++++++++                                     |  26%\n  |                                                        \n  |++++++++++++++                                    |  28%\n  |                                                        \n  |+++++++++++++++                                   |  30%\n  |                                                        \n  |++++++++++++++++                                  |  32%\n  |                                                        \n  |+++++++++++++++++                                 |  34%\n  |                                                        \n  |++++++++++++++++++                                |  36%\n  |                                                        \n  |+++++++++++++++++++                               |  38%\n  |                                                        \n  |++++++++++++++++++++                              |  40%\n  |                                                        \n  |+++++++++++++++++++++                             |  42%\n  |                                                        \n  |++++++++++++++++++++++                            |  44%\n  |                                                        \n  |+++++++++++++++++++++++                           |  46%\n  |                                                        \n  |++++++++++++++++++++++++                          |  48%\n  |                                                        \n  |+++++++++++++++++++++++++                         |  50%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  52%\n  |                                                        \n  |+++++++++++++++++++++++++++                       |  54%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  56%\n  |                                                        \n  |+++++++++++++++++++++++++++++                     |  58%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  60%\n  |                                                        \n  |+++++++++++++++++++++++++++++++                   |  62%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  64%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++                 |  66%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  68%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++               |  70%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  72%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++             |  74%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  76%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++           |  78%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  80%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++         |  82%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  84%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++       |  86%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  88%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  92%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++   |  94%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++++ |  98%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%\n\n# simulate posterior\nout <- coda.samples(mod1, n.iter=1000, thin=1,\n                    variable.names=c(\"alpha\", \"beta\", \"sdy\"))\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************| 100%\n\n# store parameter estimates\nggd <- ggs(out)\na <- ggd$value[which(ggd$Parameter == \"alpha\")]\nb <- ggd$value[which(ggd$Parameter == \"beta\")]\nd <- data.frame(a, b)\n\nIncorporating error in the covariate: I’m assuming that we have substantive knowledge about covariate measurement represented in the prior for the precision in X. Further, the prior for the true X values reflects knowledge of the distribution of our X value in the population from which the sample was taken.\n\n\n# specify model\ncat(\"\n    model {\n## Priors\nalpha ~ dnorm(0, .001)\nbeta ~ dnorm(0, .001)\nsdy ~ dunif(0, 100)\ntauy <- 1 / (sdy * sdy)\ntaux ~ dunif(.03, .05)\n\n## Likelihood\n  for (i in 1:n){\n    truex[i] ~ dnorm(0, .04)\n    x[i] ~ dnorm(truex[i], taux)\n    y[i] ~ dnorm(mu[i], tauy)\n    mu[i] <- alpha + beta * truex[i]\n  }\n}\n    \", fill=T, file=\"xyerror.txt\")\n\n# bundle data\njags_d <- list(x = obsx, y = obsy, n = length(obsx))\n\n# initiate model\nmod2 <- jags.model(\"xyerror.txt\", data=jags_d,\n                   n.chains=3, n.adapt=1000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 80\n   Unobserved stochastic nodes: 44\n   Total graph size: 214\n\nInitializing model\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |+                                                 |   2%\n  |                                                        \n  |++                                                |   4%\n  |                                                        \n  |+++                                               |   6%\n  |                                                        \n  |++++                                              |   8%\n  |                                                        \n  |+++++                                             |  10%\n  |                                                        \n  |++++++                                            |  12%\n  |                                                        \n  |+++++++                                           |  14%\n  |                                                        \n  |++++++++                                          |  16%\n  |                                                        \n  |+++++++++                                         |  18%\n  |                                                        \n  |++++++++++                                        |  20%\n  |                                                        \n  |+++++++++++                                       |  22%\n  |                                                        \n  |++++++++++++                                      |  24%\n  |                                                        \n  |+++++++++++++                                     |  26%\n  |                                                        \n  |++++++++++++++                                    |  28%\n  |                                                        \n  |+++++++++++++++                                   |  30%\n  |                                                        \n  |++++++++++++++++                                  |  32%\n  |                                                        \n  |+++++++++++++++++                                 |  34%\n  |                                                        \n  |++++++++++++++++++                                |  36%\n  |                                                        \n  |+++++++++++++++++++                               |  38%\n  |                                                        \n  |++++++++++++++++++++                              |  40%\n  |                                                        \n  |+++++++++++++++++++++                             |  42%\n  |                                                        \n  |++++++++++++++++++++++                            |  44%\n  |                                                        \n  |+++++++++++++++++++++++                           |  46%\n  |                                                        \n  |++++++++++++++++++++++++                          |  48%\n  |                                                        \n  |+++++++++++++++++++++++++                         |  50%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  52%\n  |                                                        \n  |+++++++++++++++++++++++++++                       |  54%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  56%\n  |                                                        \n  |+++++++++++++++++++++++++++++                     |  58%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  60%\n  |                                                        \n  |+++++++++++++++++++++++++++++++                   |  62%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  64%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++                 |  66%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  68%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++               |  70%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  72%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++             |  74%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  76%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++           |  78%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  80%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++         |  82%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  84%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++       |  86%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  88%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  92%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++   |  94%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++++ |  98%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%\n\n# simulate posterior\nout <- coda.samples(mod2, n.iter=30000, thin=30,\n                    variable.names=c(\"alpha\", \"beta\", \"tauy\", \"taux\"))\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************| 100%\n\n# store parameter estimates\nggd <- ggs(out)\na2 <- ggd$value[which(ggd$Parameter == \"alpha\")]\nb2 <- ggd$value[which(ggd$Parameter == \"beta\")]\nd2 <- data.frame(a2, b2)\n\nNow let’s see how the two models perform.\n\n\nggplot(observed_data, aes(x=obsx, obsy)) +\n  geom_abline(aes(intercept=a, slope=b), data=d, \n              color=\"red\", alpha=0.05) +\n  geom_abline(aes(intercept=a2, slope=b2), data=d2, \n              color=\"dodgerblue\", alpha=0.05) +\n  geom_abline(aes(intercept=alpha, slope=beta),\n              data=parms, color=\"green\", size=1.5, linetype=\"dashed\") +\n  geom_point(shape=19, size=3) +\n  theme_minimal() +\n  xlab(\"X values\") + \n  ylab(\"Observed Y values\") +\n  ggtitle(\"Model results with and without modeling error in X\")\n\n\nThe dashed green line shows the model that generated the data, i.e. the “true” line. The red lines show the posterior for the naive model ignoring error in X, while the less-biased blue lines show the posterior for the model incorporating error in X.\n\n\n",
    "preview": "posts/2018-12-23-bayesian-model-ii-regression/bayesian-model-ii-regression_files/figure-html5/compare-models-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-23-modeling-habitat-diversity-and-species-richness/",
    "title": "Modeling habitat diversity and species richness",
    "description": "Experimenting with an agent based model of habitat diversity and species richness in R.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-04-20",
    "categories": [],
    "contents": "\nHow does habitat diversity affect species richness? Perhaps intuition suggests that habitat diversity increases species richness by facilitating niche or resource partitioning among species. But, for a fixed area, as habitat heterogeneity increases, the area that can be allocated to each habitat type decreases. A recent paper provides a theoretical and empirical treatment of the habitat area-heterogeneity trade-off’s consequences for species richness (Allouche et al. 2012). Both treatments of the subject indicated that the relationship between habitat heterogeneity and species richness may be unimodal, rather than strictly increasing.\nConceptually, this is expected to occur when on the left side of the curve, increasing habitat heterogeneity opens up new regions in niche space, facilitating colonization by new species. However, as heterogeneity continues to increase, each species has fewer habitat patches to utilize, population sizes decrease, and local extinction risk increases due to demographic stochasticity.\nTo explore this idea theoretically, Allouche et al. (2012) developed an individually based model using a continuous time Markov process. The details of their modeling approach can be found in the supplement to their article, which I recommend. In this post, I’ll demonstrate how to implement a discrete time version of their model in R. Thanks to the agent-based modeling working group at the University of Colorado for providing motivation to code up model in R.\nModel structure\nThis model is spatially implicit, with \\(A\\) equally connected sites. Each site falls on an environmental condition axis, receiving some value \\(E\\) that characterizes local conditions. The environmental conditions for each site are uniformly distributed between two values that dictate the range of environmental conditions in a focal area. The local range of environmental conditions is a subset of some global range. There are \\(N\\) species in the regional pool that can colonize habitat patches. Each species has some environmental optimum \\(\\mu_i\\), and some niche width \\(\\sigma_i\\), which together define a Gaussian function for the probability of establishment given a colonization attempt and a habitat patch environmental condition \\(E\\).\nIt is assumed that all individuals that occupy a patch have the same per-timestep probabilities of death and reproduction. If an individual reproduces, the number of offspring it produces is a Poisson distributed random variable, and each individual offspring attempts to colonize one randomly selected site. At each time-step, every site has an equal probability of a colonization attempt by an individual from each species in the regional pool. Every habitat patch holds only one individual.\nOffspring and immigrants from the regional pool do not displace individuals from habitat patches when they attempt to colonize. In empty sites, offspring receive colonization priority, with regional colonization occurring after breeding. When multiple offspring or immigrants from the regional pool could establish in an empty site, one successful individual is randomly chosen to establish regardless of species identity.\nParameters\nThe following parameters are supplied to the function alloucheIBM():\nA = number of sites; N = number of species in the regional pool; ERmin = global environmental conditions minimum; ERmax = global environmental conditions maximum; Emin = local environmental minimum; Emax = local environmental maximum; sig = niche width standard deviation for all species; pM = per timestep probability of mortality; pR = per timestep probability of reproduction; R = per capita expected number of offspring; and I = per timestep probability of attempted colonization by an immigrant from the regional pool for each patch.\nImplementation in R\nThe function alloucheIBM() does the majority of work for this model:\n\n\n\nThe function returns a list containing a vector of species richness at each timestep, the proportion of sites occupied at each timestep, a state array containing all occupancy information for each patch, species, and timestep, and lastly a dataframe containing information on the niches of each species in the regional pool.\nUsing this function we can simulate richness through time:\n\n\n\nFinally, we can address the issue of habitat heterogeneity and its effect on species richness. There are many ways to approach this issue, and many parameter combinations to consider. Allouche et al. (2012) provides a thorough treatment of the subject; I’ll demonstrate just one result: that under certain conditions, species richness peaks at intermediate levels of habitat heterogeneity.\nTo construct a range of habitat heterogeneity values, let’s construct an interval and take subsequently narrower intervals centered around the middle of the original interval.\n\n\n\nNow, for each interval, we can iteratively run the model and track species richness. Because species richness tends to vary through time, let’s take the mean of the final 100 timesteps as a measure of species richness for each model run, and record the standard deviation to track variability.\n\n\n\nOf course, the shape of this relationship is sensitive to the parameters. As an example, changing niche width to increase or decrease niche overlap will mediate the strength of interspecific competition for space. Also, increasing reproductive rates may buffer each species from stochastic extinction so that the relationship between environmental heterogeneity and richness is monotonically increasing. Furthermore, here I centered all intervals around the same value, but the exact position of the environmental heterogeneity interval will affect the net establishment probability for each site, depending on how the interval relates to species niches. The parameter space is yours to explore.\nThese types of stochastic simulation models are fairly straightforward to implement in R. Indeed there’s a package dedicated to facilitating the implementation of such models: simecol. There’s even a book (Soetaert and Herman 2008).\n\n\nAllouche, Omri, Michael Kalyuzhny, Gregorio Moreno-Rueda, Manuel Pizarro, and Ronen Kadmon. 2012. “Area–Heterogeneity Tradeoff and the Diversity of Ecological Communities.” Proceedings of the National Academy of Sciences 109 (43): 17495–17500. https://doi.org/10.1073/pnas.1208652109.\n\n\nSoetaert, Karline, and Peter MJ Herman. 2008. A Practical Guide to Ecological Modelling: Using R as a Simulation Platform. Springer Science & Business Media.\n\n\n\n\n",
    "preview": "posts/2018-12-23-modeling-habitat-diversity-and-species-richness/modeling-habitat-diversity-and-species-richness_files/figure-html5/run-model-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-22-interactive-two-host-sir-model/",
    "title": "Interactive two host SIR model",
    "description": "Creating an interactive two host SIR model in R and shiny.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-02-20",
    "categories": [
      "shiny",
      "teaching"
    ],
    "contents": "\nThis is an example of interfacing R, shiny, and deSolve to produce an interactive environment where users can explore model behavior by altering parameters in an easy to use GUI.\nThe model tracks the number of susceptible, infectious, and recovered individuals in two co-occuring host species. The rates of change for each class are represented as a system of differential equations:\n\\[\\dot{S_1} = (b_1 - \\Delta_1N_1)N_1 - d_1S_1 - S_1 (\\beta_{11} I_1 + \\beta_{12} I_2)\\]\n\\[\\dot{I_1} =  S_1 (\\beta_{11} I_1 + \\beta_{12} I_2) - (d_1 + \\alpha_1 + \\sigma_1)I_1\\]\n\\[\\dot{R_1} = \\sigma_1 I_1 - d_1R_1\\]\n\\[\\dot{S_2} = (b_2 - \\Delta_2N_2)N_2 - d_2S_2 - S_2 (\\beta_{22} I_2 + \\beta_{21} I_1)\\]\n\\[\\dot{I_2} = S_2 (\\beta_{22} I_2 + \\beta_{21} I_1) - (d_2 + \\alpha_2 + \\sigma_2)I_2\\]\n\\[\\dot{R_2} = \\sigma_2 I_2 - d_2R_2\\]\nWhere \\(S_i\\), \\(I_i\\), and \\(R_i\\) represent the density of susceptible, infectious, and recovered individuals respectively of species \\(i\\). The total number of individuals of each species is \\(N_i\\). Per capita birth and death rates are represented by \\(b_i\\) and \\(d_i\\), and the strength of density dependence in population growth is \\(\\delta_i\\). Transmission rates from species \\(j\\) to species \\(i\\) are given by \\(\\beta_{ij}\\). The pathogen imposes additional mortality for infected individuals at rate \\(\\alpha_{i}\\), and infected individuals recover at rate \\(\\sigma_{i}\\) so that the average infectious period is \\(\\frac{1}{\\sigma_{i}}\\). Here, it is assumed that the pathogen does not castrate its hosts. Thus, susceptible, infectious, and recovered individuals reproduce at the same rate.\nEpidemiological models often differentiate between two transmission dynamics. With density-dependent transmission, the number of host contacts and transmission events increases with the density of individuals (as shown in the above system of equations). In contrast, with frequency-dependent transmission, hosts have a constant contact rate so that the transmission rate depends on the relative proportion of infectious individuals. As an example, models of sexually transmitted infections often assume frequency dependent transmission, implying that the number of sexual partners one has is independent of population density. To incorporate frequency dependent transmission into the above model, it is necessary to divide the transmission term \\(S\\sum{(\\beta I)}\\) by \\(N\\).\nBased on this system of equations, a criterion for pathogen invasion called \\(R_0\\) can be derived based on the dominant eigenvalue of the next generation matrix (Dobson 2004). If \\(R_0 < 1\\), the pathogen does not invade; if \\(R_0>1\\), the pathogen invades.\nBuilding the R shiny app\nShiny requires two files to run: a file containing all of the calculations, plotting functionality, etc., and a file defining a user interface.\nHere is the file defining what you want the server to do. Note the use of ifelse() to have either density- or frequency-dependent transmission.\nHere is the file defining the user interface.\nHere is a link to the resulting app.\n\n\n\n\n\n",
    "preview": "posts/2018-12-22-interactive-two-host-sir-model/interactive-two-host-sir-model_files/figure-html5/plot-img-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-20-interactive-stage-structured-population-model/",
    "title": "Interactive stage-structured population model",
    "description": "Building an interactive stage-structured population model in R with shiny.",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-02-16",
    "categories": [
      "teaching",
      "shiny"
    ],
    "contents": "\nThis is an example of interfacing R and shiny to allow users to explore a biological model often encountered in an introductory ecology class. We are interested the growth of a population that is composed of multiple, discrete stages or age classes. Patrick H. Leslie provides an in-depth derivation of the model in a 1945 paper (Leslie 1945).\nThe population at time \\(t\\) is represented by a vector \\(\\bar{x}_t\\), where each element of the vector represents the number of individuals in each age class (e.g. if a population has \\(n\\) age classes, then \\(\\bar{x}_t\\) has \\(n\\) elements). Time is considered discrete and we assume that the population is censused prior to breeding. We assume that individuals within each age class are identical, and that each has some probability of maturing to the next age class, surviving (staying in the same age class), and reproduction. Changes in the population from one timestep to another are represented as:\n\\[\\bar{x}_{t+1}=L\\bar{x}_t\\]\nwhere \\(L\\) is an \\(n\\) x \\(n\\) Leslie matrix (or more generally, a projection matrix) that describes the contribution of each age class to the population at time \\(t+1\\).\nSuppose we are tasked with modeling the annual dynamics of a population with four age classes, and \\(t\\) represents years. For simplicity, we model only females and assume that plenty of males are available for breeding. Individuals in the first age class survive to class 2 with probability 0.1, class 2 individuals survive to class 3 with probability 0.5, class 3 individuals survive to class 4 with probability 0.9, and class four individuals survive each year with probability 0.7. Only the fourth age class is reproductive, with individuals producing 100 class 1 individuals per year.\nEquivalently, as a Leslie matrix:\n\\[\\left[\\begin{array}{rrrr}\n    0 & 0 & 0 & 100 \\\\\n    .1 & 0 & 0 & 0 \\\\\n    0 & .5 & 0 & 0 \\\\\n    0 & 0 & .9 & .7\n  \\end{array}\\right]\\]\nThe long term population growth rate is related to the dominant eigenvalue \\(\\lambda_{1}\\) of \\(L\\). If \\[\\lambda_{1} < 1\\], the population declines to extinction, and if \\[\\lambda_{1} > 1\\] the population increases.\nFrom a management perspective, it is often useful to know how limited resources may be allocated to increase population growth or prevent extinction. In other words, if an element \\[l_{ij}\\] such as fecundity or survival could be manipulated by managers, how much would the long term population growth rate change? To this end, one can calculate the sensitivity of the dominant eigenvalue to small changes in \\(l_{ij}\\):\n\\[\n\\frac{\\partial \\lambda_{1}}{\\partial l_{ij}} = \\frac{(w_{1}){i}(v_{1})_{j}}{\\bar{w}_{1}^{T} \\bar{v}_{1}}\n\\]\nwhere \\(w_1\\) and \\(v_1\\) are left and right eigenvectors, respectively, associated with the dominant eigenvalue. Because survival and fecundity are on different scales, sensitivity is often scaled by a factor of \\(\\frac{L_{ij}}{\\lambda_1}\\) for a measure of elasticity.\nThe shiny app\nFiles are accessible in this repository. Please feel free to clone for your own use and/or contribute.\nHere is a link to the resulting app.\n\n\n\n\n\nLeslie, Patrick H. 1945. “On the Use of Matrices in Certain Population Mathematics.” Biometrika, 183–212. http://dx.doi.org/10.2307/2332297.\n\n\n\n\n",
    "preview": "posts/2018-12-20-interactive-stage-structured-population-model/interactive-stage-structured-population-model_files/figure-html5/plot-img-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-22-dynamic-community-occupancy-model-jags/",
    "title": "Dynamic community occupancy modeling with R and JAGS",
    "description": "Multi-species, multi-timestep occupancy model in R and JAGS",
    "author": [
      {
        "name": "Maxwell B. Joseph",
        "url": {}
      }
    ],
    "date": "2013-02-04",
    "categories": [
      "jags"
    ],
    "contents": "\nThis post is intended to provide a simple example of how to construct and make inferences on a multi-species multi-year occupancy model using R, JAGS, and the ‘rjags’ package. This is not intended to be a standalone tutorial on dynamic community occupancy modeling (MacKenzie et al. 2002; Royle and Kéry 2007; Kéry and Royle 2008; Dorazio et al. 2010). Royle and Dorazio’s Hierarchichal Modeling and Inference in Ecology also provides a clear explanation of simple one species occupancy models, multispecies occupancy models, and dynamic (multiyear) occupancy models, among other things (Royle and Dorazio 2008). There’s also a wealth of code provided here by Elise Zipkin, J. Andrew Royle, and others.\nBefore getting started, we can define two convenience functions:\n\n\n\nThen, initializing the number of sites, species, years, and repeat surveys (i.e. surveys within years, where the occupancy status of a site is assumed to be constant),\n\n\n\nwe can begin to consider occupancy. We’re interested in making inferences about the rates of colonization and population persistence for each species in a community, while estimating and accounting for imperfect detection.\nOccupancy status at site \\(j\\), by species \\(i\\), in year \\(t\\) is represented by \\(z(j,i,t)\\). For occupied sites \\(z=1\\); for unoccupied sites \\(z=0\\). However, \\(Z\\) is incompletely observed: it is possible that a species \\(i\\) is present at a site \\(j\\) in some year \\(t\\) (\\(z(j,i,t)=1\\)) but species \\(i\\) was never seen at at site \\(j\\) in year \\(t\\) across all \\(k\\) repeat surveys because of imperfect detection. These observations are represented by \\(x(j,i,t,k)\\). Here we assume that there are no “false positive” observations. In other words, if \\(\\sum_{1}^{k}x(j,i,t,k)>0\\) , then \\(z(j,i,t)=1\\). If a site is occupied, the probability that \\(x(j,i,t,k)=1\\) is represented as a Bernoulli trial with probability of detection \\(p(j,i,t,k)\\), such that\n\\[\nx(j,i,t,k) \\sim \\text{Bernoulli}(z(j,i,t)p(j,i,t,k))\n\\]\nThe occupancy status \\(z\\) of species \\(i\\) at site \\(j\\) in year \\(t\\) is modeled as a Markov Bernoulli trial. In other words whether a species is present at a site in year \\(t\\) is influenced by whether it was present at year \\(t−1\\).\n\\[\nz(j,i,t) \\sim \\text{Bernoulli}(\\psi(j,i,t))\n\\]\nwhere for \\(t>1\\)\n\\[\n\\text{logit}(\\psi_{j,i,t})=\\beta_i + \\rho_i z(i, j, t-1)\n\\]\nand in year one \\((t=1)\\)\n\\[\n\\text{logit}(\\psi_{j,i,1})=\\beta_i + \\rho_i z_0(i, j)\n\\]\nwhere the occupancy status in year 0, \\(z_0(i,j) \\sim \\text{Bernoulli}(\\rho_{0i})\\), and \\(\\rho_{0i} \\sim \\text{Uniform}(0,1)\\). \\(\\beta_i\\) and \\(\\rho_i\\) are parameters that control the probabilities of colonization and persistence. If a site was unoccupied by species \\(i\\) in a previous year \\(z(i,j,t−1)=0\\), then the probability of colonization is given by the antilogit of \\(\\beta_i\\). If a site was previously occupied \\(z(i,j,t−1)=1\\), the probability of population persistence is given by the anitlogit of \\(\\beta_i + \\rho_i\\). We assume that the distributions of species specific parameters are defined by community level hyperparameters such that \\(\\beta_i \\sim \\text{Normal}(\\mu_\\beta, \\sigma_\\beta)\\) and \\(rho_i \\sim \\text{Normal}(\\mu_\\rho, \\sigma_\\rho)\\). We can generate occupancy data as follows:\n\n\n\nFor simplicity, we’ll assume that there are no differences in species detectability among sites, years, or repeat surveys, but that detectability varies among species. We’ll again use hyperparameters to specify a distribution of detection probabilities in our community, such that \\(\\text{logit}(p_i) \\sim \\text{Normal}(\\mu_p, \\sigma_p)\\).\n\n\n\nWe can now generate our observations based on occupancy states and detection probabilities. Although this could be vectorized for speed, let’s stick with nested for loops in the interest of clarity.\n\n\n\nNow that we’ve collected some data, we can specify our model:\n\n\n\nNext, bundle up the data.\n\n\n\nProvide initial values.\n\n\n\nAs a side note, it is helpful in JAGS to provide initial values for the incompletely observed occupancy state \\(z\\) that are consistent with observed presences, as provided in this example with zinit. In other words if \\(x(j,i,t,k)=1\\), provide an intial value of 1 for \\(z(j,i,t)\\). Unlike WinBUGS and OpenBUGS, if you do not do this, you’ll often (but not always) encounter an error message such as:\n\n# Error in jags.model(file = 'com_occ.txt', data = data, n.chains = 3) :\n# Error in node x[1,1,2,3] Observed node inconsistent with unobserved\n# parents at initialization\nNow we’re ready to monitor and make inferences about some parameters of interest using JAGS.\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 480\n   Unobserved stochastic nodes: 318\n   Total graph size: 1789\n\nInitializing model\n\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |+                                                 |   2%\n  |                                                        \n  |++                                                |   4%\n  |                                                        \n  |+++                                               |   6%\n  |                                                        \n  |++++                                              |   8%\n  |                                                        \n  |+++++                                             |  10%\n  |                                                        \n  |++++++                                            |  12%\n  |                                                        \n  |+++++++                                           |  14%\n  |                                                        \n  |++++++++                                          |  16%\n  |                                                        \n  |+++++++++                                         |  18%\n  |                                                        \n  |++++++++++                                        |  20%\n  |                                                        \n  |+++++++++++                                       |  22%\n  |                                                        \n  |++++++++++++                                      |  24%\n  |                                                        \n  |+++++++++++++                                     |  26%\n  |                                                        \n  |++++++++++++++                                    |  28%\n  |                                                        \n  |+++++++++++++++                                   |  30%\n  |                                                        \n  |++++++++++++++++                                  |  32%\n  |                                                        \n  |+++++++++++++++++                                 |  34%\n  |                                                        \n  |++++++++++++++++++                                |  36%\n  |                                                        \n  |+++++++++++++++++++                               |  38%\n  |                                                        \n  |++++++++++++++++++++                              |  40%\n  |                                                        \n  |+++++++++++++++++++++                             |  42%\n  |                                                        \n  |++++++++++++++++++++++                            |  44%\n  |                                                        \n  |+++++++++++++++++++++++                           |  46%\n  |                                                        \n  |++++++++++++++++++++++++                          |  48%\n  |                                                        \n  |+++++++++++++++++++++++++                         |  50%\n  |                                                        \n  |++++++++++++++++++++++++++                        |  52%\n  |                                                        \n  |+++++++++++++++++++++++++++                       |  54%\n  |                                                        \n  |++++++++++++++++++++++++++++                      |  56%\n  |                                                        \n  |+++++++++++++++++++++++++++++                     |  58%\n  |                                                        \n  |++++++++++++++++++++++++++++++                    |  60%\n  |                                                        \n  |+++++++++++++++++++++++++++++++                   |  62%\n  |                                                        \n  |++++++++++++++++++++++++++++++++                  |  64%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++                 |  66%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++                |  68%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++               |  70%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++              |  72%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++             |  74%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++            |  76%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++           |  78%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++          |  80%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++         |  82%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++        |  84%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++       |  86%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++      |  88%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++    |  92%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++   |  94%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%\n  |                                                        \n  |+++++++++++++++++++++++++++++++++++++++++++++++++ |  98%\n  |                                                        \n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************| 100%\n\n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |*                                                 |   2%\n  |                                                        \n  |**                                                |   4%\n  |                                                        \n  |***                                               |   6%\n  |                                                        \n  |****                                              |   8%\n  |                                                        \n  |*****                                             |  10%\n  |                                                        \n  |******                                            |  12%\n  |                                                        \n  |*******                                           |  14%\n  |                                                        \n  |********                                          |  16%\n  |                                                        \n  |*********                                         |  18%\n  |                                                        \n  |**********                                        |  20%\n  |                                                        \n  |***********                                       |  22%\n  |                                                        \n  |************                                      |  24%\n  |                                                        \n  |*************                                     |  26%\n  |                                                        \n  |**************                                    |  28%\n  |                                                        \n  |***************                                   |  30%\n  |                                                        \n  |****************                                  |  32%\n  |                                                        \n  |*****************                                 |  34%\n  |                                                        \n  |******************                                |  36%\n  |                                                        \n  |*******************                               |  38%\n  |                                                        \n  |********************                              |  40%\n  |                                                        \n  |*********************                             |  42%\n  |                                                        \n  |**********************                            |  44%\n  |                                                        \n  |***********************                           |  46%\n  |                                                        \n  |************************                          |  48%\n  |                                                        \n  |*************************                         |  50%\n  |                                                        \n  |**************************                        |  52%\n  |                                                        \n  |***************************                       |  54%\n  |                                                        \n  |****************************                      |  56%\n  |                                                        \n  |*****************************                     |  58%\n  |                                                        \n  |******************************                    |  60%\n  |                                                        \n  |*******************************                   |  62%\n  |                                                        \n  |********************************                  |  64%\n  |                                                        \n  |*********************************                 |  66%\n  |                                                        \n  |**********************************                |  68%\n  |                                                        \n  |***********************************               |  70%\n  |                                                        \n  |************************************              |  72%\n  |                                                        \n  |*************************************             |  74%\n  |                                                        \n  |**************************************            |  76%\n  |                                                        \n  |***************************************           |  78%\n  |                                                        \n  |****************************************          |  80%\n  |                                                        \n  |*****************************************         |  82%\n  |                                                        \n  |******************************************        |  84%\n  |                                                        \n  |*******************************************       |  86%\n  |                                                        \n  |********************************************      |  88%\n  |                                                        \n  |*********************************************     |  90%\n  |                                                        \n  |**********************************************    |  92%\n  |                                                        \n  |***********************************************   |  94%\n  |                                                        \n  |************************************************  |  96%\n  |                                                        \n  |************************************************* |  98%\n  |                                                        \n  |**************************************************| 100%\n\nIterations = 11001:21000\nThinning interval = 1 \nNumber of chains = 2 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean     SD Naive SE Time-series SE\nbeta[1]  0.7813 0.3202 0.002264       0.008857\nbeta[2]  1.0468 0.4867 0.003441       0.018857\nbeta[3]  0.8522 0.3162 0.002236       0.007391\nlp[1]    4.3741 1.0743 0.007597       0.012395\nlp[2]    0.5424 0.2900 0.002051       0.003992\nlp[3]    3.6658 0.7431 0.005254       0.007818\nrho[1]  -1.8498 0.4820 0.003408       0.012032\nrho[2]  -2.4447 0.6907 0.004884       0.023822\nrho[3]  -2.1160 0.5040 0.003564       0.011320\n\n2. Quantiles for each variable:\n\n            2.5%     25%     50%     75%   97.5%\nbeta[1]  0.14747  0.5686  0.7791  1.0012  1.3968\nbeta[2]  0.33931  0.7367  0.9735  1.2575  2.1867\nbeta[3]  0.26391  0.6370  0.8411  1.0598  1.4899\nlp[1]    2.73335  3.6223  4.2153  4.9444  6.9273\nlp[2]   -0.01054  0.3467  0.5369  0.7349  1.1234\nlp[3]    2.41690  3.1469  3.5894  4.0979  5.3426\nrho[1]  -2.79556 -2.1745 -1.8518 -1.5268 -0.8797\nrho[2]  -4.07422 -2.8239 -2.3514 -1.9686 -1.3375\nrho[3]  -3.17608 -2.4312 -2.1003 -1.7746 -1.1768\n\n\n\n\nDorazio, Robert M, Marc Kery, J Andrew Royle, and Matthias Plattner. 2010. “Models for Inference in Dynamic Metacommunity Systems.” Ecology 91 (8): 2466–75. https://doi.org/10.1890/09-1033.1.\n\n\nKéry, M, and J Andrew Royle. 2008. “Hierarchical Bayes Estimation of Species Richness and Occupancy in Spatially Replicated Surveys.” Journal of Applied Ecology 45 (2): 589–98. https://doi.org/10.1111/j.1365-2664.2007.01441.x.\n\n\nMacKenzie, Darryl I, James D Nichols, Gideon B Lachman, Sam Droege, J Andrew Royle, and Catherine A Langtimm. 2002. “Estimating Site Occupancy Rates When Detection Probabilities Are Less Than One.” Ecology 83 (8): 2248–55. https://doi.org/10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2.\n\n\nRoyle, J Andrew, and Robert M Dorazio. 2008. Hierarchical Modeling and Inference in Ecology: The Analysis of Data from Populations, Metapopulations and Communities. Elsevier. https://doi.org/10.1016/B978-0-12-374097-7.50001-5.\n\n\nRoyle, J Andrew, and Marc Kéry. 2007. “A Bayesian State-Space Formulation of Dynamic Occupancy Models.” Ecology 88 (7): 1813–23. https://doi.org/10.1890/06-0669.1.\n\n\n\n\n",
    "preview": "posts/2018-12-22-dynamic-community-occupancy-model-jags/dynamic-community-occupancy-model-jags_files/figure-html5/fit-model-1.png",
    "last_modified": "2022-06-14T20:48:53-06:00",
    "input_file": {}
  }
]
